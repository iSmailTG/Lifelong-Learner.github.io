[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this Blog I share my learning journey through machine learning and datascience."
  },
  {
    "objectID": "posts/Fastai_ch2/Questionnaire.html",
    "href": "posts/Fastai_ch2/Questionnaire.html",
    "title": "Chapter 2: Questionnaire",
    "section": "",
    "text": "Q1:\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n___ * There is many cases where the bear_classifaction model could produce low results because of low quality of training data: - imbalance dataset: where we have much datapoint of one class way more than other classes, what causes the model to be biased toward one class. - the image used in training are low quality, low resolution.\nQ2: Where do text models currently have a major deficiency? ___ * The current transformers models shows oustanding results in generating texts and essais, understanding (in a way!) human language and can participate in a full conversation on different topics and give understandable and admirable responses. * However the way models learn from text is way different than the human do. Models needs a huge amount of text data: \n\nIn this paper,Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data Emily Bender and Alexander Koller consider whether LMs such as GPT-3 or BERT can ever learn to “understand” language? the researchers insists on deffirentiate between form (which LMs are good at understanding) and meaning( which obviously LMs can’t understand).\n\nQ3:\nWhat are the possible negative societal implications of text generation models?\n___ * If someone uses these LMs to generate highly-compeling responds on social media in order to spred misinformation or encourage conflits.\nQ4:\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n___\n* In this case we need to set a system where there’s a human intervention.\nQ5:\nWhat are the steps of the Drivetrain approach?\n___ - Define your Objective\n- Levers\n- Data\n- Models\nQ6:\nWhat is DataLoaders?\n___ * DataLoaders is a Fastai thin class that coutains dataloader for training and validation.\nQ7:\nWhat four things do we need to tell fastai to create DataLoaders?\n___ * Data we have * How to get items * How to label them * How to create train/validation\nQ8:\nWhat does the splitter parameter to DataBlock do? ___ * Splitter provide the way we want our data set to be splited.\nQ9:\nHow do we ensure a random split always gives the same validation set?\n___ * By fixing the seed value.\nQ10:\nWhat letters are often used to signify the independent and dependent variables?\n____ * x for independent variables, y for dependent.\nQ11:\nWhat’s the difference between crop, pad, and squish Resize() approaches? When might you choose one over the other?\n___\n\nCrop is the default Resize() method, which crop the image and take desired dimension, this may cause losing important information.\nPad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images), this may results in a lower effective resolution for the part of the image we actually use.\nSquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nThe better method is something depends on the problem we have, type of data we will use..\nWe will see later different methods, like RandomResizeCrop and many more.\n\nQ12:\nWhat is data augmentation? Why is it needed?\n___\n* Data augmentation refer to the process of generating more datapoints from the actaual data we have, and representing it within the dataset. * For example we could take an image and do some type of transformation to it, like flipping it or ratating it the resize the crop it, which will give us many images with different views and sizes. * This method other than making our dataset larger, it make it rich and diverse which will without doubt influence the generalization of the model.\nQ13:\nWhat is the difference between item_tfms and batch_tfms?\n___ * item_tfms is done on cpu, batch_tfms on gpu.\nQ14:\nWhat is a confusion matrix?\n___ * confusion_matrix return where the model get wrong prediction and what was the actual label.\nQ15:\nWhat does export save do?\n____ * It saves 3 things: - the architecture - the updated parameters(weihts+biases) - the way we built dataloaders\nQ15:\nWhat is it called when we use a model for getting predictions, instead of training?\n____ * Inference"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "As we saw in the previous Chapter, we can create powerful model with only 6 lines of code\n\nAlthought we should understand the constraints of the process and not overestimate the capabilities of deep learnig, this may lead to frustaingtly poor result\n\nAlso we need to not overestimate the constraints, and underestimate what could do with deep learning"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "Deep Learning In Production",
    "text": "Deep Learning In Production\nWhen we create a model that met our objectives, we can then pass to the Production phase, where we transform the model into an appliction/service etc..\nBut first we need to export the model into a file:\n\nlearn.export()\n\nLet’s check that the file exists, by using the ls method that fastai adds to Python’s Path class:\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\n\nNow with the export.pkl file we can create normal program that takes inputs(images) and produce results (predictions) just like any traditional app\nAt this point we won’t call it a model anymore, we call it inference.\n\n >Using the trained model as program\nThe export() function allow us to save the model in oreder to use it later, and as we know model is Architecture + Parameters, Fastai by default saves also the method of which we’ve created the DataLoaders, because otherwise we have to define it again in order to work with the new data we will feed to the model.\n\nFrom Model to Inference\nThis file export.pkl is allways needed wherever we will create an app from it, for now we will use it whithin this notebook in order to create a small app that can predict bears type from image we will provide.\nWhen we use a model for getting predictions, instead of training, we call it inference.\nTo create inference learner from export.pkl file we use load_learner:\n\nlearn_inf = load_learner(path/'export.pkl')\n\n\n# predicting one image\nlearn_inf.predict('images/bear.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([8.0130e-07, 1.0000e+00, 2.0153e-07]))\n\n\n\nThis has returned three things:\n\nthe predicted category(label) in the same format we originally provided (in this case that’s a string),\nthe index of the predicted category, and the probabilities of each category.\nthe last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories.\n\nAt inference time, you can access the DataLoaders as an attribute of the Learner:\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\nWe can see here that if we index into the vocab with the integer returned by predict then we get back “grizzly,” as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly.\n\n\nGradio + HugginFace Spaces\n\nIf we want to share our model with a broader audience, and showcase our skills we need to create a real app that can be used outside of the datascience/machine learning word where nobody know or have the ability to use jupyter notebook or python.. and that’s why we will show using a combination of python package Gradio that will allow us to build our app, then host it on HuggingFace\n\n\n# firstly install gradio\n!pip install gradio\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting gradio\n  Downloading gradio-3.6-py3-none-any.whl (5.3 MB)\n     |████████████████████████████████| 5.3 MB 5.2 MB/s \nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\nCollecting pydub\n  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\nCollecting paramiko\n  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n     |████████████████████████████████| 212 kB 67.2 MB/s \nCollecting httpx\n  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\nCollecting orjson\n  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n     |████████████████████████████████| 270 kB 62.9 MB/s \nCollecting ffmpy\n  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\nCollecting fastapi\n  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n     |████████████████████████████████| 55 kB 3.8 MB/s \nRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\nCollecting h11<0.13,>=0.11\n  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n     |████████████████████████████████| 54 kB 3.7 MB/s \nRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\nCollecting pycryptodome\n  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n     |████████████████████████████████| 2.3 MB 47.6 MB/s \nRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.8.2)\nCollecting markdown-it-py[linkify,plugins]\n  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\nCollecting python-multipart\n  Downloading python-multipart-0.0.5.tar.gz (32 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\nCollecting uvicorn\n  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n     |████████████████████████████████| 56 kB 5.3 MB/s \nCollecting websockets\n  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n     |████████████████████████████████| 112 kB 70.9 MB/s \nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\nCollecting starlette==0.20.4\n  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n     |████████████████████████████████| 63 kB 2.5 MB/s \nCollecting anyio<5,>=3.4.0\n  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n     |████████████████████████████████| 80 kB 10.6 MB/s \nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (2022.9.24)\nCollecting httpcore<0.16.0,>=0.15.0\n  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n     |████████████████████████████████| 68 kB 6.7 MB/s \nCollecting rfc3986[idna2008]<2,>=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting mdit-py-plugins\n  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n     |████████████████████████████████| 46 kB 4.5 MB/s \nCollecting linkify-it-py~=1.0\n  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\nCollecting uc-micro-py\n  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.4)\nCollecting pynacl>=1.0.1\n  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n     |████████████████████████████████| 856 kB 63.6 MB/s \nCollecting cryptography>=2.5\n  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n     |████████████████████████████████| 4.0 MB 41.4 MB/s \nCollecting bcrypt>=3.1.3\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n     |████████████████████████████████| 593 kB 68.7 MB/s \nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.25.11)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\nBuilding wheels for collected packages: ffmpy, python-multipart\n  Building wheel for ffmpy (setup.py) ... done\n  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=bf42b4bf9ba2633a885814063d3340770c60207e05f131d9e165fafd4a5e1aa5\n  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n  Building wheel for python-multipart (setup.py) ... done\n  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=f8ff9e663a939cd5bba16c9c7ce235f69c0fab5ae8199bbfce9b5fdb3361d05e\n  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\nSuccessfully built ffmpy python-multipart\nInstalling collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\nSuccessfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.1 fastapi-0.85.1 ffmpy-0.3.0 gradio-3.6 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.19.0 websockets-10.3\n\n\n\n# create labels from dalaloaders vocab\nlabels = learn.dls.vocab\n# predicting function that take an image as input and use learn.predict to ouput:\n# prediction, prediction index, and probability.\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\n\nHere we will use gradio in order to create an app withing this notebook.\n\n\n\nimport gradio as gr\ngr.Interface(fn=predict, inputs=gr.inputs.Image(shape=(512, 512)), outputs=gr.outputs.Label(num_top_classes=3)).launch(share=True)\n\n/usr/local/lib/python3.7/dist-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n  \"Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n  warnings.warn(value)\n/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n  warnings.warn(value)\n\n\nColab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\nRunning on public URL: https://bbc80cb39aa58cc0.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n(<gradio.routes.App at 0x7f45d2b49710>,\n 'http://127.0.0.1:7860/',\n 'https://bbc80cb39aa58cc0.gradio.app')\n\n\n\nStill the best way of creating app for inference is to use Hugginface platform with help of gradio.\nHere is the app that classify bears types based the .pkl file we created from our model we trained: Here"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html",
    "href": "posts/Fastai_ch1/Chapter1.html",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 34.6 MB/s \n     |████████████████████████████████| 1.3 MB 53.8 MB/s \n     |████████████████████████████████| 5.3 MB 53.3 MB/s \n     |████████████████████████████████| 441 kB 65.8 MB/s \n     |████████████████████████████████| 1.6 MB 46.3 MB/s \n     |████████████████████████████████| 212 kB 71.6 MB/s \n     |████████████████████████████████| 115 kB 71.6 MB/s \n     |████████████████████████████████| 163 kB 73.8 MB/s \n     |████████████████████████████████| 127 kB 72.7 MB/s \n     |████████████████████████████████| 115 kB 73.2 MB/s \n     |████████████████████████████████| 7.6 MB 48.1 MB/s \nMounted at /content/gdrive"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "href": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Brief History of Neural Network",
    "text": "Brief History of Neural Network\n\nIn 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, both devoloped a concept called Artificial Neuron which is seen today as the first theorotical demonstration of a machine that’s “..capable of perceiving, recognizing and identifying its surrondings”\n\nIn the 50’s Frank Rossenblatt devoloped a device based on the principiles of Artificial Neuron\n\nRosenblatt invented The Perceptron which was capable of recognizing simple shapes and patterns\n\n\nIn 1969 Marvin minsky write a book called Perceptrons where he showed that Perceptron limits of solving critical math problem\n\n\nParallel Distributed Processing (PDP)\n\nAfter a long winter of Deep Learning, a group of researchers at MIT released a papper in 1986 released the most influencial papper in history of Neural Network\n\nAuthors claimed that PDP approach was closer to how human brain works\n\nPDP require some enviroments elements:\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\n1980’s and 90’s\n\nDuring this epoch researcher started to build models with 2 layers of neurons\n\nWe saw real world application of Deep Learning"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "href": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Top to bottom learning approach",
    "text": "Top to bottom learning approach\n\nStart with the end point\n\nBuild a foundation of intuition through application then build on it with theory\nShow students how individuals pieces of theory are combined in a real-world application\n\nTeach through examples\n\nProvide a context and a purpose for abstract concepts\n\n\n\nWhat is Machine Learning?\nTraditional Programming:\n- It’s hard to explicitly code hard tasks specialy when you don’t know the exact steps\n\n\n\na-traditional-program.png\n\n\nMachine Learning\n- A program that need to be showed examples of the desired tasks so it can Learn from them\n- Demand sufficient amount of examples(Data)\n- In 1949 Arthur Samuel manage to build a machine that can play checker - This work introduced multiple concepts to the world of machine learning: - The idea of a weight assignment\n- The fact that every weight assignment has some actual performance\n- The requirement that there be an automatic means of testing that performance\n- The need for a mechanism (i.e., another automatic process) for improving the performance by changing the weight assignments\n- Update the weight values based on the performance with the current values\n\n\n\nprogram-using-weight-assignments.png\n\n\n\nA system that used Weight Assignment\n\n >Training Machine Learning model with help of weights updating\n >Using the trained model as program\nNow Let’s Build Our First Model!"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "href": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "First Deep Learning Model: It’s a Bird!",
    "text": "First Deep Learning Model: It’s a Bird!\n\nImport Dependencies\n\n#Import fastai library\n#Import fastai computer vision library \nfrom fastbook import *\nfrom fastai.vision.all import *\n\n\n\nStart working with Data\n\n# Here we use search_images_ddg in order to download an url from ddg\n# search engine accornding to the keyword we choose: birds photo\nurls = search_images_ddg('birds photo', max_images=1)\nlen(urls), urls[0]\n\n(1,\n 'https://www.wallpapergeeks.com/wp-content/uploads/2014/02/Colorful-Bird-Perched-Wallpaper.jpg')\n\n\n\n# download the photo from the url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nPath('bird.jpg')\n\n\n\n# then show it\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow we will do the same etapes we did with bird.jpg image but with forest picture\n\n# Download the url, download the image then open it\ndownload_url(search_images_ddg('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\n\n\n\nThe idea here is to build a model that can classify pictures of birds and forests accuratly, but first we need to build the Dataset\n\n# create 2 directories, one for birds and other for forest, then do the same etapes we did earlier,\n# download the urls then the images\nsearches = 'forest','bird'\npath = Path('bird_or_not')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images_ddg(f'{o} photo'))\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSince we are dowlnloading images from the web, there is a chance that some of them are corrupted, so we need to clean the dataset from them\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nNow we have two directories :\n\nbirds\nforests\n\nAs we saw before, machine learning model needs to be feed many examples in order to learn from them.\nThese examples are the images of birds and forests in our case\nWe need to tell the model bunch of informations about our data before the training process, this will be done with help of DataBlock\n\n\n\nCreating DataBlock and DataLoaders\nNow we need to build DataBlock in order to feed the dataset to the model for the training.it’s basically set of rules of how to organize the dataset for the model.\nI will write a BlogPost about this concept later\nThere’s also the concept of DataLoaders which is an iterator class that load data to the model according to the set of rules that we set earlier while creating DataBlock\n\n# create the dataBlock\n# tell the model what kind of data we'r dealing with\n# how to get the data\n# how to create validation and training set\n# how to get the labels\n# how to set tranformers(items in this case)\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\nTraining the Model\nNow we are ready to train our model and see if we could classify photo of birds and forests\nThe model we will use here is resenet18 which is a famous model that’s used widely among computer vision classification problem, 18 stands for how many layers does the model have.\nfastai comes with fine_tune() which uses the best practices of fine tuning process.\n\nlearn =  vision_learner(dls, resnet18, metrics= error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.550929\n      2.057105\n      0.447368\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.084728\n      0.225560\n      0.092105\n      00:02\n    \n    \n      1\n      0.045721\n      0.024148\n      0.000000\n      00:02\n    \n    \n      2\n      0.031065\n      0.012299\n      0.000000\n      00:02\n    \n  \n\n\n\n\nWith help of fastai library, and less than 20 lines of code we managed to build a model that can predict images of birds and forests with accuracy of 100%!\n\n\n\nFine Tuning\nis a process where we start with a model that has already be trained and we use it on our problem and on our dataset, this operation needs to adapt the pre-trained model by a bit so it fit our system\nThis approach can save us money and time, all we need to do is adapt the pre-trained model and take advantage of set of weights and use them in our problem\n\n\n\nLearner\nIn Fastai we have the Learner which takes 2 things(For now!): firsly the Dataset, and secondly the Model.\nBecause Deep Learning is more mature now,there’s a small group of Architecture that can be used in nearly any problem, that’s why the actual work of deep learning practionaire is to work on data preparation, and model deploying, more than anything else.\nThis is why Fastai integrated a famous library Timm which collect all SOTA (State Of The Art) computer vision models.\n\n\nVisualizing layers of a trained neural network\n\nIt is possible to inspect deep learning models and get insights from them\n\ncan still be challenging to fully understand\n\nVisualizing and Understanding Convolutional Networks paper\n\npublished by PhD student Matt Zeiler and his supervisor Rob Fergus in 2013\nshowed how to visualize the neural network weights learned in each layer of a model\ndiscovered the early layers in a convolutional neural network recognize edges and simple patterns which are combined in later layers to detect more complex shapes\n\nWe will see this paper in details later, but for now we could show the model architecture.\n\n\n# show the model architecture\nm = learn.model\nm\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)\n\n\n\n\nOther Deep Learning Applications\n\nImage Segmentation\n\ntraining a model to recognize the content of every single pixel in an image\n\n\n\n\n\noutput_47_2.png\n\n\n\nNatural Language Processing (NLP)\n\ngenerate text\ntranslate from one language to another\nanalyze comments\nlabel words in sentences\n\nTabular Data\n\ndata that in in the form of a table\n\nspreedsheets\ndatabases\nComma-separated Values (CSV) files\n\nmodel tries to predict the value of one column based on information in other columns\n\nRecommendation Systems\n\nmodel tries to predict the rating a user would give for something"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "",
    "text": "Do you need these for deep learning?\n\nLots of math\n\nFalse\n\nLots of data\n\nFalse\n\nLots of expensive computers\n\nFalse\n\nA PhD\n\nFalse\n\n\n\n\n\nName five areas where deep learning is now the best in the world.\n\nMedical research\nRobotics\nAssurance\nLinguistics/ Natural Lunguage Processing\nBiology\n\n\n\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nPercepton\n\n\n\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nMinsky in his book Perceptons shows the limitation of the device “perceptons” and how it cannot solve any comlex problem, the ai community did agreed with Minsky, but the did not pay attention to the solution he suggested, which is a 2 layer model.\nIn 80’s the 2 layer models were usual in ai labs. In theory a 2 layer model can solve any problem, but in practice the layers were too big and consum a lot of computation power, the solution to this is to add more layers, but this insight was not acknowledged, what leat to the 2 winter of NN.\n\n\n\nWhat is a GPU?\nIt’s a Graphic Prossessing Unit, which is used to do many computation tasks in parallel, which help to accelerate to compution of big tasks by cutting them into small tasks and compute them in parallel\n\n\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nto write a programm that recognize images in a photo we need to write a long set of rules that resume any possible photo and image, tell the computer exactly how to deal with any of them, which is way more complicated that what we can do. That’s way we use machine learning to solve those kinfd of problems, just by showing the model data and help it to learn from it.\n\n\n\nWhat did Samuel mean by “Weight Assignment”?\nWhat term do we normally use in deep learning for what Samuel called “Weights”?\nDraw a picture that summarizes Arthur Samuel’s view of a machine learning model\nweight Assignment is refer to the parameters of the model, these are what we call today weights and bias. they are set of value we assign to each data point, what make the optimization of the loss possible.\n\n\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nThis is a highly-researched topic known as interpretability of deep learning models. the natur of deep learning “deep” make it hard to really understand the way the model solve each problem, specially if the model has many layers, what makes it even hard to know exactly which layer is responsable of the procces of learning which part.\n\n\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nUniversal approximation theorem\n\n\n\nWhat do you need in order to train a model?\nIn order to train a model, we need architecture for the given problem, we first need data(+labels), then we need set of values (paramaters), then we need some kind of metric to know if our model did good or bad (loss function), and we need a way to updates these parameters in order to optimize the loss function.\n\n\n\nHow could a feedback loop impact the rollout of a predictive policing model?\nIn a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop\n\n\n\nDo we always have to use 224x224 pixel images with the cat recognition model?\nNo.\n\n\n\nWhat is the difference between classification and regression?\nClassification problem is when we need to decide between 2(or more) classes, the prediction in this problem isn’t quantity, where regression problem is focused on predecting numeric quantity.\n\n\n\nWhat is a validation set? What is a test set? Why do we need them?\nValidation set is small portion of the data set that we preserve from the training fase in order to prevent the model from memorizing the data instead of learning from it. The validation set allow us to measure the performance of the model on data that the model didn’t see before. Same we can say about test set, which is another preserved protion of data that we use in the final fase of the training in order to have a real idea of model performance.\n\n\n\nWhat will fastai do if you don’t provide a validation set?\nit will automatically create a validation set of 20% of our dataset.\nvalid_pct=0.2\n\n\n\nWhat is overfitting?\nis when the model is memorizing answears instead of learning from data.\n\n\n\nWhat is a metric? How does it differ to “loss”?\nMetric is what tells us how the model perform, in other way the loss is what we should minimize in order to optimize the model perfromance.as we will see later, sometimes, we could use the metric as loss.\n\n\n\nHow can pretrained models help?\npertrained model can be used again, we just need to fin it in our probem.\na pretrained model is a model that learned many lesson from the prior problem, and already has good set of paramters, these parameters(weights+biases) are what we seek in the proccess of using a pretrained model. this procces is called fine-tunning, which mean less money and time consuming.\n\n\n\nWhat is the “head” of a model?\nWhen using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model.\n\n\n\nWhat is an “architecture”?\nThe architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.\n\n\n\nWhat are “hyperparameters”?\nTraining models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters."
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "Further Research",
    "text": "Further Research\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:",
    "text": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:\n\nGPU stands for Graphic Processing Unit, is a specialized processor with dedicated memory that conventionally perform floating point operations required for rendering graphics.\nIn other words, it’s a single-chip processor used for extensive graphical and mathematical computations which help the CPU to achieve other tasks.\nWhile CPU is designed to handel the complex logic in code once at a time, GPU can do many small operations at the same time, which make it very convenient to deep learning where we need to do many milions of calculations in order to train a model\n\n\nWhy do Deep Learning needs GPU?\n\nGPUs are optimized for training neural networks models as they can process multiple computations simultaneously.\nFor example the model we’ve fine-tuned in chapter one resnet18 which the smaller version of resnet models with only 18 hidden layers,though it has more than 11 millions parameters, in order to do one epoch and calculate all these parameter (wights + biases) and multiply them by the input variables (images) then do the Back-probagation and update them after calculating the loss … all this multiplications will take a large amount of time if we did it on CPU."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lifelong-Learner.github.io",
    "section": "",
    "text": "Fastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\nNo matching items"
  }
]