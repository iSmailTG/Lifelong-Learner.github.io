[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ismail's Personal Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html",
    "href": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html",
    "title": "Hugging Face Course Notes: Chapter3",
    "section": "",
    "text": "In the previous chapter we learned how to use tokenizers and pretrained models to make predictions.\nIn this chapter we will see how to Fine-tune a model on our Dataset by learning:\n\nHow to prepare a large dataset for the finetuning process\nHow to use the high level API trainer to finetune a model\nHow to leverage the HuggingFace Accelerate library to easily run that custom training loop on any distributed setup\n\nBut first let’s do the usuall by picking an architecture/model/tokenizer, and then train it some sample data:\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n\n\nmdl_ckp = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(mdl_ckp)\nmodel = AutoModelForSequenceClassification.from_pretrained(mdl_ckp)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"This course is amazing!\"]\nbatch = tokenizer(sequences, truncation=True, padding=True, return_tensors='pt')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# will be axplained later\nbatch['labels'] = torch.tensor([1, 1])\n\n\n# training\noptimizer = torch.optim.AdamW(model.parameters())\nloss = model(**batch).loss\nloss.backward()\noptimizer.step()\n\n\nOf course training a model on 2 sentences will not yield a good results\nSo we need to introduce it to a larger dataset\nIn this chapter we will work with: example the MRPC (Microsoft Research Paraphrase Corpus) dataset.\n\nThe dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing)\nThis is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks\n\n\n\n\n\nWe can easily download a dataset from the Hub just like we did with models before:\n\n\n# load dataset\nfrom datasets import load_dataset\nraw_ds = load_dataset('glue', 'mrpc')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n\n\n\nDatasets are presented as DatasetDict which is an object dictionary that our datset is organized by.\n\nHere we have our training-set, validation-set and test-set.\nEach set 2 keys: features and num_rows.\nFeatures has: sentence1, sentence2, label, idx\nsentence1&2 represent the pair we need to train our model on and predict whether its paraphrased or not.\n\nWe can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:\n\n\n# training set\ntrain_ds = raw_ds['train']\ntrain_ds[22]\n\n{'sentence1': 'A BMI of 25 or above is considered overweight ; 30 or above is considered obese .',\n 'sentence2': 'A BMI between 18.5 and 24.9 is considered normal , over 25 is considered overweight and 30 or greater is defined as obese .',\n 'label': 0,\n 'idx': 24}\n\n\n\nHere we see the pair of sentences, the label and the index of that pair.\nLabels are already int value so we won’t need to preprocess them.\nWhat means label: 0?\n\n\n# what means each label\ntrain_ds.features\n\n{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}\n\n\n\n0 for not_equivalent and 1 for equivalent\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mdl_ckp)\ntrain_seq1 = tokenizer(train_ds['sentence1'])\ntrain_seq2 = tokenizer(train_ds['sentence2'])\n\n\n\n\n\nWe can’t just pass two sequences to the model and expect to get proper prediction about whether these sequences are paraphrased or not.\nWe need to apply a proper preparation of the data in order feed the model pairs of sequences instead 2 sentences separtly.\nThis can be done first whith the tokenizer, we create pairs of tokens and compute them the way BERT expect:\n\n\n#example\ninput = tokenizer('this is the first sentence', 'this is number 2')\ninput\n\n{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 102, 2023, 2003, 2193, 1016, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nThe tokenizer output: input_ids, attention_mask, but also token_type_ids.\nThis feature tells us that the tokenizer is aware that we are dealing with the two sentences, each is represented by either 0 or 1\n\n\ninput.token_type_ids\n\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n\n\nIf we convert each input_ids back to token we can have and idea of what happend:\n\n\ntokenizer.convert_ids_to_tokens(input['input_ids'])\n\n['[CLS]',\n 'this',\n 'is',\n 'the',\n 'first',\n 'sentence',\n '[SEP]',\n 'this',\n 'is',\n 'number',\n '2',\n '[SEP]']\n\n\n\nSo we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP]\nNote that not all model’s tokenizer can perform this because the way each model is trained, here BERT have seen pairs and knows how to deal with them.\nWe can then pass pairs of sentences to the tokenizer like this:\n\n\ntokenized_dataset = tokenizer(train_ds['sentence1'], train_ds['sentence2'], truncation=True, padding=True)\n\n\nThis way of tokenizing the whole dataset is not ideal since it requires huge RAM to store the dataset while we process it.\nIt will also return dictionary keys: attention_mask, input_ids, token_type_ids and its values.\nTo work around this problem we will use map() method which will keep data as dataset, and also it will give us more flexibility if we need more preprocessing more than just tokenizing.\nmap() works by applying a function to each element of the dataset, let’s create a function that tokenize pairs of sentences so the map method use it over the whole dataset:\n\n\ndef func_tokenize(example):\n  return tokenizer(example['sentence1'], example['sentence2'], truncation=True)\n\n\nThis function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\nWe didn’t include the padding here, because it’s not sufficient to pad the whole dataset based on the longest sentence, when we can do it on the batch level\nWe can pass the batching as argument in the map() method\n\n\ntokenized_datasets = raw_ds.map(func_tokenize, batched=True)\n\n\n\n\n\n\n\n\n\n\n\nLet’s take a look on pair exmaple from the training dataset:\nWe get what we expected, the 3 keys representing tokenization process, plus the dictionary key we already have: label, idx and sentence1&2:\n\n\ntokenized_datasets['train'][55].keys()\n\ndict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nNow we have to deal with the padding since we decided to apply it o the batch-level, so each batch will have its own longest sequence to pad on.\nSo we need to do a process called: Dynamic Padding.\n\n\n\n\nPutting the samples together in a single batch is done throught a function called: Collate function.\nCollate function convert our samples to Pytorch tensors and concatenate them.\nBut this can’t be done without padding, otherwise we will get different shapes for tensors.\nAs we said before the padding process should be done on batch level, which means each batch will have its samples padded according to the longest sequence otherwise we will get samples a with lot of paddings.\nIn practice we have to define a collate-function that apply the correct amount of padding to the items of the dataset we want to batch together.\n\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\nHere we test this collate function on some samples from training set.\nWe need first to remove columns idx, sentence1, sentence2 since we don’t need them.\nLet’s have a look at the length of each entry in the batch:\n\n\nsamples = tokenized_datasets['train'][:8]\nsamples = {k: v for k, v in samples.items() if k not in ['idx', 'sentence1', 'sentence2']}\n[len(x) for x in samples['input_ids']]\n\n\n[50, 59, 47, 67, 59, 50, 62, 32]\n\n\n\nsamples.keys()\n\ndict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nThese samples are varying between 32 and 67, so our job here is to pad all the other sequence in this particular in respect to the treshold.\n\n\nsample_batch = data_collator(samples)\n{k:v.shape for k, v in sample_batch.items()}\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n{'input_ids': torch.Size([8, 67]),\n 'token_type_ids': torch.Size([8, 67]),\n 'attention_mask': torch.Size([8, 67]),\n 'labels': torch.Size([8])}\n\n\n\nLet’s check again if our input_ids have the same length:\n\n\n[len(i) for i in sample_batch['input_ids']]\n\n[67, 67, 67, 67, 67, 67, 67, 67]"
  },
  {
    "objectID": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#loading-datasets-from-the-hub",
    "href": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#loading-datasets-from-the-hub",
    "title": "Hugging Face Course Notes: Chapter3",
    "section": "",
    "text": "We can easily download a dataset from the Hub just like we did with models before:\n\n\n# load dataset\nfrom datasets import load_dataset\nraw_ds = load_dataset('glue', 'mrpc')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n\n\n\nDatasets are presented as DatasetDict which is an object dictionary that our datset is organized by.\n\nHere we have our training-set, validation-set and test-set.\nEach set 2 keys: features and num_rows.\nFeatures has: sentence1, sentence2, label, idx\nsentence1&2 represent the pair we need to train our model on and predict whether its paraphrased or not.\n\nWe can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:\n\n\n# training set\ntrain_ds = raw_ds['train']\ntrain_ds[22]\n\n{'sentence1': 'A BMI of 25 or above is considered overweight ; 30 or above is considered obese .',\n 'sentence2': 'A BMI between 18.5 and 24.9 is considered normal , over 25 is considered overweight and 30 or greater is defined as obese .',\n 'label': 0,\n 'idx': 24}\n\n\n\nHere we see the pair of sentences, the label and the index of that pair.\nLabels are already int value so we won’t need to preprocess them.\nWhat means label: 0?\n\n\n# what means each label\ntrain_ds.features\n\n{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}\n\n\n\n0 for not_equivalent and 1 for equivalent\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mdl_ckp)\ntrain_seq1 = tokenizer(train_ds['sentence1'])\ntrain_seq2 = tokenizer(train_ds['sentence2'])"
  },
  {
    "objectID": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#preprocessing-the-dataset",
    "href": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#preprocessing-the-dataset",
    "title": "Hugging Face Course Notes: Chapter3",
    "section": "",
    "text": "We can’t just pass two sequences to the model and expect to get proper prediction about whether these sequences are paraphrased or not.\nWe need to apply a proper preparation of the data in order feed the model pairs of sequences instead 2 sentences separtly.\nThis can be done first whith the tokenizer, we create pairs of tokens and compute them the way BERT expect:\n\n\n#example\ninput = tokenizer('this is the first sentence', 'this is number 2')\ninput\n\n{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 102, 2023, 2003, 2193, 1016, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nThe tokenizer output: input_ids, attention_mask, but also token_type_ids.\nThis feature tells us that the tokenizer is aware that we are dealing with the two sentences, each is represented by either 0 or 1\n\n\ninput.token_type_ids\n\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n\n\nIf we convert each input_ids back to token we can have and idea of what happend:\n\n\ntokenizer.convert_ids_to_tokens(input['input_ids'])\n\n['[CLS]',\n 'this',\n 'is',\n 'the',\n 'first',\n 'sentence',\n '[SEP]',\n 'this',\n 'is',\n 'number',\n '2',\n '[SEP]']\n\n\n\nSo we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP]\nNote that not all model’s tokenizer can perform this because the way each model is trained, here BERT have seen pairs and knows how to deal with them.\nWe can then pass pairs of sentences to the tokenizer like this:\n\n\ntokenized_dataset = tokenizer(train_ds['sentence1'], train_ds['sentence2'], truncation=True, padding=True)\n\n\nThis way of tokenizing the whole dataset is not ideal since it requires huge RAM to store the dataset while we process it.\nIt will also return dictionary keys: attention_mask, input_ids, token_type_ids and its values.\nTo work around this problem we will use map() method which will keep data as dataset, and also it will give us more flexibility if we need more preprocessing more than just tokenizing.\nmap() works by applying a function to each element of the dataset, let’s create a function that tokenize pairs of sentences so the map method use it over the whole dataset:\n\n\ndef func_tokenize(example):\n  return tokenizer(example['sentence1'], example['sentence2'], truncation=True)\n\n\nThis function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\nWe didn’t include the padding here, because it’s not sufficient to pad the whole dataset based on the longest sentence, when we can do it on the batch level\nWe can pass the batching as argument in the map() method\n\n\ntokenized_datasets = raw_ds.map(func_tokenize, batched=True)\n\n\n\n\n\n\n\n\n\n\n\nLet’s take a look on pair exmaple from the training dataset:\nWe get what we expected, the 3 keys representing tokenization process, plus the dictionary key we already have: label, idx and sentence1&2:\n\n\ntokenized_datasets['train'][55].keys()\n\ndict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nNow we have to deal with the padding since we decided to apply it o the batch-level, so each batch will have its own longest sequence to pad on.\nSo we need to do a process called: Dynamic Padding.\n\n\n\n\nPutting the samples together in a single batch is done throught a function called: Collate function.\nCollate function convert our samples to Pytorch tensors and concatenate them.\nBut this can’t be done without padding, otherwise we will get different shapes for tensors.\nAs we said before the padding process should be done on batch level, which means each batch will have its samples padded according to the longest sequence otherwise we will get samples a with lot of paddings.\nIn practice we have to define a collate-function that apply the correct amount of padding to the items of the dataset we want to batch together.\n\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\nHere we test this collate function on some samples from training set.\nWe need first to remove columns idx, sentence1, sentence2 since we don’t need them.\nLet’s have a look at the length of each entry in the batch:\n\n\nsamples = tokenized_datasets['train'][:8]\nsamples = {k: v for k, v in samples.items() if k not in ['idx', 'sentence1', 'sentence2']}\n[len(x) for x in samples['input_ids']]\n\n\n[50, 59, 47, 67, 59, 50, 62, 32]\n\n\n\nsamples.keys()\n\ndict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nThese samples are varying between 32 and 67, so our job here is to pad all the other sequence in this particular in respect to the treshold.\n\n\nsample_batch = data_collator(samples)\n{k:v.shape for k, v in sample_batch.items()}\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n{'input_ids': torch.Size([8, 67]),\n 'token_type_ids': torch.Size([8, 67]),\n 'attention_mask': torch.Size([8, 67]),\n 'labels': torch.Size([8])}\n\n\n\nLet’s check again if our input_ids have the same length:\n\n\n[len(i) for i in sample_batch['input_ids']]\n\n[67, 67, 67, 67, 67, 67, 67, 67]"
  },
  {
    "objectID": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#evaluation",
    "href": "posts/HuggingFace_3/HuggungFace_NLP_course_Notes_3.html#evaluation",
    "title": "Hugging Face Course Notes: Chapter3",
    "section": "Evaluation:",
    "text": "Evaluation:\n\nFirst we need to build a compute_metrics() function in order to use it in the next training.\nThe function takes a EvalPrediction object as argument, which is basically a named tuple with:\n\npredictions field\nlabel_ids field\n\nHere we get some predictions from our model:\n\n\npredictions = trainer.predict(tokenized_datasets[\"validation\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)\n\n\n\n\n(408, 2) (408,)\n\n\n\nThe output of the predict() method is another named tuple with three fields: predictions, label_ids, and metrics.\nMetrics represent the loss on the dataset, as well as the time metrics, how much it takes the predictions on total average.\nAs we see here, predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used).\nIt represent the logits for each element of the dataset we passed to predict() .\nTo transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:\n\n\nimport numpy as np\n\npreds = np.argmax(predictions.predictions, axis=-1)\n\n\nNow we have ot build our compute_metrics() function in order to compare the predictions of the model with the actual labels.\nWe will use the metrics from the huggingFace library **’evaluate**, we just need to loa the meetrics associated with the dataset we usedmrpc`:\n\n\nimport evaluate\nmetric = evaluate.load('glue', 'mrpc')\nmetric.compute(predictions= preds, references= predictions.label_ids)\n\n\n\n\n{'accuracy': 0.8529411764705882, 'f1': 0.8993288590604027}\n\n\n\nNow let’s wrap everething in a single function:\n\n\ndef compute_metrics(eval_preds):\n  metric = evaluate.load('glue', 'mrpc')\n  logits, labels = eval_preds\n  predictions = np.argmax(logits, axis= -1)\n  return metric.compute(predictions= predictions, references= labels)\n\n\nNow let’s use it the training loop so it report all the metrics at the end of each epoch:\n\n\ntraining_args = TrainingArguments('test-train', evaluation_strategy= 'epoch')\ntrainer= Trainer(\n    model,\n    training_args,\n    train_dataset= tokenized_datasets['train'],\n    eval_dataset= tokenized_datasets['validation'],\n    data_collator= data_collator,\n    tokenizer= tokenizer,\n    compute_metrics= compute_metrics\n)\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [1377/1377 03:39, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\nF1\n\n\n\n\n1\nNo log\n0.681234\n0.843137\n0.893333\n\n\n2\n0.162700\n0.998050\n0.840686\n0.892562\n\n\n3\n0.051500\n1.032399\n0.852941\n0.898305\n\n\n\n\n\n\nTrainOutput(global_step=1377, training_loss=0.08403856129254997, metrics={'train_runtime': 219.6964, 'train_samples_per_second': 50.087, 'train_steps_per_second': 6.268, 'total_flos': 405626802939840.0, 'train_loss': 0.08403856129254997, 'epoch': 3.0})\n\n\n\nIn this section we fine-tuned a model on a dataset by using the Trainer API, which minimize the work we have to since it works out of the box.\nTrainer can be used in most NLP tasks, but what if we need to do everything manualy by using pure Pytorch?\nIn next section we will build the same Trainer by hand.\n\n# A full training\n\nIn this section we will try to achieve the same results we had with Trainer API.\nSince we already done with the preprocessing of the dataset, we just have to to some tweaks regarding some columns of the dataset we won’t use:\n\n\ntokenized_ds = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\ntokenized_ds = tokenized_ds.rename_column('label', 'labels')\ntokenized_ds.set_format('torch')\ntokenized_ds['train'].column_names\n\n['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n\n\n\nNow we need to define the DataLoader that will help us to feed the model the dataset in batches:\n\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ntrain_dataloader = DataLoader(tokenized_ds['train'], shuffle= True, batch_size= 8, collate_fn= data_collator)\neval_dataloader = DataLoader(tokenized_ds['validation'], batch_size= 8, collate_fn= data_collator)\n\n\nCheck the training data loader:\n\n\nfor batch in train_dataloader:\n  break\n{k:v.shape for k,v in batch.items()}\n\n{'labels': torch.Size([8]),\n 'input_ids': torch.Size([8, 70]),\n 'token_type_ids': torch.Size([8, 70]),\n 'attention_mask': torch.Size([8, 70])}\n\n\n\nMake sure that everything will go smoothly during the training , we pass a batch to the model:\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(mdl_ckp, num_labels= 2)\noutput= model(**batch)\noutput.loss, output.logits.shape\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n(tensor(0.5792, grad_fn=&lt;NllLossBackward0&gt;), torch.Size([8, 2]))\n\n\n\nEverything looks fine, before we build the training loop we just need 2 things:\n\noptimizer\nlearning-rate scheduler\n\nSonce we want to replicate the Trainer and its defaults parameters, the optimizer used by Trainer is Adam, we will use a slightly different optimizer AdamW:\n\n\noptimizer = AdamW(model.parameters(), lr= 5e-5)\n\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\nFinally the learning-rate scheduler used by the Trainer is just a simple linear decay from the highest learning-rate 5e-5 to 0.\nIn order to define it we just need to know the number of training steps.\n\n\nfrom transformers import get_scheduler\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler= get_scheduler(\n    'linear',\n    optimizer= optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = num_training_steps\n)\n\n\nAs we mension before, Trainer works out of the box with any set of hardware we have, but now we have to set the GPU as device during the training:\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\nmodel.to(device)\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n\n\n\nTraining Loop:\n\nNow we have all the ingredients to start out training loop:\n\n\nfrom tqdm.auto import tqdm\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n  for batch in train_dataloader:\n    batch = {k:v.to(device) for k, v in batch.items()}\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n\n    optimizer.steps()\n    lr_scheduler.steps()\n    optimizer.zero_grad()\n    progress_bar.update(1)\n\n\n\nEvaluating Loop:\n\nEvaluation loop is basically the same as compute_metrics() function we built before:\n\n\nmetric = evaluate.load('glue', 'mrpc')\nmodel.eval()\nfor batch in eval_dataloader:\n  batch = {k:v.to(device) for k, v in batch.itmes()}\n  with torch.no_grad():\n    outputs = model(**batch)\n\n  logits = outputs.logits\n  predictions = torch.argmax(logits, axis= -1)\n  metric.add_batch(predictions=predictions, references= batch['labels'] )\nmetric.compute()\n\nTo recap, in this chapter we :\n- Learned about datasets in the Hub\n- Learned how to load and preprocess datasets, including using dynamic padding and collators\n- Implemented your own fine-tuning and evaluation of a model\n- Implemented a lower-level training loop"
  },
  {
    "objectID": "posts/Fastai_ch2/Questionnaire.html",
    "href": "posts/Fastai_ch2/Questionnaire.html",
    "title": "Chapter 2: Questionnaire",
    "section": "",
    "text": "Q1:\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n___ * There is many cases where the bear_classifaction model could produce low results because of low quality of training data: - imbalance dataset: where we have much datapoint of one class way more than other classes, what causes the model to be biased toward one class. - the image used in training are low quality, low resolution.\nQ2: Where do text models currently have a major deficiency? ___ * The current transformers models shows oustanding results in generating texts and essais, understanding (in a way!) human language and can participate in a full conversation on different topics and give understandable and admirable responses. * However the way models learn from text is way different than the human do. Models needs a huge amount of text data: \n\nIn this paper,Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data Emily Bender and Alexander Koller consider whether LMs such as GPT-3 or BERT can ever learn to “understand” language? the researchers insists on deffirentiate between form (which LMs are good at understanding) and meaning( which obviously LMs can’t understand).\n\nQ3:\nWhat are the possible negative societal implications of text generation models?\n___ * If someone uses these LMs to generate highly-compeling responds on social media in order to spred misinformation or encourage conflits.\nQ4:\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n___\n* In this case we need to set a system where there’s a human intervention.\nQ5:\nWhat are the steps of the Drivetrain approach?\n___ - Define your Objective\n- Levers\n- Data\n- Models\nQ6:\nWhat is DataLoaders?\n___ * DataLoaders is a Fastai thin class that coutains dataloader for training and validation.\nQ7:\nWhat four things do we need to tell fastai to create DataLoaders?\n___ * Data we have * How to get items * How to label them * How to create train/validation\nQ8:\nWhat does the splitter parameter to DataBlock do? ___ * Splitter provide the way we want our data set to be splited.\nQ9:\nHow do we ensure a random split always gives the same validation set?\n___ * By fixing the seed value.\nQ10:\nWhat letters are often used to signify the independent and dependent variables?\n____ * x for independent variables, y for dependent.\nQ11:\nWhat’s the difference between crop, pad, and squish Resize() approaches? When might you choose one over the other?\n___\n\nCrop is the default Resize() method, which crop the image and take desired dimension, this may cause losing important information.\nPad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images), this may results in a lower effective resolution for the part of the image we actually use.\nSquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nThe better method is something depends on the problem we have, type of data we will use..\nWe will see later different methods, like RandomResizeCrop and many more.\n\nQ12:\nWhat is data augmentation? Why is it needed?\n___\n* Data augmentation refer to the process of generating more datapoints from the actaual data we have, and representing it within the dataset. * For example we could take an image and do some type of transformation to it, like flipping it or ratating it the resize the crop it, which will give us many images with different views and sizes. * This method other than making our dataset larger, it make it rich and diverse which will without doubt influence the generalization of the model.\nQ13:\nWhat is the difference between item_tfms and batch_tfms?\n___ * item_tfms is done on cpu, batch_tfms on gpu.\nQ14:\nWhat is a confusion matrix?\n___ * confusion_matrix return where the model get wrong prediction and what was the actual label.\nQ15:\nWhat does export save do?\n____ * It saves 3 things: - the architecture - the updated parameters(weihts+biases) - the way we built dataloaders\nQ15:\nWhat is it called when we use a model for getting predictions, instead of training?\n____ * Inference"
  },
  {
    "objectID": "posts/Fastai_ch2/Questionnaire.html#questionnaire",
    "href": "posts/Fastai_ch2/Questionnaire.html#questionnaire",
    "title": "Chapter 2: Questionnaire",
    "section": "",
    "text": "Q1:\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n___ * There is many cases where the bear_classifaction model could produce low results because of low quality of training data: - imbalance dataset: where we have much datapoint of one class way more than other classes, what causes the model to be biased toward one class. - the image used in training are low quality, low resolution.\nQ2: Where do text models currently have a major deficiency? ___ * The current transformers models shows oustanding results in generating texts and essais, understanding (in a way!) human language and can participate in a full conversation on different topics and give understandable and admirable responses. * However the way models learn from text is way different than the human do. Models needs a huge amount of text data: \n\nIn this paper,Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data Emily Bender and Alexander Koller consider whether LMs such as GPT-3 or BERT can ever learn to “understand” language? the researchers insists on deffirentiate between form (which LMs are good at understanding) and meaning( which obviously LMs can’t understand).\n\nQ3:\nWhat are the possible negative societal implications of text generation models?\n___ * If someone uses these LMs to generate highly-compeling responds on social media in order to spred misinformation or encourage conflits.\nQ4:\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n___\n* In this case we need to set a system where there’s a human intervention.\nQ5:\nWhat are the steps of the Drivetrain approach?\n___ - Define your Objective\n- Levers\n- Data\n- Models\nQ6:\nWhat is DataLoaders?\n___ * DataLoaders is a Fastai thin class that coutains dataloader for training and validation.\nQ7:\nWhat four things do we need to tell fastai to create DataLoaders?\n___ * Data we have * How to get items * How to label them * How to create train/validation\nQ8:\nWhat does the splitter parameter to DataBlock do? ___ * Splitter provide the way we want our data set to be splited.\nQ9:\nHow do we ensure a random split always gives the same validation set?\n___ * By fixing the seed value.\nQ10:\nWhat letters are often used to signify the independent and dependent variables?\n____ * x for independent variables, y for dependent.\nQ11:\nWhat’s the difference between crop, pad, and squish Resize() approaches? When might you choose one over the other?\n___\n\nCrop is the default Resize() method, which crop the image and take desired dimension, this may cause losing important information.\nPad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images), this may results in a lower effective resolution for the part of the image we actually use.\nSquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nThe better method is something depends on the problem we have, type of data we will use..\nWe will see later different methods, like RandomResizeCrop and many more.\n\nQ12:\nWhat is data augmentation? Why is it needed?\n___\n* Data augmentation refer to the process of generating more datapoints from the actaual data we have, and representing it within the dataset. * For example we could take an image and do some type of transformation to it, like flipping it or ratating it the resize the crop it, which will give us many images with different views and sizes. * This method other than making our dataset larger, it make it rich and diverse which will without doubt influence the generalization of the model.\nQ13:\nWhat is the difference between item_tfms and batch_tfms?\n___ * item_tfms is done on cpu, batch_tfms on gpu.\nQ14:\nWhat is a confusion matrix?\n___ * confusion_matrix return where the model get wrong prediction and what was the actual label.\nQ15:\nWhat does export save do?\n____ * It saves 3 things: - the architecture - the updated parameters(weihts+biases) - the way we built dataloaders\nQ15:\nWhat is it called when we use a model for getting predictions, instead of training?\n____ * Inference"
  },
  {
    "objectID": "posts/Haystack_2/ch3_haystack.html",
    "href": "posts/Haystack_2/ch3_haystack.html",
    "title": "Utilizing Existing FAQs for Question Answering",
    "section": "",
    "text": "Introduction:\n\nUsually the Extractive Question Answering systems uses pure data-texts to generate answers, but in some cases it we could useful to use them on previous FAQs as dataset\nThis can be appealing for many reasons:\n\nWe already have data\nInference time is reduced\nMuch control over answers\n\nThe problem is that this can generalize good enough only on similar questions, which make this method good for certain situations only.\n\n\n\nCreate simple DocumentStore:\n\nAs we saw before InMemoryDocumentStore is an easy way for creating DocumentStore for simple prototyping.\n\n\nfrom haystack.document_stores import InMemoryDocumentStore\ndocument_store = InMemoryDocumentStore()\n\n\n\nCreate a Retriever using embeddings:\n\nThe idea here to create embeddings for questions we will get from users\nThis embeddings must match the FAQs we have\nFirst we need to create the embeddings from a model we use\nThen apply the same embeddings on the FAQs we will use as dataset\n\n\nfrom haystack.nodes import EmbeddingRetriever\n\nretriever = EmbeddingRetriever(document_store = document_store,\n                               embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\",\n                               use_gpu = True,\n                               scale_score = False)\n\n\nDownload FAQs dataset\n\n\n# download\nfrom haystack.utils import fetch_archive_from_http\ndoc_dir = 'data/tutorial'\nurls = 'https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/small_faq_covid.csv.zip'\nfetch_archive_from_http(url = urls, output_dir = doc_dir)\n\nTrue\n\n\n\nHere we use Pandas to manipulate the dataset we just downloaded.\nFirst create the dataframe\n\n\nimport pandas as pd\ndf = pd.read_csv(f'{doc_dir}/small_faq_covid.csv')\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nquestion\nanswer\nanswer_html\nlink\nname\nsource\ncategory\ncountry\nregion\ncity\nlang\nlast_update\n\n\n\n\n0\nWhat is a novel coronavirus?\nA novel coronavirus is a new coronavirus that ...\n&lt;p&gt;A novel coronavirus is a new coronavirus th...\n\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...\nFrequently Asked Questions\nCenter for Disease Control and Prevention (CDC)\nCoronavirus Disease 2019 Basics\nUSA\nNaN\nNaN\nen\n2020/03/17\n\n\n1\nWhy is the disease being called coronavirus di...\nOn February 11, 2020 the World Health Organiza...\n&lt;p&gt;On February 11, 2020 the World Health Organ...\n\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...\nFrequently Asked Questions\nCenter for Disease Control and Prevention (CDC)\nCoronavirus Disease 2019 Basics\nUSA\nNaN\nNaN\nen\n2020/03/17\n\n\n2\nWhy might someone blame or avoid individuals a...\nPeople in the U.S. may be worried or anxious a...\n&lt;p&gt;People in the U.S. may be worried or anxiou...\n\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...\nFrequently Asked Questions\nCenter for Disease Control and Prevention (CDC)\nCoronavirus Disease 2019 Basics\nUSA\nNaN\nNaN\nen\n2020/03/17\n\n\n3\nHow can people help stop stigma related to COV...\nPeople can fight stigma and help, not hurt, ot...\n&lt;p&gt;People can fight stigma and help, not hurt,...\n\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...\nFrequently Asked Questions\nCenter for Disease Control and Prevention (CDC)\nHow It Spreads\nUSA\nNaN\nNaN\nen\n2020/03/17\n\n\n4\nWhat is the source of the virus?\nCoronaviruses are a large family of viruses. S...\n&lt;p&gt;Coronaviruses are a large family of viruses...\n\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...\nFrequently Asked Questions\nCenter for Disease Control and Prevention (CDC)\nHow It Spreads\nUSA\nNaN\nNaN\nen\n2020/03/17\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nSince we download the data from internet we should clean it properly before use in kind of data processing\n\n\ndf.fillna(value='', inplace= True)\ndf[\"question\"] = df[\"question\"].apply(lambda x: x.strip())\nquestions = list(df.question.values)\n\n\nCreate Embeddings on the questions\n\n\ndf.embedding = retriever.embed_queries(queries=questions).tolist()\nlen(df.embedding)\n\n\n\n\nUserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  df.embedding = retriever.embed_queries(queries=questions).tolist()\n\n\n213\n\n\n\nRename the question series with content\n\n\ndf = df.rename(columns={'question':'content'})\n\n\nConvert te dataframe into a list of dics and index them in our DocumentStore\n\n\ndocs_to_index = df.to_dict(orient='records')\ndocument_store.write_documents(docs_to_index)\n\n\n\nAsk questions\n\nWe first need to initialize the pipeline:\n\n\nfrom haystack.pipelines import FAQPipeline\npipe = FAQPipeline(retriever=retriever)\n\n\nfrom haystack.utils import print_answers\n\n# Run any question and change top_k to see more or less answers\nprediction = pipe.run(query=\"How is the virus spreading?\", params={\"Retriever\": {\"top_k\": 1}})\n\nprint_answers(prediction, details=\"medium\")\n\n\n\n\nWARNING:haystack.document_stores.memory:Skipping some of your documents that don't have embeddings. To generate embeddings, run the document store's update_embeddings() method.\n\n\n'Query: How is the virus spreading?'\n'Answers:'\n[]"
  },
  {
    "objectID": "posts/Kaggling_0/Kaggling_Tutorial__0.html",
    "href": "posts/Kaggling_0/Kaggling_Tutorial__0.html",
    "title": "Kaggling Tutorial #0: Download A Dataset from Kaggle Using API-Key",
    "section": "",
    "text": "How To Download Dataset From Kaggle into Colab NoteBook:\n\nIn this mini tutorial we will create a colab notebook and download a Kaggle dataset using Kaggle API-key.\nHaving the possibility to work on different Free computing platform, gives us a wide choices to do multiple prototyping in parallel.\n\n\n\nGetting the API-Key:\n\nFirst we need to get API-Key form Kaggle website: settings:\n\nLink\n\nClick on Create New Token, it will download a json file: kaggle.json\n\nThen we need to upload this file into colab notebook where we will work with the dataset\nHere we create a new directory kaggle and copy the kaggle.json inside:\n\n\n!mkdir ~/.kaggle\n\n\n!cp kaggle.json ~/.kaggle\n\n\nNow we need to modify the permissions and access mode of kaggle.json:\n\n\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n\nDownloading the Dataset:\n\nSince we have all ingredients, we can now download our dataset from kaggle:\n\n\n! kaggle competitions download linking-writing-processes-to-writing-quality\n\nDownloading linking-writing-processes-to-writing-quality.zip to /content\n 90% 97.0M/108M [00:00&lt;00:00, 137MB/s]\n100% 108M/108M [00:00&lt;00:00, 135MB/s] \n\n\n\nThe file we get after this command is .zip file, which contains all files and datasets we will work with\nIn order to unzip this file we will use zipfile:\n\n\nfrom zipfile import ZipFile\nfile = 'linking-writing-processes-to-writing-quality.zip'\nwith ZipFile(file, 'r') as zip:\n\n    # list all the contents of the zip file\n    zip.printdir()\n\n    # extract all files\n    print('extraction...')\n    zip.extractall()\n    print('Done!')\n\nFile Name                                             Modified             Size\nsample_submission.csv                          2023-10-02 17:22:24           48\ntest_logs.csv                                  2023-10-02 17:22:24          398\ntrain_logs.csv                                 2023-10-02 17:22:30    485679766\ntrain_scores.csv                               2023-10-02 17:23:10        32132\nextraction...\nDone!\n\n\n\n\nConclusion:\n\nWe succefully download a dataset from Kaggle using API-Key\nWe can take it from here and apply all kind of data manipulation, EDA, training model on it …"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html",
    "href": "posts/Fastai_ch1/Chapter1.html",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 34.6 MB/s \n     |████████████████████████████████| 1.3 MB 53.8 MB/s \n     |████████████████████████████████| 5.3 MB 53.3 MB/s \n     |████████████████████████████████| 441 kB 65.8 MB/s \n     |████████████████████████████████| 1.6 MB 46.3 MB/s \n     |████████████████████████████████| 212 kB 71.6 MB/s \n     |████████████████████████████████| 115 kB 71.6 MB/s \n     |████████████████████████████████| 163 kB 73.8 MB/s \n     |████████████████████████████████| 127 kB 72.7 MB/s \n     |████████████████████████████████| 115 kB 73.2 MB/s \n     |████████████████████████████████| 7.6 MB 48.1 MB/s \nMounted at /content/gdrive"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "href": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Brief History of Neural Network",
    "text": "Brief History of Neural Network\n\nIn 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, both devoloped a concept called Artificial Neuron which is seen today as the first theorotical demonstration of a machine that’s “..capable of perceiving, recognizing and identifying its surrondings”\n\nIn the 50’s Frank Rossenblatt devoloped a device based on the principiles of Artificial Neuron\n\nRosenblatt invented The Perceptron which was capable of recognizing simple shapes and patterns\n\n\nIn 1969 Marvin minsky write a book called Perceptrons where he showed that Perceptron limits of solving critical math problem\n\n\nParallel Distributed Processing (PDP)\n\nAfter a long winter of Deep Learning, a group of researchers at MIT released a papper in 1986 released the most influencial papper in history of Neural Network\n\nAuthors claimed that PDP approach was closer to how human brain works\n\nPDP require some enviroments elements:\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\n1980’s and 90’s\n\nDuring this epoch researcher started to build models with 2 layers of neurons\n\nWe saw real world application of Deep Learning"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "href": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Top to bottom learning approach",
    "text": "Top to bottom learning approach\n\nStart with the end point\n\nBuild a foundation of intuition through application then build on it with theory\nShow students how individuals pieces of theory are combined in a real-world application\n\nTeach through examples\n\nProvide a context and a purpose for abstract concepts\n\n\n\nWhat is Machine Learning?\nTraditional Programming:\n- It’s hard to explicitly code hard tasks specialy when you don’t know the exact steps\n\n\n\na-traditional-program.png\n\n\nMachine Learning\n- A program that need to be showed examples of the desired tasks so it can Learn from them\n- Demand sufficient amount of examples(Data)\n- In 1949 Arthur Samuel manage to build a machine that can play checker - This work introduced multiple concepts to the world of machine learning: - The idea of a weight assignment\n- The fact that every weight assignment has some actual performance\n- The requirement that there be an automatic means of testing that performance\n- The need for a mechanism (i.e., another automatic process) for improving the performance by changing the weight assignments\n- Update the weight values based on the performance with the current values\n\n\n\nprogram-using-weight-assignments.png\n\n\n\nA system that used Weight Assignment\n\n &gt;Training Machine Learning model with help of weights updating\n &gt;Using the trained model as program\nNow Let’s Build Our First Model!"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "href": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "First Deep Learning Model: It’s a Bird!",
    "text": "First Deep Learning Model: It’s a Bird!\n\nImport Dependencies\n\n#Import fastai library\n#Import fastai computer vision library \nfrom fastbook import *\nfrom fastai.vision.all import *\n\n\n\nStart working with Data\n\n# Here we use search_images_ddg in order to download an url from ddg\n# search engine accornding to the keyword we choose: birds photo\nurls = search_images_ddg('birds photo', max_images=1)\nlen(urls), urls[0]\n\n(1,\n 'https://www.wallpapergeeks.com/wp-content/uploads/2014/02/Colorful-Bird-Perched-Wallpaper.jpg')\n\n\n\n# download the photo from the url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nPath('bird.jpg')\n\n\n\n# then show it\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\nNow we will do the same etapes we did with bird.jpg image but with forest picture\n\n# Download the url, download the image then open it\ndownload_url(search_images_ddg('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\nThe idea here is to build a model that can classify pictures of birds and forests accuratly, but first we need to build the Dataset\n\n# create 2 directories, one for birds and other for forest, then do the same etapes we did earlier,\n# download the urls then the images\nsearches = 'forest','bird'\npath = Path('bird_or_not')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images_ddg(f'{o} photo'))\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSince we are dowlnloading images from the web, there is a chance that some of them are corrupted, so we need to clean the dataset from them\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nNow we have two directories :\n\nbirds\nforests\n\nAs we saw before, machine learning model needs to be feed many examples in order to learn from them.\nThese examples are the images of birds and forests in our case\nWe need to tell the model bunch of informations about our data before the training process, this will be done with help of DataBlock\n\n\n\nCreating DataBlock and DataLoaders\nNow we need to build DataBlock in order to feed the dataset to the model for the training.it’s basically set of rules of how to organize the dataset for the model.\nI will write a BlogPost about this concept later\nThere’s also the concept of DataLoaders which is an iterator class that load data to the model according to the set of rules that we set earlier while creating DataBlock\n\n# create the dataBlock\n# tell the model what kind of data we'r dealing with\n# how to get the data\n# how to create validation and training set\n# how to get the labels\n# how to set tranformers(items in this case)\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\n\nTraining the Model\nNow we are ready to train our model and see if we could classify photo of birds and forests\nThe model we will use here is resenet18 which is a famous model that’s used widely among computer vision classification problem, 18 stands for how many layers does the model have.\nfastai comes with fine_tune() which uses the best practices of fine tuning process.\n\nlearn =  vision_learner(dls, resnet18, metrics= error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.550929\n2.057105\n0.447368\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.084728\n0.225560\n0.092105\n00:02\n\n\n1\n0.045721\n0.024148\n0.000000\n00:02\n\n\n2\n0.031065\n0.012299\n0.000000\n00:02\n\n\n\n\n\n\nWith help of fastai library, and less than 20 lines of code we managed to build a model that can predict images of birds and forests with accuracy of 100%!\n\n\n\nFine Tuning\nis a process where we start with a model that has already be trained and we use it on our problem and on our dataset, this operation needs to adapt the pre-trained model by a bit so it fit our system\nThis approach can save us money and time, all we need to do is adapt the pre-trained model and take advantage of set of weights and use them in our problem\n\n\n\nLearner\nIn Fastai we have the Learner which takes 2 things(For now!): firsly the Dataset, and secondly the Model.\nBecause Deep Learning is more mature now,there’s a small group of Architecture that can be used in nearly any problem, that’s why the actual work of deep learning practionaire is to work on data preparation, and model deploying, more than anything else.\nThis is why Fastai integrated a famous library Timm which collect all SOTA (State Of The Art) computer vision models.\n\n\nVisualizing layers of a trained neural network\n\nIt is possible to inspect deep learning models and get insights from them\n\ncan still be challenging to fully understand\n\nVisualizing and Understanding Convolutional Networks paper\n\npublished by PhD student Matt Zeiler and his supervisor Rob Fergus in 2013\nshowed how to visualize the neural network weights learned in each layer of a model\ndiscovered the early layers in a convolutional neural network recognize edges and simple patterns which are combined in later layers to detect more complex shapes\n\nWe will see this paper in details later, but for now we could show the model architecture.\n\n\n# show the model architecture\nm = learn.model\nm\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)\n\n\n\n\nOther Deep Learning Applications\n\nImage Segmentation\n\ntraining a model to recognize the content of every single pixel in an image\n\n\n\n\n\noutput_47_2.png\n\n\n\nNatural Language Processing (NLP)\n\ngenerate text\ntranslate from one language to another\nanalyze comments\nlabel words in sentences\n\nTabular Data\n\ndata that in in the form of a table\n\nspreedsheets\ndatabases\nComma-separated Values (CSV) files\n\nmodel tries to predict the value of one column based on information in other columns\n\nRecommendation Systems\n\nmodel tries to predict the rating a user would give for something"
  },
  {
    "objectID": "posts/Fastai_ch5/Questionnaire.html",
    "href": "posts/Fastai_ch5/Questionnaire.html",
    "title": "Chapter 5: Questionnaire",
    "section": "",
    "text": "Q1:\nWhy do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\n____\n* Reisizing images is done image per image on cpu, while transformation is done one gpu. This method is called presizing: - crop the image and resize it to 460 by 460 first, this operation is done on CPU. - then we do the data augmentation in batches, by cropping a rotated random part of that 460^2 image, and taking the cropped image then resize again to a 224 by 224 image, all this operation are done on batch level, which mean on GPU\nQ2:\nIf you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book’s website for suggestions.\n___\n* Will write serie of blog posts about Regular Expression\nQ3:\nWhat are the two ways in which data is most commonly provided, for most deep learning datasets?\n___\n* Data is usually provided as : - Individual files like images of text - Comma separted Values csv.\nQ4:\nLook up the documentation for L and try using a few of the new methods that it adds.\n____\n* Later\nQ5:\nLook up the documentation for the Python pathlib module and try using a few methods of the Path class.\n___\n* Later\nQ6:\nGive two examples of ways that image transformations can degrade the quality of the data\n____\n\nInterpolations lead to an image with pixel values that are estimated using estimated pixel values which loses quality each time.\nRotating an image 45 degrees creates empty space in the corners.\n\nQ7:\nWhat method does fastai provide to view the data in a DataLoaders?\n___\nDataLoader.show_batch\nQ8:\nWhat method does fastai provide to help you debug a DataBlock?\n___\nDataBlock.summary\nQ9:\nShould you hold off on training a model until you have thoroughly cleaned your data?\n___\n* No. It’s better to start using the building a model as soon as possible, we could even use it as data cleaning tool.**\nQ10:\nWhat method does fastai provide to help you debug a DataBlock?\n___\n* Plot_Confusion_Matrix: for displaying where the model make bad predictions the most. * Plot_Top_Losses: A method that displays the images with the highest loss value.\nQ11:\nWhat are the two pieces that are combined into cross-entropy loss in PyTorch?\n___\n* Softmax function and Negative Log Likelihood Loss.\nQ12:\nWhat are the two properties of activations that softmax ensures? Why is this important?\n____\n* It make sure all activations add uo to 1 * It help the model to pick one class\nQ13:\nWhen might you want your activations to not have these two properties?\n____\n* When we have multi-label classification, more than one label for one image.\nQ14:\nCalculate the exp and softmax columns of &lt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).\n___\nLater\nQ15:\nWhy can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\n___\n* When we have more than 2 classes to class torch.where it’s not ideal, because it build only to select 1 between 2 categories.\nQ16:\nWhat is the value of log(-2)? Why?\n____\n* The logarithm is the inverse of the exponential function, which mean logarithm of a value is the result of that value after applying the exponential, and there’s no number that can result a nigative number after the exponential, so log(-2) = not defined\nQ17:\nWhat are two good rules of thumb for picking a learning rate from the learning rate finder?\n___\n* Pick Learning rate smaller 10x than the minimum value of loss * Use the learning rate at the last point that the loss value was decreasing.\nQ18:\nWhat two steps does the fine_tune method do?\n____\n* Train the added layer using random weights for one epoch * Unfreeze all the layers and train them all together as normal model for number of epochs\nQ19:\nIn Jupyter notebook, how do you get the source code for a method or function?\n____\n* By using ?? after the function.\nQ20:\nWhat are discriminative learning rates?\n____\n* It’s a method that allow us to use different learning rate for each part of the neural network. Using a pretrained model means that the earlier layers are trained for many epochs which mean that the parameters don’t need to be updated by much, in other hand the last layers needs to be matched with task we have. That’s why it’s good to pick slightly smaller value for learning rate for the earlier layers, and bigger one for the last ones.\nQ21:\nHow is a Python slice object interpreted when passed as a learning rate to fastai?\n____\n* The first value in the slice object sets the lowest learning rate. * The second value in the slice object sets the highest learning rate."
  },
  {
    "objectID": "posts/HuggingFace_1/HuggingFace_NLP_course_Notes.html",
    "href": "posts/HuggingFace_1/HuggingFace_NLP_course_Notes.html",
    "title": "Hugging Face Course Notes: Chapter1",
    "section": "",
    "text": "In this series I wiil cover notes I took from the Hugging Face NLP course with code snippets and examples.\n\n\n\nIn this course we will have 9 Chapters\nFrom chapter 1 to 4 we will cover the main conceptsof **Transformers** library:\n\nHow transformer models works\nHow to use a model from Hugging Face Hub\nHow to fine-tune it on your dataset and share the result\n\nChapter 5 to 8 covers the basics of HF Datasets and Tokenizer"
  },
  {
    "objectID": "posts/HuggingFace_1/HuggingFace_NLP_course_Notes.html#course-overview",
    "href": "posts/HuggingFace_1/HuggingFace_NLP_course_Notes.html#course-overview",
    "title": "Hugging Face Course Notes: Chapter1",
    "section": "",
    "text": "In this course we will have 9 Chapters\nFrom chapter 1 to 4 we will cover the main conceptsof **Transformers** library:\n\nHow transformer models works\nHow to use a model from Hugging Face Hub\nHow to fine-tune it on your dataset and share the result\n\nChapter 5 to 8 covers the basics of HF Datasets and Tokenizer"
  },
  {
    "objectID": "posts/Fastai_ch7/Questionnaire.html",
    "href": "posts/Fastai_ch7/Questionnaire.html",
    "title": "Chapter 7: Questionnaire",
    "section": "",
    "text": "Q1:\nWhat is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?\n* ImageNet is dataset with 1.3 million images and 1000 gategories, while Imagenette is a dataset that represent a small portion of ImageNet with 10 classes. * For studying/devloping ideas/ prototyping we better use a small dataset.\nQ2:\nWhat is normalization?\n* Normalization is a method that get the mean close to 0, and the standar diviation clos to 1 (ideally mean==0, std==1)\nQ3:\nWhy didn’t we have to care about normalization when using a pretrained model?\n* Using pretrained models through vision_learner set the Normalization method automatically.\nQ4:\nWhat is progressive resizing?\n* Progressive resizing is the idea of using small images in the earlier epochs of training phase, then changing the size of the images by a bit and fine tune the model for more epochs, repeat this process till we reach the original size of the image from the dataset.\nQ5:\nWhat is test time augmentation? How do you use it in fastai?\n* Validation set by default uses centre crop for images, which will leads to information lost, TTA addresses this problem by cropping from multiple areas of the image and calculate the predictions of all this crops, then take the average(or the max). preds,targs = learn.tta()\naccuracy(preds, targs).item()\nQ6:\nIs using TTA at inference slower or faster than regular inference? Why?\n* It will take more time than regular inference, because the model calculate the prediction of an image more than once.\nQ7:\nWhat is Mixup? How do you use it in fastai?\n* It’s a data augmenatation method that takes 2 images and mix them together. In fastai mixup used as callback : cbs=Mixup()\nQ10:\nWhat is the idea behind label smoothing?\n* It’s a technique that change the one-hot-encodings value from 0 and 1 to float values, this reduce the overfitting and produce better performance.\nQ11:\nWhen using label smoothing with five categories, what is the target associated with the index 1?\n\nlabels = [0, 1, 0, 0, 0]\nparam = 0.1\n\ndef label_smoothing(labels, param):\n    new_labels = []\n    for label in labels:\n        if label == 1:\n            new_label = 1 - param + param / len(labels)\n            new_labels.append(new_label)\n        else:\n            new_label = param / len(labels)\n            new_labels.append(new_label)\n    return new_labels\n\nlabel_smoothing(labels, param)\n\n[0.02, 0.92, 0.02, 0.02, 0.02]\n\n\nQ12:\nWhat is the first step to take when you want to prototype quick experiments on a new dataset?\n* First do the protoype and experiments, if it takes more than couple of minutes, then we need to consider new subset of that dataset."
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html",
    "href": "posts/Haystack_1/Haystack_1.html",
    "title": "Building QAs System Using Haystack",
    "section": "",
    "text": "Haystack Library is an end-2-end framework that allow us orchesting many components in order to build LLM application with minimum lines of code."
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html#initializing-the-documentstore",
    "href": "posts/Haystack_1/Haystack_1.html#initializing-the-documentstore",
    "title": "Building QAs System Using Haystack",
    "section": "Initializing the DocumentStore:",
    "text": "Initializing the DocumentStore:\n\nIn this tutorial we will use the basic type of the DocumentStore class, which is InMemoryDocumentStore\n\n\nfrom haystack.document_stores import InMemoryDocumentStore\ndocument_store= InMemoryDocumentStore(use_bm25=True)\n\n\nDocumentStore is a like a database or a warehouse that need to be filled with data/documents.\nHere we use the fetch_archive_from_http function to download our documents from the web.\nThe downloaded docements needed to be prepared and organized to be processed in the next step:\n\n\nfrom haystack.utils import fetch_archive_from_http\ndoc_dir='data/directory_project'\nfetch_archive_from_http(url='https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip', output_dir= doc_dir)\n\nTrue\n\n\n\nNow we have all our data stored in this path: data/directory_project and assigned to doc_dir\nAt this momment our data is raw and should be converted in Document objects according the HayStack standars. In order to do that we will use TextIndexingPipeline and write them into DocumentStore\n\n\nimport os\nfrom haystack.pipelines.standard_pipelines import TextIndexingPipeline\n\nfiles_to_index = [doc_dir + '/' + f for f in os.listdir(doc_dir)]\nindexing_pipeline = TextIndexingPipeline(document_store)\nindexing_pipeline.run_batch(file_paths=files_to_index)"
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html#initializing-a-retriever",
    "href": "posts/Haystack_1/Haystack_1.html#initializing-a-retriever",
    "title": "Building QAs System Using Haystack",
    "section": "Initializing A Retriever:",
    "text": "Initializing A Retriever:\n\nA retriever will map through all documents we stored and find the more likely dosuments that contains possible answer to our question\nHere we initialize the BM25Retriever algorithm and make it access the InMemoryDocumentStore\n\n\nfrom haystack.nodes import BM25Retriever\nretriever= BM25Retriever(document_store=document_store)"
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html#initializing-the-reader",
    "href": "posts/Haystack_1/Haystack_1.html#initializing-the-reader",
    "title": "Building QAs System Using Haystack",
    "section": "Initializing the Reader:",
    "text": "Initializing the Reader:\n\nThe Reader get access to all texts from Retriver and extracts candidates answers\nReader is based on LLM’s\nIn this turorial we used the roberta-base-squad2 model\n\n\nfrom haystack.nodes import FARMReader\nreader = FARMReader(model_name_or_path='deepset/roberta-base-squad2', use_gpu=True)"
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html#creating-the-retriver-reader-pipeline",
    "href": "posts/Haystack_1/Haystack_1.html#creating-the-retriver-reader-pipeline",
    "title": "Building QAs System Using Haystack",
    "section": "Creating the Retriver-Reader Pipeline:",
    "text": "Creating the Retriver-Reader Pipeline:\n\nSince we have everything we need to build this Q/A system nom, all we have to do is put every piece together in one Pipeline\n\n\nfrom haystack.pipelines import ExtractiveQAPipeline\npipe = ExtractiveQAPipeline(reader, retriever)"
  },
  {
    "objectID": "posts/Haystack_1/Haystack_1.html#asking-questions",
    "href": "posts/Haystack_1/Haystack_1.html#asking-questions",
    "title": "Building QAs System Using Haystack",
    "section": "Asking Questions:",
    "text": "Asking Questions:\n\npredictions = pipe.run(query='who is the most powerful creature?', params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n\nInferencing Samples: 100%|██████████| 1/1 [00:03&lt;00:00,  3.66s/ Batches]\n\n\n\nfrom haystack.utils import print_answers\nprint_answers(predictions, details='minimum')\n\n'Query: who is the most powerful creature?'\n'Answers:'\n[   {   'answer': 'Khal Drogo',\n        'context': '\\n'\n                   '\\n'\n                   \"'''Khal Drogo''' is a fictional character in the ''A Song \"\n                   \"of Ice and Fire'' series of fantasy novels by American \"\n                   'author George R. R. Martin and in t'},\n    {   'answer': 'Night King',\n        'context': '\\n'\n                   '\\n'\n                   \"The '''Night King''' is a fictional character appearing in \"\n                   \"the HBO high fantasy television series ''Game of \"\n                   \"Thrones'', based on George R. R. Martin'\"},\n    {   'answer': 'Drogo',\n        'context': ' prove to be fundamental to her growth as both a ruler and '\n                   'a conqueror.\\n'\n                   '\\n'\n                   'Drogo is portrayed by Jason Momoa in the HBO television '\n                   'adaptation.\\n'\n                   '\\n'\n                   '==Overvi'},\n    {   'answer': 'Jon Snow',\n        'context': 'rge and sinewy man that towers over others, such as Davos '\n                   'Seaworth and Jon Snow, a Baratheon trait. He lacks the '\n                   'long black hair of his brothers, and '},\n    {   'answer': 'Night King',\n        'context': 'urdik in seasons 6 to 8.\\n'\n                   '\\n'\n                   '==Description==\\n'\n                   \"In ''Game of Thrones'', the Night King is physically \"\n                   'distinguished from the other White Walkers by his \"crow'}]"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html",
    "href": "posts/Fastai_ch4/Ch4.html",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 27.5 MB/s \n     |████████████████████████████████| 5.3 MB 55.1 MB/s \n     |████████████████████████████████| 441 kB 70.9 MB/s \n     |████████████████████████████████| 1.3 MB 56.3 MB/s \n     |████████████████████████████████| 1.6 MB 57.3 MB/s \n     |████████████████████████████████| 115 kB 72.1 MB/s \n     |████████████████████████████████| 163 kB 71.5 MB/s \n     |████████████████████████████████| 212 kB 55.8 MB/s \n     |████████████████████████████████| 127 kB 75.4 MB/s \n     |████████████████████████████████| 115 kB 75.4 MB/s \n     |████████████████████████████████| 7.6 MB 56.7 MB/s \nMounted at /content/gdrive\nfrom fastbook import *\nfrom fastai.vision.widgets import *\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/Fastai_ch4/Ch4.html#pixels-the-foundations-of-computer-vision",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\nComputer are good with numbers, thats why in order to make them do computer vision tasks we need to turn images to series of numbers\nWe will use a small version of the famous dataset MNIST which contains only 2 digits 3 and 7.\nOur task here is create a Neural Network from scratch that can calssify 3 from 7.\n\n\n#download the dataset;\n#MNIST_simple is a small mnist that contains only 7s and 3s\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:01&lt;00:00]\n    \n    \n\n\n\n#here is where the dataset is stored\npath\n\nPath('/root/.fastai/data/mnist_sample')\n\n\n\n# we can use ls() to investigate the dataset\n# we have 3 directories: train, valid, labels.csv\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\n\n#inside of train/valid folders, there's is 2 folders: 7, 3\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\n\n#let's have a look at what in those folders while we sorted them an put them into variables ('threes and sevens')\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\n\n\nthrees[-1], threes[22]\n\n(Path('/root/.fastai/data/mnist_sample/train/3/9991.png'),\n Path('/root/.fastai/data/mnist_sample/train/3/10210.png'))\n\n\n\n#let's open it \nimg_path = threes[1]\nimg = Image.open(img_path)\nimg\n\n\n\n\n\n\n\n\n\nIn a computer, everything is represented as a numbers.\nTo view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor.\n\n\n# here we use numpy array to represent that image \"img\" as array (matrix) \n# the img here is represented as pixels\n# the darker pixel are 0 or have values colser to 0, and the lighter pixels have higher values\narray(img)[4:10, 4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n#here we use Pytorch tensor which is the numpy array function of pytorch\n#they have the same code, and the behave basically the same (in most cases), \n#the main difference is tensors can be computed on GPu\na =tensor(img)[4:10, 4:10]\na\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\nWe ciuld convert a image into array/tensor, then represent it as pandas DataFrame by coloring each pixel using background_gradient(), the darker the pixel is the closer it is to the highest value 252, this the image is the closer to 0.\n\n\n# here we demonstrate the image and pixels values\n# values of each pixel varies between 0 and 255\nimg_t= tensor(img)\ndf = pd.DataFrame(img_t)\ndf.style.set_properties(**{'font-size':'4pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\n0\n0\n0\n0\n0\n0\n\n\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n51\n119\n253\n253\n253\n76\n0\n0\n0\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n183\n253\n253\n139\n0\n0\n0\n0\n0\n0\n0\n\n\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n182\n253\n253\n104\n0\n0\n0\n0\n0\n0\n0\n\n\n18\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n249\n253\n253\n36\n0\n0\n0\n0\n0\n0\n0\n\n\n19\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n60\n214\n253\n253\n173\n11\n0\n0\n0\n0\n0\n0\n0\n\n\n20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n98\n247\n253\n253\n226\n9\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n42\n150\n252\n253\n253\n233\n53\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n22\n0\n0\n0\n0\n0\n0\n42\n115\n42\n60\n115\n159\n240\n253\n253\n250\n175\n25\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n23\n0\n0\n0\n0\n0\n0\n187\n253\n253\n253\n253\n253\n253\n253\n197\n86\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n24\n0\n0\n0\n0\n0\n0\n103\n253\n253\n253\n253\n253\n232\n67\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n25\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n26\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n27\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#base-line-model",
    "href": "posts/Fastai_ch4/Ch4.html#base-line-model",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Base-Line Model",
    "text": "Base-Line Model\n\nIs always good to start with a base-line model then try to build something more comlex on top of it\nBase-line model help us build intuition and understand the prespectives of the problem\nConsum less time to build\n\n\nPixel Similarity\n\nCalculate the average values for each pixel location across all images for each digit\n\nThis will generate a blurry image of the target digit\n\nCompare the values for each pixel location in a new image to the average\n\n\n# store all images of 7s and 3s as a list of tensors\nsevens_tensors = [tensor(Image.open(o)) for o in sevens]\nthrees_tensors = [tensor(Image.open(o)) for o in threes]\nlen(sevens_tensors), len(threes_tensors)\n\n(6265, 6131)\n\n\n\nso now we have bunch of tensors, since they’re not image object anymore, we will use show_image of fast ai instead of PILimage\nRemember we can always use show_image?? to read the documentation/ the code\n\n\n# show a image from the tensor list\nshow_image(threes_tensors[330]);\n\n\n\n\n\n\n\n\n\nNow we need to compute the value of each pixel in respect to all images (for that digit)\nPut all images in a list of 3 dimension tensors, then stack them into a single tensor.\n\nstack images via pytorch function torch.stack\nand scale pixel values from the range [0,255] to [0,1]\n\n\n\nstacked_threes= torch.stack(threes_tensors).float()/255\nstacked_sevens= torch.stack(sevens_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\n.shape attribute tells us about the lenth of each axis\n\nin this case we can see we have 6131 images, each of size 28*28 pixel\n\nCalculate the mean values for each pixel location across all images\n\n\nmean3 = stacked_threes.mean(0)\nmean7 = stacked_sevens.mean(0)\n\n\nSow our ideal 3 and 7\n\n\nshow_image(mean3)\nshow_image(mean7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#pick a single 3, 7 to compare it with the ideal one\na_3 = stacked_threes[55]\na_7 = stacked_sevens[29]\n\n\nshow_image(a_3)\nshow_image(a_7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso now let’s say that we want to recognize if a_3 is a 3 or 7?\nto do that we can measure the distance between either of the two ideal 3 and 7**\n\nbecause just compunting the difference canot give us the right answear always since in some cases the difference will be negative!\n\n\nto avoide that we can take 2 approaches:\n* Take the mean of the absolute value of differences, this method called L1 norm or mean absolute difference .\n* Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring), this method called L2 norm or root mean squared error (RMSE)\n\n# Let's try both\ndis_3_abs = (a_3-mean3).abs().mean()\ndis_3_sqr = ((a_3-mean3)**2).mean().sqrt()\ndis_3_abs, dis_3_sqr\n\n(tensor(0.1280), tensor(0.2356))\n\n\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1832), tensor(0.3356))\n\n\n\nbased on those numbers the a_3 seems to be close to the ideal three than a_7to the ideal seven which seems to be right\nIn pytorch there’s a function that represent all of that for us:\n\ntorch.nn.functional, wich is recommended to be called as F(in fastai this recommendation is standarized!).\nl1_loss stand for l1 norm, and mse_loss stand for l2 norm\n\nmse_loss still need sqrt() to fully executed\n\n\n\n\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()\n\n(tensor(0.1832), tensor(0.3356))\n\n\n\n\nComputing metrics using Broadcasting\n\nIf we want to test the accuracy of a model we better measure it on validation set\nso let’s create a tensor of 3’s and 7’s of validation set directory, then calculate the accuracy of our “model” based on every tensor(image) in validation set\n\n\n#create tensors from image in validation set, then stack all image together\nvalid_ten_3 = torch.stack([tensor(Image.open(o)) \n                           for o in (path/'valid'/'3').ls()])\n#turn them into float and devide them by 255\nvalid_ten_3 = valid_ten_3.float()/255\nvalid_ten_7 = torch.stack([tensor(Image.open(o))\n                           for o in (path/'valid'/'7').ls()])\nvalid_ten_7 = valid_ten_7.float()/255\nvalid_ten_3.shape,valid_ten_7.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\n#create a function that will calculate the mean absolute error\ndef min_abser(a, b): return (a-b).abs().mean((-1,-2))\nmin_abser(valid_ten_3[345], mean3)\n\ntensor(0.1123)\n\n\n\nBut this is only the absolut error with one image. Now we need to calculate this distance between the ideal 3/7 with all image in validation set in order to evaluate our model.\nThe easy method that will pop up in our mind in order to do that is to loop over all images in validation set and use the function min_abser() to calulate the difference over all of those images\nThe better way, is to use a method called Broadcasting which is a way of extending a tensor to match to others in order to do calulations\n\n\n#example of broadcasting:\ntensor([2, 3, 4])+tensor(-2)\n\ntensor([0, 1, 2])\n\n\n\npytorch execute a calculation between 2 tensors with different ranks(dimension), we will take adventage of that method in order to do same with our case\n\n\n# shape of mean3/7 and the shape of validation set tensor\nmean3.shape, valid_ten_3.shape\n\n(torch.Size([28, 28]), torch.Size([1010, 28, 28]))\n\n\n\nThere’s 1010 image and we want to compare that ideal image mean3 against\nis_3() decide whether its 3 or 7 by copmuting which output of min_abser() is smaller\n\n\ndef is_3(x): return min_abser(x,mean3) &lt; min_abser(x,mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\nNote that when we convert the Boolean response to a float, we get 1.0 for True and 0.0 for False. Thanks to broadcasting, we can also test it on the full validation set of 3s:\n\n\nis_3(valid_ten_3)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\n\nNow we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:\n\n\naccuracy_3s =      is_3(valid_ten_3).float() .mean()\naccuracy_7s = (1 - is_3(valid_ten_7).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\n\nWe’re getting +90% accuracy just by doing base-line model!!\nThe base-Line model is good for understanding the problem and building the intuition, but we didn’t build a deep learning model yet.\n\nwe did not add the Learning fase to our model, which is a crucial part according to Arthur Samuel definition\n\nIn other words, this model cannot be updated and improved by learning\n\nwe can’t improve pixel similarity approach by modifying set of parameters, because we don’t have one! and that’s why we need SGD"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#stochastic-gradient-descent",
    "href": "posts/Fastai_ch4/Ch4.html#stochastic-gradient-descent",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nInstead of trying to find similarity between an image and the “ideal image” we could add set of weights to each pixel, such as the heighest are associated with darker pixel and lowest are more likely to be white.\n\nfor example the pixel bottom right are white in 7s, so we will give them low weights\n\nThis can be represented as a function and set of weight values for each possible digit:\ndef pr_eight(x,w): return (x*w).sum()\n\nx is the image we’re predicting which will be represented as vector\nw is the weights of the image, also vector\n\nThe idea to find a method by which we update the weights, such as it can help us to make to predict better by a bit, then repeat those steps many times til we get the best prediction we can get.\n\nfind the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not.\n\nHere are the steps that we are going to require, to turn this function into a machine learning classifier:\n\n\nInitialize the weights.\n\n\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\n\n\nBased on these predictions, calculate how good the model is (its loss).\n\n\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\n\n\nStep (that is, change) all the weights based on that calculation.\n\n\nGo back to the step 2, and repeat the process.\n\n\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer). \n\n\n\n\nApllying SGD on a simple case\n\nBefore applying these steps to our image classification problem, let’s illustrate what they look like in a simpler case.\nFirst we will define a very simple function f, the quadratic—let’s pretend that this is our loss function, and x is a weight parameter of the function:\n\n\ndef f(x): return x**2\n\n\nplot_function(f, 'x', 'x**2')\n\n\n\n\n\n\n\n\nLet’s pick a rondom value for the paramter, and calculating the loss\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\n\n\n\n\n\n\n\nnow we will increase/decrease the parameter value by just a bit(0.5)and see what will happen:\n\n\nplot_function(f,'x','x**2')\nplt.scatter(-1.5, f(-1.5), color='red' );\nplt.scatter(-1,f(-1), color='blue');\n\n\n\n\n\n\n\n\n\nIt seem that our loss get better(remember, the whole objective of this process is to minimize the loss to 0), we better keep increase the paramter but this time by (0.3), then(0.3):\n\n\nplot_function(f,'x','x**2')\nplt.scatter(-1.5, f(-1.5), color='red' );\nplt.scatter(-1,f(-1), color='blue');\nplt.scatter(-0.7,f(-0.7), color='green');\nplt.scatter(-0.5, f(-0.5), color='orange');\n\n\n\n\n\n\n\n\n\nThe main idea here is to adjust the paramters in a way that causes a minimal loss \nWe can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:\n\n\n\n\nScreenshot 2022-09-13 at 14-25-28 04_mnist_basics - Jupyter Notebook.png\n\n\n\n\nCalculating Gradients\n\nIt’s obvious now that in order to optimize the loss function we need to update the weights.\n\nto do that we need the help of calculus, it will allow us to update the weights in the direction that optimize the loss\n\n\n\n\nCalculating derivative on Pytorch\n\n#let's assume this variable\ndef f(x): return x**2\nxt = tensor(3.).requires_grad_()\n\n\nhere we create a tensor at value 3 the we call the method requires_grad_ which will calculate the gradients in respect to that variable at that value.\n\n\n# add the function f(x)=x**2 as a Y\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\n\n# now we calculate the gradients using the backward() \nyt.backward()\n\n\n#now we can views the gradients by checking the `grad` attribute of the tensor:\nxt.grad\n\ntensor(6.)\n\n\n\n#now we will repeat the same steps but now with a vector:\nxt = tensor([3.,4.,10.]).requires_grad_()\n\n\ndef f(x): return (x**2).sum()\n\n\n#let's check our function\nf(xt)\n\ntensor(125., grad_fn=&lt;SumBackward0&gt;)\n\n\n\nyt = f(xt)\n\n\n#calculate the gradients\nyt.backward()\n\n\n#as expected 2xt\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\n\n#let's create another example\nx= tensor(3.).requires_grad_()\nw= tensor(4.).requires_grad_()\nb= tensor(5.).requires_grad_()\ny = x * w + b\ny\n\ntensor(17., grad_fn=&lt;AddBackward0&gt;)\n\n\n\ny.backward()\n\n\nprint('d(y)/dx= ', x.grad)\nprint('d(y)/dw= ', w.grad)\nprint('d(y)/bx= ', b.grad)\n\nd(y)/dx=  tensor(4.)\nd(y)/dw=  tensor(3.)\nd(y)/bx=  tensor(1.)\n\n\n\nNow we have all the ingredients to apply what we have learned on a real problem.\n\nthe goal here is to apply these 7 steps we saw in order to optimize the weights which will effect the loss, which also effect the accuracy of the model\n\none last thing before we do that, we will talk about Learnin rate\nGradient descent allow us to correct our weights by taking steps toward the optimal value of these weights, but it didn’t tell us how big or small these steps are, thats why we have to initilize a learning rate.\nLearning rate is a value, by which our gradient calculate new weights in order to get better loss.\nIn general the learning rate is some randome value we chase between 0.1 and 0.001\nOnce we have picked a learning rate we then adjust the parameters using this simple function:\n\nw-=gradient(w)*lr\n\n\nAn End-to-End SGD Example\n\nSuppose we want to predict the speed of a Roller Coaster over a hump.\nWe want to build a model that predict how the speed change over time\n\nwe measure the speed of the 20 seconds manualy/ every second:\n\n\n\ntime = torch.arange(20).float();\ntime\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n\n\n\nIt might look something like this:\n\n\n# we add some noise to the data since this is the way we usualy find data in real world\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\n\n\n\n\n\n\n\nThe goal here is to find a function that matches our observastions using SGD\nWe chose here a quadratic function!.\n\nwe need to distinguish clearly between the input of the function t (the that’s the product of our observation) and it’s parameters params\n\n\n\n#let's just assume this  quadratic function!!!\n\ndef f(t, params):\n    a, b, c= params\n    return a*(t**2)+(b*t)+c\n\n\nThe we need to set a loss function that will tell us how good or bad our prediction of the parameters a, b, c\n\n\ndef mse(preds, targets): return((preds - targets)**2).mean().sqrt()\n\n\n\nStep 1: Initialize the parameters\n\nfirst we need to initialize the parametrs by telling pytorch that we want to track its gradients\n\n\nparams= torch.randn(3).requires_grad_()\nparams\n\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n\n#??\norig_params = params.clone()\n\n\n\nStep 2: Calculate the predictions\n\nNext we calculate the prediction\n\n\npreds = f(time, params)\npreds\n\ntensor([ 1.3525e+00, -1.6391e-01, -3.2121e+00, -7.7919e+00, -1.3903e+01, -2.1547e+01, -3.0721e+01, -4.1428e+01, -5.3666e+01, -6.7436e+01, -8.2738e+01, -9.9571e+01, -1.1794e+02, -1.3783e+02,\n        -1.5926e+02, -1.8222e+02, -2.0671e+02, -2.3274e+02, -2.6029e+02, -2.8938e+02], grad_fn=&lt;AddBackward0&gt;)\n\n\nThe pred here are the function we create earlier. * f(t, params): am bm c= params return a*(t**2)+(b*t)+c - where the t==time, and params are the initialized values we took in the step before, so the f will be calculated by taking those values as a part of it\n\n# plot the actual calues and our prediction \ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\n\nThe blue dot represent the actal value\nThe red dots represent what our model\n\npredictions are really bad, but we need to remember that these prediction are based on Random Values\n\nSo our job here is to update these values to get better predictions\n\n\n\nStep 3: Calculate the loss\n\nWe calculate now the loss of our prediction\nRemember:\n\ndef mse(preds, targets): return((preds - targets)**2).mean().sqrt()\n\nloss= mse(preds, speed)\nloss\n\ntensor(160.6979, grad_fn=&lt;SqrtBackward0&gt;)\n\n\n\nOur goal is to improve this loss function by minimizing it, to be as close as possible to the target(speed)\n\n\n\nStep 4: Calculate the gradients\n\nIn order to minimize the loss function we use the gradients to aproximate how the parameters can be changed to achieve our goal\n\n\nloss.backward()\nparams.grad\n\ntensor([-165.5151,  -10.6402,   -0.7900])\n\n\n\nparams.grad\n\ntensor([-165.5151,  -10.6402,   -0.7900])\n\n\n\nparams\n\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n\n\nStep 5: Step the weights\n\nWe need to updated the weights(paramaters) we just calculated\n\nfirst we need to fix the learning rate\nthen we need to call .data on the parameters and parameters.grad, it’s like telling pytorch to not track the change in the gradients at this point.\n\n\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n\nLet’s see if the loss has improved:\n\n\npreds = f(time,params)\nmse(preds, speed)\n\ntensor(160.4228, grad_fn=&lt;SqrtBackward0&gt;)\n\n\n\nwhat’s about the plot\n\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\n\nNow let’s execute all steps in one function\n\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n\nLet’s execute this steps for 100s of times\n\n\nfor i in range (1200): apply_step(params)\n\n160.42279052734375\n160.14772033691406\n159.87269592285156\n159.59768676757812\n159.3227081298828\n159.04774475097656\n158.7728271484375\n158.4979248046875\n158.22305297851562\n157.9481964111328\n157.67337036132812\n157.39857482910156\n157.12380981445312\n156.84906005859375\n156.5743408203125\n156.29966735839844\n156.02499389648438\n155.75035095214844\n155.4757537841797\n155.20118713378906\n154.92662048339844\n154.65211486816406\n154.37762451171875\n154.1031494140625\n153.82872009277344\n153.55430603027344\n153.27992248535156\n153.0055694580078\n152.73126220703125\n152.4569549560547\n152.18270874023438\n151.90847778320312\n151.63426208496094\n151.36009216308594\n151.08595275878906\n150.81185913085938\n150.5377655029297\n150.26370239257812\n149.9897003173828\n149.71571350097656\n149.44175720214844\n149.16783142089844\n148.89393615722656\n148.6200714111328\n148.3462371826172\n148.0724334716797\n147.79867553710938\n147.52493286132812\n147.25123596191406\n146.97756958007812\n146.7039337158203\n146.43032836914062\n146.15676879882812\n145.88323974609375\n145.60971069335938\n145.3362579345703\n145.06283569335938\n144.78941345214844\n144.51605224609375\n144.24273681640625\n143.96942138671875\n143.6961669921875\n143.4229278564453\n143.14974975585938\n142.87661743164062\n142.6034698486328\n142.3303985595703\n142.05734252929688\n141.78433227539062\n141.5113525390625\n141.23841857910156\n140.96554565429688\n140.69265747070312\n140.41983032226562\n140.14703369140625\n139.87429809570312\n139.60157775878906\n139.3289031982422\n139.05625915527344\n138.78367614746094\n138.51112365722656\n138.2386016845703\n137.96612548828125\n137.69369506835938\n137.42127990722656\n137.14894104003906\n136.87660217285156\n136.60433959960938\n136.33209228515625\n136.0598907470703\n135.7877197265625\n135.515625\n135.2435302734375\n134.9715118408203\n134.69952392578125\n134.42759704589844\n134.1556854248047\n133.88381958007812\n133.61199951171875\n133.3402099609375\n133.06849670410156\n132.79681396484375\n132.52517700195312\n132.25357055664062\n131.98204040527344\n131.71051025390625\n131.43905639648438\n131.16763305664062\n130.89627075195312\n130.62493896484375\n130.35366821289062\n130.08242797851562\n129.81126403808594\n129.54013061523438\n129.26902770996094\n128.99798583984375\n128.72698974609375\n128.4560546875\n128.18515014648438\n127.91431427001953\n127.64350891113281\n127.37277221679688\n127.1020736694336\n126.83142852783203\n126.56082916259766\n126.29029083251953\n126.01979064941406\n125.74934387207031\n125.47895812988281\n125.2086410522461\n124.93834686279297\n124.6681137084961\n124.39794921875\n124.12782287597656\n123.85773468017578\n123.58772277832031\n123.31776428222656\n123.04785919189453\n122.77799987792969\n122.5082015991211\n122.23847198486328\n121.96878814697266\n121.69915771484375\n121.42959594726562\n121.16007995605469\n120.89063262939453\n120.6212387084961\n120.35188293457031\n120.08260345458984\n119.81338500976562\n119.54422760009766\n119.27513122558594\n119.00609588623047\n118.73711395263672\n118.46820831298828\n118.19935607910156\n117.93054962158203\n117.66181182861328\n117.39315795898438\n117.12455749511719\n116.85600280761719\n116.5875473022461\n116.31912994384766\n116.05079650878906\n115.78250122070312\n115.51428985595703\n115.24613952636719\n114.97806549072266\n114.71004486083984\n114.44210052490234\n114.17422485351562\n113.90641784667969\n113.63868713378906\n113.37100982666016\n113.10340118408203\n112.83587646484375\n112.56842041015625\n112.30104064941406\n112.03372192382812\n111.7664794921875\n111.49931335449219\n111.23222351074219\n110.96520233154297\n110.6982421875\n110.43138122558594\n110.16458129882812\n109.89786529541016\n109.63121795654297\n109.36465454101562\n109.09815979003906\n108.83175659179688\n108.56542205810547\n108.2991714477539\n108.03300476074219\n107.76690673828125\n107.50091552734375\n107.23497772216797\n106.96913146972656\n106.70336151123047\n106.43769073486328\n106.17208862304688\n105.90658569335938\n105.64115142822266\n105.37582397460938\n105.1105728149414\n104.84541320800781\n104.58033752441406\n104.31534576416016\n104.05045318603516\n103.78563690185547\n103.52091979980469\n103.25628662109375\n102.99176025390625\n102.72731018066406\n102.46295928955078\n102.1987075805664\n101.9345474243164\n101.67047119140625\n101.40650177001953\n101.14262390136719\n100.87885284423828\n100.61516571044922\n100.35159301757812\n100.0881118774414\n99.8247299194336\n99.56144714355469\n99.29827117919922\n99.03519439697266\n98.772216796875\n98.50934600830078\n98.24658966064453\n97.98394012451172\n97.72138977050781\n97.45895385742188\n97.19661712646484\n96.93439483642578\n96.67227172851562\n96.41026306152344\n96.14837646484375\n95.88658905029297\n95.62493133544922\n95.3633804321289\n95.1019287109375\n94.84060668945312\n94.57940673828125\n94.31830596923828\n94.0573501586914\n93.79650115966797\n93.5357666015625\n93.27516174316406\n93.0146713256836\n92.75431823730469\n92.49407958984375\n92.23397827148438\n91.97398376464844\n91.7141342163086\n91.45439910888672\n91.1948013305664\n90.93533325195312\n90.67599487304688\n90.41679382324219\n90.15772247314453\n89.89878845214844\n89.63999938964844\n89.3813247680664\n89.122802734375\n88.86441802978516\n88.60618591308594\n88.34807586669922\n88.09011840820312\n87.8322982788086\n87.57463073730469\n87.31710815429688\n87.05973052978516\n86.80250549316406\n86.54542541503906\n86.28850555419922\n86.03172302246094\n85.77511596679688\n85.51864624023438\n85.26234436035156\n85.00618743896484\n84.75019836425781\n84.49436950683594\n84.23870086669922\n83.98320007324219\n83.72785949707031\n83.4726791381836\n83.21766662597656\n82.96282958984375\n82.70816802978516\n82.45367431640625\n82.19934844970703\n81.94519805908203\n81.69123077392578\n81.43744659423828\n81.18382263183594\n80.93038940429688\n80.67713928222656\n80.42406463623047\n80.17118072509766\n79.91848754882812\n79.66597747802734\n79.41366577148438\n79.16154479980469\n78.90961456298828\n78.65788269042969\n78.4063491821289\n78.15501403808594\n77.90387725830078\n77.65293884277344\n77.4022216796875\n77.15169525146484\n76.90137481689453\n76.65127563476562\n76.40138244628906\n76.15170288085938\n75.9022445678711\n75.65299224853516\n75.40396881103516\n75.15515899658203\n74.90658569335938\n74.6582260131836\n74.41009521484375\n74.16220092773438\n73.9145278930664\n73.6670913696289\n73.4198989868164\n73.17293548583984\n72.92621612548828\n72.67974853515625\n72.43351745605469\n72.18753051757812\n71.94180297851562\n71.69631958007812\n71.45108795166016\n71.20611572265625\n70.9614028930664\n70.71694946289062\n70.47276306152344\n70.22883605957031\n69.98519134521484\n69.74180603027344\n69.49870300292969\n69.25586700439453\n69.01331329345703\n68.77104187011719\n68.52906036376953\n68.28734588623047\n68.04594421386719\n67.80481719970703\n67.56399536132812\n67.3234634399414\n67.08323669433594\n66.84332275390625\n66.60370635986328\n66.36438751220703\n66.12539672851562\n65.88671875\n65.64836120605469\n65.41032409667969\n65.17259979248047\n64.93521881103516\n64.69815063476562\n64.46143341064453\n64.22505187988281\n63.98899841308594\n63.7532958984375\n63.5179328918457\n63.282928466796875\n63.04827117919922\n62.81398010253906\n62.580047607421875\n62.34648132324219\n62.11327362060547\n61.88043975830078\n61.64798355102539\n61.41590118408203\n61.184200286865234\n60.95288848876953\n60.72196960449219\n60.49143981933594\n60.26130294799805\n60.031578063964844\n59.80224609375\n59.573326110839844\n59.34482192993164\n59.116729736328125\n58.889068603515625\n58.66182327270508\n58.43500518798828\n58.2086296081543\n57.98268127441406\n57.757179260253906\n57.532127380371094\n57.307525634765625\n57.0833740234375\n56.85968780517578\n56.6364631652832\n56.413700103759766\n56.19141387939453\n55.969608306884766\n55.74828338623047\n55.52744674682617\n55.30709457397461\n55.087249755859375\n54.867897033691406\n54.649051666259766\n54.430721282958984\n54.21290969848633\n53.995609283447266\n53.77884292602539\n53.56260299682617\n53.34690475463867\n53.131744384765625\n52.91713333129883\n52.703067779541016\n52.489566802978516\n52.2766227722168\n52.06424331665039\n51.852447509765625\n51.641231536865234\n51.43058776855469\n51.220542907714844\n51.01108932495117\n50.802242279052734\n50.593997955322266\n50.38636779785156\n50.179351806640625\n49.972965240478516\n49.7672119140625\n49.56209182739258\n49.35761260986328\n49.153778076171875\n48.95060348510742\n48.74808120727539\n48.54623031616211\n48.345054626464844\n48.14455032348633\n47.944732666015625\n47.745609283447266\n47.547176361083984\n47.34945297241211\n47.152435302734375\n46.95613479614258\n46.76055908203125\n46.565704345703125\n46.3715934753418\n46.17822265625\n45.985599517822266\n45.79372787475586\n45.60261535644531\n45.41227340698242\n45.22270965576172\n45.03392028808594\n44.84592056274414\n44.65871810913086\n44.47231674194336\n44.286712646484375\n44.1019287109375\n43.91796875\n43.734832763671875\n43.552528381347656\n43.37106704711914\n43.190452575683594\n43.01069259643555\n42.83179473876953\n42.65375900268555\n42.476600646972656\n42.30031967163086\n42.12492752075195\n41.9504280090332\n41.77682876586914\n41.6041374206543\n41.43235778808594\n41.26150131225586\n41.09156799316406\n40.92256164550781\n40.75450134277344\n40.5873908996582\n40.42121887207031\n40.256011962890625\n40.091773986816406\n39.92850112915039\n39.766204833984375\n39.60489273071289\n39.4445686340332\n39.285240173339844\n39.12691116333008\n38.96958923339844\n38.81327438354492\n38.65798568725586\n38.50371551513672\n38.35047149658203\n38.19826889038086\n38.04710006713867\n37.896976470947266\n37.74790954589844\n37.59988784790039\n37.45293426513672\n37.30704116821289\n37.1622200012207\n37.01847457885742\n36.87580490112305\n36.73421859741211\n36.593719482421875\n36.454307556152344\n36.31599807739258\n36.17878341674805\n36.04267501831055\n35.90766906738281\n35.77377700805664\n35.640995025634766\n35.509334564208984\n35.378787994384766\n35.24936294555664\n35.121063232421875\n34.993892669677734\n34.86784744262695\n34.74293899536133\n34.619163513183594\n34.49652099609375\n34.37501907348633\n34.2546501159668\n34.13542556762695\n34.01734161376953\n33.90039825439453\n33.78459548950195\n33.66993713378906\n33.55642318725586\n33.44404983520508\n33.332820892333984\n33.22273635864258\n33.113792419433594\n33.00598907470703\n32.89932632446289\n32.79380416870117\n32.68942642211914\n32.58617401123047\n32.484066009521484\n32.38309097290039\n32.28324508666992\n32.18452835083008\n32.086936950683594\n31.99047088623047\n31.89512825012207\n31.800899505615234\n31.70779037475586\n31.61579132080078\n31.5248966217041\n31.435110092163086\n31.346420288085938\n31.25882911682129\n31.17232894897461\n31.086910247802734\n31.002580642700195\n30.919322967529297\n30.83713722229004\n30.756017684936523\n30.675962448120117\n30.596956253051758\n30.51900291442871\n30.442092895507812\n30.36622428894043\n30.291379928588867\n30.217559814453125\n30.14476203918457\n30.07297134399414\n30.00218963623047\n29.93239974975586\n29.863605499267578\n29.79578971862793\n29.72895050048828\n29.6630802154541\n29.598169326782227\n29.534212112426758\n29.4711971282959\n29.409116744995117\n29.347970962524414\n29.287738800048828\n29.228425979614258\n29.170013427734375\n29.11249351501465\n29.055862426757812\n29.000110626220703\n28.945226669311523\n28.891206741333008\n28.83803367614746\n28.78570556640625\n28.734214782714844\n28.683551788330078\n28.63370132446289\n28.584657669067383\n28.53641700744629\n28.488964080810547\n28.442289352416992\n28.396390914916992\n28.35125160217285\n28.306867599487305\n28.263225555419922\n28.220321655273438\n28.178142547607422\n28.136682510375977\n28.095928192138672\n28.055875778198242\n28.016508102416992\n27.977825164794922\n27.9398136138916\n27.902463912963867\n27.865768432617188\n27.829721450805664\n27.794307708740234\n27.759521484375\n27.725353240966797\n27.691795349121094\n27.65884017944336\n27.626476287841797\n27.594696044921875\n27.56348991394043\n27.532852172851562\n27.50277328491211\n27.473243713378906\n27.444255828857422\n27.415802001953125\n27.38787269592285\n27.360462188720703\n27.333559036254883\n27.30715560913086\n27.281248092651367\n27.25582504272461\n27.230878829956055\n27.206403732299805\n27.182390213012695\n27.158830642700195\n27.135719299316406\n27.113048553466797\n27.090810775756836\n27.06899642944336\n27.047603607177734\n27.02661895751953\n27.006038665771484\n26.985857009887695\n26.966068267822266\n26.946659088134766\n26.927631378173828\n26.908971786499023\n26.890676498413086\n26.872737884521484\n26.855154037475586\n26.837913513183594\n26.821012496948242\n26.804445266723633\n26.788204193115234\n26.77228546142578\n26.756681442260742\n26.74138832092285\n26.726398468017578\n26.711708068847656\n26.697311401367188\n26.683198928833008\n26.66937255859375\n26.65582275390625\n26.642541885375977\n26.629533767700195\n26.616785049438477\n26.604291915893555\n26.592052459716797\n26.580062866210938\n26.568313598632812\n26.55680274963379\n26.54552459716797\n26.53447723388672\n26.52365493774414\n26.513051986694336\n26.502666473388672\n26.492490768432617\n26.482528686523438\n26.472766876220703\n26.46320343017578\n26.453842163085938\n26.44466781616211\n26.43568229675293\n26.4268856048584\n26.41826820373535\n26.409826278686523\n26.401559829711914\n26.39346694946289\n26.38553810119629\n26.377775192260742\n26.37017250061035\n26.362730026245117\n26.355438232421875\n26.348299026489258\n26.341310501098633\n26.33446502685547\n26.327762603759766\n26.32120132446289\n26.314775466918945\n26.30848503112793\n26.302322387695312\n26.29629135131836\n26.29038429260254\n26.28460121154785\n26.27894401550293\n26.27340316772461\n26.267974853515625\n26.262662887573242\n26.257463455200195\n26.252370834350586\n26.247386932373047\n26.242507934570312\n26.23773193359375\n26.233055114746094\n26.228477478027344\n26.223997116088867\n26.219608306884766\n26.215314865112305\n26.211111068725586\n26.20699691772461\n26.20296859741211\n26.19902801513672\n26.195167541503906\n26.191389083862305\n26.18769073486328\n26.184070587158203\n26.18052864074707\n26.177061080932617\n26.173667907714844\n26.17034339904785\n26.16709327697754\n26.163909912109375\n26.16079330444336\n26.157745361328125\n26.154760360717773\n26.151840209960938\n26.14898109436035\n26.146183013916016\n26.14344596862793\n26.140766143798828\n26.13814353942871\n26.135576248168945\n26.133060455322266\n26.130603790283203\n26.128196716308594\n26.125839233398438\n26.123537063598633\n26.121280670166016\n26.11907196044922\n26.11690902709961\n26.114797592163086\n26.11272430419922\n26.110698699951172\n26.108718872070312\n26.10677719116211\n26.10487937927246\n26.103023529052734\n26.10120391845703\n26.09942626953125\n26.09768295288086\n26.095979690551758\n26.094310760498047\n26.092681884765625\n26.091081619262695\n26.089519500732422\n26.08799171447754\n26.08649253845215\n26.085031509399414\n26.083599090576172\n26.082195281982422\n26.080820083618164\n26.079477310180664\n26.07816505432129\n26.07687759399414\n26.075618743896484\n26.074386596679688\n26.073179244995117\n26.072004318237305\n26.070846557617188\n26.069719314575195\n26.068614959716797\n26.06753158569336\n26.06647300720215\n26.06543731689453\n26.064424514770508\n26.063432693481445\n26.062463760375977\n26.061511993408203\n26.060583114624023\n26.059673309326172\n26.05878257751465\n26.057912826538086\n26.05706214904785\n26.056228637695312\n26.055410385131836\n26.05461311340332\n26.0538330078125\n26.053068161010742\n26.05232048034668\n26.05158805847168\n26.050870895385742\n26.0501708984375\n26.049484252929688\n26.04881477355957\n26.04815673828125\n26.04751205444336\n26.046886444091797\n26.046268463134766\n26.04566764831543\n26.04507827758789\n26.04450035095215\n26.0439395904541\n26.043384552001953\n26.0428466796875\n26.04231834411621\n26.04180145263672\n26.04129409790039\n26.040800094604492\n26.040315628051758\n26.03984260559082\n26.039377212524414\n26.038921356201172\n26.038476943969727\n26.038042068481445\n26.037616729736328\n26.037200927734375\n26.036794662475586\n26.036394119262695\n26.0360050201416\n26.03562355041504\n26.03525161743164\n26.03488540649414\n26.034526824951172\n26.034177780151367\n26.03383445739746\n26.033498764038086\n26.033170700073242\n26.032852172851562\n26.032535552978516\n26.032228469848633\n26.03192901611328\n26.031635284423828\n26.03134536743164\n26.031064987182617\n26.03078842163086\n26.030517578125\n26.030254364013672\n26.02999496459961\n26.02974510192871\n26.029499053955078\n26.029254913330078\n26.029020309448242\n26.028785705566406\n26.0285587310791\n26.028337478637695\n26.028120040893555\n26.027908325195312\n26.027700424194336\n26.027498245239258\n26.02729606628418\n26.027101516723633\n26.02690887451172\n26.026721954345703\n26.026540756225586\n26.0263614654541\n26.026187896728516\n26.026016235351562\n26.025848388671875\n26.025684356689453\n26.025524139404297\n26.025365829467773\n26.02521324157715\n26.025062561035156\n26.02491569519043\n26.024770736694336\n26.024627685546875\n26.024494171142578\n26.024356842041016\n26.02422523498535\n26.02409553527832\n26.023969650268555\n26.023847579956055\n26.023725509643555\n26.023605346679688\n26.023488998413086\n26.023374557495117\n26.023263931274414\n26.023155212402344\n26.023048400878906\n26.0229434967041\n26.022842407226562\n26.022741317749023\n26.02264404296875\n26.022546768188477\n26.02245330810547\n26.022363662719727\n26.02227210998535\n26.022186279296875\n26.022096633911133\n26.022016525268555\n26.02193260192871\n26.021852493286133\n26.021772384643555\n26.02169418334961\n26.021617889404297\n26.02154541015625\n26.021472930908203\n26.02140235900879\n26.021331787109375\n26.021265029907227\n26.021198272705078\n26.021133422851562\n26.02107048034668\n26.021007537841797\n26.020946502685547\n26.02088737487793\n26.020828247070312\n26.020771026611328\n26.020713806152344\n26.020658493041992\n26.020605087280273\n26.020551681518555\n26.0205020904541\n26.020450592041016\n26.020402908325195\n26.020353317260742\n26.020305633544922\n26.0202579498291\n26.020214080810547\n26.02016830444336\n26.020126342773438\n26.020084381103516\n26.020042419433594\n26.020000457763672\n26.019960403442383\n26.019920349121094\n26.019882202148438\n26.019845962524414\n26.019807815551758\n26.019771575927734\n26.019737243652344\n26.01970100402832\n26.01966667175293\n26.019634246826172\n26.019601821899414\n26.019569396972656\n26.01953887939453\n26.019508361816406\n26.01947784423828\n26.019451141357422\n26.019418716430664\n26.019393920898438\n26.019365310668945\n26.019336700439453\n26.019309997558594\n26.019285202026367\n26.019258499145508\n26.019235610961914\n26.019208908081055\n26.019187927246094\n26.019163131713867\n26.019142150878906\n26.01911735534668\n26.01909637451172\n26.019075393676758\n26.019054412841797\n26.019033432006836\n26.019012451171875\n26.018991470336914\n26.018972396850586\n26.018953323364258\n26.01893424987793\n26.0189151763916\n26.018898010253906\n26.01888084411621\n26.018861770629883\n26.01884651184082\n26.018829345703125\n26.018814086914062\n26.018798828125\n26.018781661987305\n26.01876449584961\n26.018753051757812\n26.018735885620117\n26.018722534179688\n26.018707275390625\n26.018693923950195\n26.018678665161133\n26.018667221069336\n26.018651962280273\n26.018638610839844\n26.018625259399414\n26.018613815307617\n26.01860237121582\n26.018590927124023\n26.018579483032227\n26.018566131591797\n26.0185546875\n26.018545150756836\n26.01853370666504\n26.018524169921875\n26.018510818481445\n26.01850128173828\n26.018491744995117\n26.018484115600586\n26.018470764160156\n26.018461227416992\n26.018451690673828\n26.018442153930664\n26.018434524536133\n26.018423080444336\n26.018417358398438\n26.018409729003906\n26.018400192260742\n26.018390655517578\n26.018383026123047\n26.018375396728516\n26.018367767333984\n26.018360137939453\n26.018352508544922\n26.01834487915039\n26.01833724975586\n26.018329620361328\n26.018321990966797\n26.018314361572266\n26.018308639526367\n26.01830291748047\n26.01829719543457\n26.018287658691406\n26.01828384399414\n26.01827621459961\n26.01827049255371\n26.018264770507812\n26.018259048461914\n26.018251419067383\n26.018245697021484\n26.018239974975586\n26.018234252929688\n26.01822853088379\n26.01822280883789\n26.018217086791992\n26.01820945739746\n26.018207550048828\n26.018199920654297\n26.01819610595703\n26.0181884765625\n26.018184661865234\n26.0181827545166\n26.018177032470703\n26.018171310424805\n26.01816749572754\n26.01816177368164\n26.018157958984375\n26.01815414428711\n26.01814842224121\n26.018144607543945\n26.018138885498047\n26.01813507080078\n26.018131256103516\n26.01812744140625\n26.018123626708984\n26.01811981201172\n26.018115997314453\n26.018110275268555\n26.018108367919922\n26.018102645874023\n26.01810073852539\n26.018096923828125\n26.018091201782227\n26.01808738708496\n26.018085479736328\n26.018081665039062\n26.018077850341797\n26.018075942993164\n26.018070220947266\n26.01806640625\n26.018062591552734\n26.018062591552734\n26.018056869506836\n26.018054962158203\n26.018049240112305\n26.018049240112305\n26.01804542541504\n26.018041610717773\n26.018037796020508\n26.018035888671875\n26.018033981323242\n26.018030166625977\n26.01802635192871\n26.018022537231445\n26.018020629882812\n26.01801872253418\n26.018014907836914\n26.01801300048828\n26.01801109313965\n26.01800537109375\n26.018003463745117\n26.018001556396484\n26.01799774169922\n26.01799774169922\n26.017993927001953\n26.017990112304688\n26.017988204956055\n26.01798439025879\n26.017982482910156\n26.017980575561523\n26.01797866821289\n26.017974853515625\n26.017972946166992\n26.017969131469727\n26.017967224121094\n26.01796531677246\n26.017963409423828\n26.017959594726562\n26.017959594726562\n26.017955780029297\n26.017953872680664\n26.0179500579834\n26.017948150634766\n26.0179443359375\n26.0179443359375\n26.017942428588867\n26.017940521240234\n26.01793670654297\n26.01793670654297\n26.017932891845703\n26.017929077148438\n26.017929077148438\n26.017927169799805\n26.01792335510254\n26.01792335510254\n26.017919540405273\n26.01791763305664\n26.017913818359375\n26.017911911010742\n26.017911911010742\n26.017908096313477\n26.017906188964844\n26.017902374267578\n26.017902374267578\n26.017898559570312\n26.017898559570312\n26.017894744873047\n26.017892837524414\n26.017892837524414\n26.01789093017578\n26.01788902282715\n26.01788330078125\n26.01788330078125\n26.017881393432617\n26.017879486083984\n26.01787757873535\n26.01787567138672\n26.01787567138672\n26.01786994934082\n26.01786994934082\n26.017868041992188\n26.017868041992188\n26.01786231994629\n26.01786231994629\n26.017860412597656\n26.017860412597656\n26.01785659790039\n26.01785659790039\n26.017852783203125\n26.017850875854492\n26.017847061157227\n26.017847061157227\n26.017845153808594\n26.017845153808594\n26.017841339111328\n26.017837524414062\n26.017837524414062\n26.01783561706543\n26.017833709716797\n26.017831802368164\n26.017831802368164\n26.01782989501953\n26.01782989501953\n26.017826080322266\n26.017822265625\n26.017822265625\n26.017820358276367\n26.017818450927734\n26.0178165435791\n26.01781463623047\n26.017812728881836\n26.017810821533203\n26.01780891418457\n26.017807006835938\n26.017805099487305\n26.017805099487305\n26.01780128479004\n26.017799377441406\n26.017797470092773\n26.017799377441406\n26.017793655395508\n26.01779556274414\n26.01778793334961\n26.017789840698242\n26.01778793334961\n\n\n\nAfter 1200 iterations we manage to reduce the loss from 160. to 26.\nNow let’s plot the process\n\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAfter each iteration we will have a brand new quadratic function, which will be much more close to the actual one that represent the real data\n\n\n\nStep 7: stop\n\nAfter n# of iterations we decide to stop\n\n\n\nSummarizing Gradient Descent\n\nAt the beginnig we start with the Weights\n\npick the randomly if we build the model from scartch\nget them pretrained from another model : TransferLearning\n\nBuild a Loss Function\n\nallow us to see how good or bad the outputs the model gives us\nthen we try to change/update the weights in way that makes the loss function better==lower\n\nTo find a way to do that (update the weights to optimize the loss) we use calculus to caltulate the Gradients\n\ngradients descent tells us either we decrease or increase the weights in order to minimize the loss (that simple!!)\n\nWe then iterate till we reached the lowest point\nStop"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#the-mnist-loss-function",
    "href": "posts/Fastai_ch4/Ch4.html#the-mnist-loss-function",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\nWe saw previously the function pr_eights() where we represent the input images x as vector, just like the w weight vector.\n\nhere we will do the same with our MNIST_sample\n\nWe already have our dataset for 3s and 7s as tensors == the x of the function:\n\ntraining-set: stacked_threes, stacked_sevens\nvalidation-set : valid_ten_3, valid_ten_7\n\nWe’ll concatenate them all into a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor)\n\nby using .view\nwhich is a Pytorh method that changes the shape of a tensor without changing its content\n-1 is a special parameter to .view that mean: make this axis as big as necessary to fit the data\n\n\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\n\ntrain_x.shape\n\ntorch.Size([12396, 784])\n\n\n\nWe need a label for each image. We’ll use 1 for 3s and 0 for 7s:\n\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens))\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396]))\n\n\n\nThe problem here is that train_x and train_y don’t match in terms of shape.\nHere we will use unsqueeze() method to train_y which will return rank 2 tensor: \n\n\ntrain_y= train_y.unsqueeze(1)\ntrain_y.shape\n\ntorch.Size([12396, 1])\n\n\n\nHere we create dset dataset for training by ziping both dependent and independent variables.\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\n\nGo through the same steps for validation set\n\n\nvalid_x = torch.cat([valid_ten_3, valid_ten_7]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_ten_3) + [0]*len(valid_ten_7)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\nNow we begin the 7 steps we saw earlier but now for the mnist model\n### 1-INIT the parameters\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nInitialize weights\n\n\nweights = init_params((28*28, 1))\nweights.shape\n\ntorch.Size([784, 1])\n\n\n\nInitialize the bias\n\n\nbias = init_params(1)\nbias\n\ntensor([0.6863], requires_grad=True)\n\n\n\n2-Prediction calculation\n\nNow we can calculate the prediction based on those randome weights and biases for one image[0]\nIn order to multiply 2 matrix you need to make sure they have same number of inner product:\n\nnumber of columns first matrix==number of rows second matrix\n\n\n\n(train_x[0]*weights.T).sum()+bias\n\ntensor([20.2336], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nIn python matrix multiplication is represented by @\nHere we create a function that return our training set as matrix multiplied @ by weights then added to the bias radome value\n\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[20.2336],\n        [17.0644],\n        [15.2384],\n        ...,\n        [18.3804],\n        [23.8567],\n        [28.6816]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nCheck the accuracy of our modl based on random weights\n\n\n# Let's check our accuracy\ncorrects = (preds&gt;0.0).float()==train_y\ncorrects\n\ntensor([[ True],\n        [ True],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\n# check the mean\ncorrects.float().mean().item()\n\n0.4912068545818329\n\n\n\nNow let’s see what the change in accuracy is for a small change in one of the weights (note that we have to ask PyTorch not to calculate gradients as we do this, which is what with torch.no_grad() is doing here):\n\n\nwith torch.no_grad(): weights[0] *= 1.0001\n\n\npreds = linear1(train_x)\n((preds&gt;0.0).float() == train_y).float().mean().item()\n\n0.4912068545818329\n\n\n\nAs we notice even with the change we commited in one of the weights we didn’t observe any change at all in the model accuracy! which rise a problem for our method. we need a formula that can reflexes the changes we commited to the parameters, so we update the parameters in a way that make our predictions better.\nThe problem with the thresh preds&gt;0.0 is that the gradients are allways equal to zero, because as we know the gradients are calculate as rise over run, and in this case the small change in parameters is rarely could change the prediction from 3 to 7 or vice-versa, so the gradients will allways indicate to 0.\n\nInstead we need a loss function, which when our weights results a slightly better prediction, gives us a slightly better loss function\n\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\n\nHere we assume that the trgts is the image we want to predict, 1 for 3’s and 0 for 7’s,\nSo in this case the model is confident in the first image it’s a 3 with 0.9, and not sure about the second image with 0.4, and way too incorrect in the 3d image with 0.2\nThe objective is to build a loss function that will calculate the accuracy of the model and returns some metrics that will be updated if we update our parameters\n\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\ntorch.where is a way of puting specific condition, in other way we can say this:\n\nif targets==1: loss = 1-predictions else: targets==0\n\ntorch.where(trgts==1, 1-prds, prds).mean()\n\ntensor(0.4333)\n\n\n\nSee if the loss will be updated if we change the weights.\n\nwe will pass this tensor [0.9, 0.4, 0.8] and see how to change the prediction from 0.2 to 0.8 will efects the loss function\n\n\n\nmnist_loss(tensor([0.9, 0.4, 0.8]), trgts)\n\ntensor(0.2333)\n\n\n\nAs we see here the loss function gets better when we minimize the distance between the prediction and the target in the third image\n\nThe problem that we still face in this method is that we assume that the predictions will allways be between 0 and 1\nTo solve this problem we need a function that return any prediction how matter bigger that 1 or smaller than 0 to the interval between those 2 numbers\nIn fact there’s a function that do the same exact thing, we call it SIGMOID FUNCTION\n\n\n\nSigmoid Function\nThe sigmoid function always outputs a number between 0 and 1. It’s defined as follows:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\n\n\n\n\nWhatever input we give it to Sigmoid it will always return a number between 0 and 1\nNow let’s update the mnist_loss by adding sigmoid to its inputs\n\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nSGD and Mini-Batches\n\nNow we have our loss function for the SGD, we need to know which method we will take in order to update the gradients, we can update them after taking all the data points or after calculating each data points.\nThe first method will take a lot of time, and the second will not use much information, so we will take another track which is to use Mini-Batches and update the gradients after one mini-batch\nHow many datapoints in each batch is called batch-size. a larger batch size will produce more accurate result but it will take much time, and smaller batch-size will need many epochs to learn but it will be faster\nWe will see later how to decide the suitable batch-size for each situation\nWe will use DataLoader in order to shuffle data items before we create the mini-batches, so we vary the data items in each of the mini batches\n\n\n#for example\nc = range(15)\ndl = DataLoader(c, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([ 3, 12,  8, 10,  2]),\n tensor([ 9,  4,  7, 14,  5]),\n tensor([ 1, 13,  0,  6, 11])]\n\n\n\nBut in training model scenarios we won’t need any pyhton collection, we want a collection containing dependent and independent variables.\n\nA tuple that contains dependents and independent variables called in Pytorch a Dataset\n\nHere is a simple dataset:\n\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\nWhen we pass a Dataset to a DataLoader we will get back mini-batches which are themselves tuples of tensors representing batches of independent and dependent variables:\n\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n (tensor([2, 4]), ('c', 'e'))]\n\n\n\n\nPutting It All Together\nNow we have :\n* dset, valid_dset * linear1 * mnist_loss * sigmoid_function\nwe can create the model from scratch\nFirst we re-intialize our parameters:\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n\nThen create the DataLoader from Dataset\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nWe’ll do the same for the validation set:\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nLet’s create a mini-batch of size 4 for testing:\n\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\nCall linear1 model we created earlier on the batch:\n\n\npreds = linear1(batch)\npreds\n\ntensor([[-2.1876],\n        [-8.3973],\n        [ 2.5000],\n        [-4.9473]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nCreate the loss function:\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.7419, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nNow we can calculate the gradients:\n\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0061), tensor([-0.0420]))\n\n\n\nLet’s put that all these steps in a single function:\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\nAnd test it..\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0121), tensor([-0.0840]))\n\n\n\nBut look what happens if we call it twice:\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0182), tensor([-0.1260]))\n\n\n\nThe gradients have changed!\n\nin order to not calculate the gradients and add to the last one every time we call this function calc_gradwe need to use grad.zero_ which set the current gradients to zero\n\n\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\nOur only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\nLet’s build a function that calculate the batch accuracy\n\n\n# accuracy of the batch\n(preds&gt;0.0).float() == train_y[:4]\n\ntensor([[False],\n        [False],\n        [ True],\n        [False]])\n\n\n\nFunction to calculate our validation accuracy:\n\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\n\n# check if it works\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.2500)\n\n\n\n# all together\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.5262\n\n\n\nNow lets train for one epoch and see if the accuracy improve\n\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6663\n\n\n\nThats promising\n\nfrom 0.4642 to 0.6096 after one epoch\n\nThen do a few more:\n\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8265 0.89 0.9183 0.9276 0.9398 0.9467 0.9506 0.9525 0.9559 0.9579 0.9599 0.9608 0.9613 0.9618 0.9633 0.9638 0.9647 0.9657 0.9672 0.9677 \n\n\n\n\nCreating an Optimizer\n\nAs we know the model we created is only for learning .\n\nin real world scenarios we do not need to implement everything from scratch, framworks like Pytorch and Fastai provide us with everything.\n\nThe linear1 model we created can be remplaced with nn.Linear which can do the same work and more\n\nnn.Linear combine the role of linear1 and weights+bias\n\n\n\nlinear_model = nn.Linear(28*28, 1)\n\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\n\nw, b = linear_model.parameters()\nw.shape, b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nWe can create optimizer class\n\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\n\n# passing the model params to the optimizer\nopt = BasicOptim(linear_model.parameters(), lr)\n\n\n# simplifying the epoch training function\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\n# validation doesn't change\nvalidate_epoch(linear_model)\n\n0.4606\n\n\n\n# simplifying the training loop:\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\n# same results as before\ntrain_model(linear_model, 20)\n\n0.4932 0.7686 0.8555 0.9136 0.9346 0.9482 0.957 0.9634 0.9658 0.9678 0.9697 0.9717 0.9736 0.9746 0.9761 0.9771 0.9775 0.9775 0.978 0.9785 \n\n\n\nThe good thing is all of this is provided by Fastai:\n\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.8179 0.8496 0.9141 0.9346 0.9482 0.957 0.9619 0.9658 0.9673 0.9692 0.9712 0.9741 0.9751 0.9761 0.9775 0.9775 0.978 0.9785 0.979 \n\n\n\nFastai also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders:\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nTo create a Learner we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nNow we can call fit:\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636709\n0.503144\n0.495584\n00:00\n\n\n1\n0.429828\n0.248517\n0.777233\n00:00\n\n\n2\n0.161680\n0.155361\n0.861629\n00:00\n\n\n3\n0.072948\n0.097721\n0.917566\n00:00\n\n\n4\n0.040128\n0.073205\n0.936212\n00:00\n\n\n5\n0.027210\n0.059466\n0.950442\n00:00\n\n\n6\n0.021837\n0.050799\n0.957802\n00:00\n\n\n7\n0.019398\n0.044980\n0.964181\n00:00\n\n\n8\n0.018122\n0.040853\n0.966143\n00:00\n\n\n9\n0.017330\n0.037788\n0.968106\n00:00\n\n\n\n\n\n\n\nAdding a Nonlinearity\n\nSo far we managed to create a linear function that can predict hand written digit with high performance.\nBut still, it’s just a simple linear classifier, with very constraint abilities.\nTo make it more complex and capable handle complex tasks, we need to add something non-linear between two linear classifiers.\n\nThis is gives us a Neural network\n\nHere a basic architecture of a neural network:\n\n\ndef simple_net(xb): \n    res = xb@w1 + b1 # first linear function\n    res = res.max(tensor(0.0)) # non linear function\n    res = res@w2 + b2  # 2nd linear function\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n\nw1.shape, b1.shape\n\n(torch.Size([784, 30]), torch.Size([30]))\n\n\n\nthe lines: res = xb@w1 + b1 and res@w2 + b2 are two classifier, basic linear function similar to the nn.Linear we use previously\nWhile the line res = res.max(tensor(0.0)) is a non linear function\n\nthis function called rectified linear unit ReLu which return every negative number with 0\n\nSo if we think about this architecture we have here:\n\nfirst we have a linear function that does the matrix multiplication between dataset tensors and initialzed weights + bias and output 30 (we can chose any number) features, which represent for each a different mix of pixels\nthese outputs are taken by the ReLU and converted to 0 if they are negativem and x==y if it not, then output also 30 features to the next linear layer, which will do the sam computaion as the first and output the results\n\nIn pythorch there’s a modul that fit our neural nets here: nn.Sequential().\n\ntake results from a layer to another\n\nwe also replace the linear function we built by nn.Linear and max((0.0)) with nn.ReLU()\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\n#lets try the model again but now with 2 layers\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.333021\n0.396112\n0.512267\n00:00\n\n\n1\n0.152461\n0.235238\n0.797350\n00:00\n\n\n2\n0.083573\n0.117471\n0.911678\n00:00\n\n\n3\n0.054309\n0.078720\n0.940628\n00:00\n\n\n4\n0.040829\n0.061228\n0.956330\n00:00\n\n\n5\n0.034006\n0.051490\n0.963690\n00:00\n\n\n6\n0.030123\n0.045381\n0.966634\n00:00\n\n\n7\n0.027619\n0.041218\n0.968106\n00:00\n\n\n8\n0.025825\n0.038200\n0.969087\n00:00\n\n\n9\n0.024441\n0.035901\n0.969578\n00:00\n\n\n10\n0.023321\n0.034082\n0.971541\n00:00\n\n\n11\n0.022387\n0.032598\n0.972031\n00:00\n\n\n12\n0.021592\n0.031353\n0.974485\n00:00\n\n\n13\n0.020904\n0.030284\n0.975466\n00:00\n\n\n14\n0.020300\n0.029352\n0.975466\n00:00\n\n\n15\n0.019766\n0.028526\n0.975466\n00:00\n\n\n16\n0.019288\n0.027788\n0.976448\n00:00\n\n\n17\n0.018857\n0.027124\n0.977429\n00:00\n\n\n18\n0.018465\n0.026523\n0.978410\n00:00\n\n\n19\n0.018107\n0.025977\n0.978901\n00:00\n\n\n20\n0.017777\n0.025479\n0.978901\n00:00\n\n\n21\n0.017473\n0.025022\n0.979392\n00:00\n\n\n22\n0.017191\n0.024601\n0.980373\n00:00\n\n\n23\n0.016927\n0.024213\n0.980373\n00:00\n\n\n24\n0.016680\n0.023855\n0.981354\n00:00\n\n\n25\n0.016449\n0.023521\n0.981354\n00:00\n\n\n26\n0.016230\n0.023211\n0.981354\n00:00\n\n\n27\n0.016023\n0.022922\n0.981354\n00:00\n\n\n28\n0.015827\n0.022653\n0.981845\n00:00\n\n\n29\n0.015641\n0.022401\n0.981845\n00:00\n\n\n30\n0.015463\n0.022165\n0.981845\n00:00\n\n\n31\n0.015294\n0.021944\n0.983317\n00:00\n\n\n32\n0.015132\n0.021736\n0.982826\n00:00\n\n\n33\n0.014977\n0.021541\n0.982826\n00:00\n\n\n34\n0.014828\n0.021357\n0.982336\n00:00\n\n\n35\n0.014686\n0.021184\n0.982336\n00:00\n\n\n36\n0.014549\n0.021019\n0.982336\n00:00\n\n\n37\n0.014417\n0.020864\n0.982336\n00:00\n\n\n38\n0.014290\n0.020716\n0.982336\n00:00\n\n\n39\n0.014168\n0.020576\n0.982336\n00:00\n\n\n\n\n\n\nAt this point we did everything we can to improve the model performane, and we got an accuracy of 982826, which is very solid number.\nfrom here on, all we can do is looking inside our model and understand the mechanic of it in each steps\n\n\n# we can see the architecture of the model\nm =learn.model\n\n\nAs expected the model architecture contains 2 layers and a non-linear function in-between.\n\n\nm\n\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\n\n\nWe could also see what did the model learn in each layer\n\n\nw, b= m[0].parameters()\n\n\nshow_image(w[22].view(28,28))"
  },
  {
    "objectID": "posts/Build a Large Language Model/Chapter-2/Untitled.html",
    "href": "posts/Build a Large Language Model/Chapter-2/Untitled.html",
    "title": "Chapter 2: Build a Large Language Model",
    "section": "",
    "text": "In order to build an LLM we need to provide a very large chunk of text.\nIn this chapter we will discuss how to prepare the text-dataset and feed it to LLM, and various technics and methods used in data preparation context.\n\n\n\n\nLLM cannot perform any kind of computation on raw text, it can only work with numbers.\nTherefor we need to apply some kind of numerical transformation to all input text.\nThis transformation called Embedding and the numerical representation of each word is Vector\nThe embedding process work also on other data format like Audio Video .. but each type has its own embedding model.Image\nMany algorithms have been developed to produce embeddings for words.\nThe famous one was World2Vec.\n\nIts approach was to train Neural Network on predicting embedding of a given word based on its context, and Vice-Versa.\nThe main idea here is that words that have appear in similar context atend to have similar meaning which also efect their embedding.\nSo if these words are projected in two dimensional embedding they will be appeared in clusters. Image\n\nIn current LLM embeddings are way mor larger and have higher dimensionality.\n\nFor example GPT-2 (117 Millions Parameters) used embeddings of 768 dimensions, where the largest GPT-3 (117 Billions Parameters) uses 12.288.\nEach word will be projected to 12.288 dimensions.\n\n\n\n\n\n\nBreaking Text into Units: Tokenization splits text into smaller pieces called tokens (words, subwords, or characters), which are the building blocks the model uses to process and understand language.\nMapping Tokens to Numbers: Each token is assigned a unique numerical identifier, allowing the LLM to work with numbers instead of raw text during computations.\nEfficient Representation: The way text is tokenized affects how efficiently the LLM processes input and generates output, balancing between accuracy (preserving meaning) and memory usage (fewer tokens).\nHere we will work with raw text called The Veredict and apply some kind of tokenization on it:\n\n\nimport urllib.request\n\nurl =  (\"https://raw.githubusercontent.com/rasbt/\"\n \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n \"the-verdict.txt\")\nfile_path = \"the-veredict.txt\"\nurllib.request.urlretrieve(url, file_path)\n\n('the-veredict.txt', &lt;http.client.HTTPMessage at 0x7a241c17b650&gt;)"
  },
  {
    "objectID": "posts/Build a Large Language Model/Chapter-2/Untitled.html#about-this-chapter",
    "href": "posts/Build a Large Language Model/Chapter-2/Untitled.html#about-this-chapter",
    "title": "Chapter 2: Build a Large Language Model",
    "section": "",
    "text": "In order to build an LLM we need to provide a very large chunk of text.\nIn this chapter we will discuss how to prepare the text-dataset and feed it to LLM, and various technics and methods used in data preparation context.\n\n\n\n\nLLM cannot perform any kind of computation on raw text, it can only work with numbers.\nTherefor we need to apply some kind of numerical transformation to all input text.\nThis transformation called Embedding and the numerical representation of each word is Vector\nThe embedding process work also on other data format like Audio Video .. but each type has its own embedding model.Image\nMany algorithms have been developed to produce embeddings for words.\nThe famous one was World2Vec.\n\nIts approach was to train Neural Network on predicting embedding of a given word based on its context, and Vice-Versa.\nThe main idea here is that words that have appear in similar context atend to have similar meaning which also efect their embedding.\nSo if these words are projected in two dimensional embedding they will be appeared in clusters. Image\n\nIn current LLM embeddings are way mor larger and have higher dimensionality.\n\nFor example GPT-2 (117 Millions Parameters) used embeddings of 768 dimensions, where the largest GPT-3 (117 Billions Parameters) uses 12.288.\nEach word will be projected to 12.288 dimensions.\n\n\n\n\n\n\nBreaking Text into Units: Tokenization splits text into smaller pieces called tokens (words, subwords, or characters), which are the building blocks the model uses to process and understand language.\nMapping Tokens to Numbers: Each token is assigned a unique numerical identifier, allowing the LLM to work with numbers instead of raw text during computations.\nEfficient Representation: The way text is tokenized affects how efficiently the LLM processes input and generates output, balancing between accuracy (preserving meaning) and memory usage (fewer tokens).\nHere we will work with raw text called The Veredict and apply some kind of tokenization on it:\n\n\nimport urllib.request\n\nurl =  (\"https://raw.githubusercontent.com/rasbt/\"\n \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n \"the-verdict.txt\")\nfile_path = \"the-veredict.txt\"\nurllib.request.urlretrieve(url, file_path)\n\n('the-veredict.txt', &lt;http.client.HTTPMessage at 0x7a241c17b650&gt;)"
  },
  {
    "objectID": "posts/Fastai_ch6/Ch6.html",
    "href": "posts/Fastai_ch6/Ch6.html",
    "title": "Chapter 6: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "In previous chapter we learned how to pick the right learning rate, and how the number of epochs may effect the accuarcy of our model.\nIn this chapter we will learn about two other types of computer vision problem:\n\nMulti-label classification: is when we want to predict one or more label per image (or even none)\nRegression: is when the label is a quantative number(s) rather than a categories\n\nIn the process will study more deeply the output activations, targets, and loss functions in deep learning models.\n\n\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 13.1 MB/s \n     |████████████████████████████████| 1.3 MB 27.4 MB/s \n     |████████████████████████████████| 5.3 MB 41.8 MB/s \n     |████████████████████████████████| 441 kB 48.6 MB/s \n     |████████████████████████████████| 1.6 MB 23.9 MB/s \n     |████████████████████████████████| 115 kB 50.9 MB/s \n     |████████████████████████████████| 163 kB 42.7 MB/s \n     |████████████████████████████████| 212 kB 18.8 MB/s \n     |████████████████████████████████| 127 kB 52.8 MB/s \n     |████████████████████████████████| 115 kB 48.9 MB/s \n     |████████████████████████████████| 7.6 MB 45.9 MB/s \nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\n\n\n\n\nAs we briefly explain, multi-label classfication is when we predict more than category for one image or even zero category.\nIn fact the bear classfier we built earlier is a good example of multi-label calssification , the only exception is that our model doesn’t have the feature of returning zero class if the model isn’t confidently sure about neither of the classes\nIn practice it is more likely to see an images that match more than 1 categories or zero, but it’s rarely to see models being trained for that prorpose.\nFirst, let’s see what a multi-label dataset looks like, then we’ll explain how to get it ready for our model. we’ll see that the architecture of the model does not change from the last chapter; only the loss function does.\n\n\n\n\nFor this chapter we will work with Pascal Dataset which provide multi-label categories per image.\nFirst download the data\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n\n\n\n\n\n\n    \n      \n      100.00% [1637801984/1637796771 03:06&lt;00:00]\n    \n    \n\n\n\nThis dataset is differente from what we’ve seen till now, it’s not structured by filename or folder, instead comes with CSV(Comma-Separated Values) telling us what label is assigned to each image.\n\nwe will use pandas to see analyze the data\n\n\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n...\n...\n...\n...\n\n\n5006\n009954.jpg\nhorse person\nTrue\n\n\n5007\n009955.jpg\nboat\nTrue\n\n\n5008\n009958.jpg\nperson bicycle\nTrue\n\n\n5009\n009959.jpg\ncar\nFalse\n\n\n5010\n009961.jpg\ndog\nFalse\n\n\n\n\n5011 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n###Constructing a DataBlock\n\nNow we will go through the steps of creating DataLoaders objects from DataFrame.\nThe easiest way is to use DataBlock API.\nBut first we need to define each of these concepts:\n\nDataset: A collection that return a tuple with dependent and independent variales when we index into it ( in this case dataframe)\nDataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\n\nOn the top of these, fastai provides two classes for bringing training and validation set together:\n\nDatasets: a class that contains training dataset and validation dataset\nDataLoaders: a class that contains training DataLoader and validation DataLoader\n\nLet’s create a DataBlock with no parameters, then create Datasets object from in by passing the actual DataFrame we will use df\n\n\ndblock = DataBlock()\n\n\n# create datasets objects\ndsets = dblock.datasets(df)\n\n\nBy default the dataset is randomly splited to train and validation 80%/20%\n\n\nlen(dsets.train), len(dsets.valid)\n\n(4009, 1002)\n\n\n\nIf we call the first item from one of the Datasets it return a row of DataFrame twice, assuming that we have two things: input and target, which we will build later.\n\n\nx, y = dsets.train[0]\nx, y\n\n(fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object, fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object)\n\n\n\nThe dependent variable is the image name, and the independent variable is the label, so let’s grab them\n\n\nx['fname'], x['labels']\n\n('008663.jpg', 'car person')\n\n\n\nThe goal here is to tell Datablock how identify the x’s and y’s of the dataset.\nWe will use get_x and get_y functions\n\n\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n('005620.jpg', 'aeroplane')\n\n\n\nThe problem with lambdas is cannot be saved when we create a learner, this is why it’s better to avoid them.\n\n\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock =  DataBlock(get_x= get_x, get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n('002549.jpg', 'tvmonitor')\n\n\n\nfrom PIL import Image\n\n\nImage.open(path/'train'/'002549.jpg')\n\n\n\n\n\n\n\n\n\nIn order to open an image we need the path\nAs we know some images has more the one label, that why we need to split them by space.\nLet’s recreate the datablock by adding this 2 things (path, split)\n\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock =  DataBlock(get_x= get_x, get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n(Path('/root/.fastai/data/pascal_2007/train/002844.jpg'), ['train'])\n\n\n\nNow we can open the image just by calling the [0] of the index of that item in dataset\nTo open image and do the conversion to tensors, we will use block types to provide us with set of transforms: ImageBlock and MultiCategoryBlock\n\nwe used ImageBlock before, it open the image from the path\nbefore we used CategoryBlock which cannot be used here, because it returned a single integer, but here we have multiple labels for each image, thats why we need MultiCategoryBlock\n\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x=get_x,\n                   get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\ndsets.train[0][1]\n\nTensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nAs we see the list of categories contains zeros and one 1.\nThe zeros represent all the other categories that doesn’t match the image, and obviously the 1 represent the label\nThis is known as One-Hot Encoding\nLet’s see which category represent this particular image by using toch.where\n\n\nidxs = torch.where(dsets.train[77][1]==1.)[0]\ndsets.train.vocab[idxs]\n\n(#2) ['person','sofa']\n\n\n\nTill we use the randome splitter provided by default, instead of using the is_valid which can be used as splitter\n\n\ndf.is_valid\n\n0        True\n1        True\n2        True\n3       False\n4        True\n        ...  \n5006     True\n5007     True\n5008     True\n5009    False\n5010    False\nName: is_valid, Length: 5011, dtype: bool\n\n\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x,\n                   get_y=get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\nlen(dsets.train), len(dsets.valid)\n\n(2501, 2510)\n\n\n\nOne last we have to do before creating our dataloaders, is to make sure that all images are the same size by using RandomResizeCrop\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\n\ndls.show_batch(nrows=2, ncols=4)\n\n\n\n\n\n\n\n\n\n\n\n\nNow we need to create a Learner, we know that learner is defined by 4 things:\n\nmodel (resenet18)\ndataloaders, we already created it\nOptimizer (SGD)\nloss-function: we need to make sure that we create a suitable loss function for this type of model.First we create a learner and look at its activations\n\n\n\nlearn = vision_learner(dls, resnet18)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\nNow we bring one batch and deconstruct it with x and y, then call the model as function by passing the independent variable as parameter, which will return activations.\n\n\nx, y= to_cpu(dls.train.one_batch())\nactivs= learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\n\nactivs[2]\n\nTensorBase([ 2.1179, -0.0294,  0.7001, -0.3637,  0.9945,  3.5996, -3.0180,  1.5298,  0.8906, -0.3150,  0.7787,  0.9151,  3.0681, -4.6584,  1.9598, -0.6030, -1.8170,  2.2310,  1.1888, -0.0595],\n           grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nAs we see here the activations aren’t yet scaled between 0 and 1, so we need to use Sigmoid() to do that.\nThe loss we will use here is similar to the one we used in mnist dataset: mnist_loss, the only different is we will add log().\n\n\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, 1-inputs, inputs).log().mean()\n\n\nBecause we have a one-hot-encoded dependent variable, we can’t use nll_loss or softmax.\nSoftmax make all predictions sum to 1, and push one activation to be much larger that the others, due to use of exp, but in our case we may have more than one target we need to predict, so the sum of all activations to 1 will be am issue here\nIn other hand nll_loss as we saw returns the value of one activation, the that is corresponding to the single label. but we have have multiple labels!\nOne other benefit of this function binary_cross_entropy is that it uses the broadcasting technic, by apllying the logic -torch.where(targets==1, 1-inputs, inputs) to all labels.\n\nit’s like asking each image: is that a cat?, is that a chair? is that a person?.. and after each question calculating the different between the predicted value and the actual value and return it as loss.\n\nPytorch provide us with functions and modules that do exactly the same.\nF.binary_cross_entropy and nn.BCELoss calculate the cross-enropy on one-hot-encoded target, but without inculding the sigmoid()\nThe built-in sigmoid() version of these two are: F.binary_cross_entropy_with_logits and nn.BCEWithLogitsLoss.\nSo the equivalent built-in function to our binary_cross_entropy is nn.BCEWithLogitsLoss\n\n\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\n\nTensorMultiCategory(1.0342, grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nAlthough we don’t need to tell fastai to use this function as a loss, because it will pick nn.BCEWithLogitsLoss() automatically since we have multiple category labels. ___\nIn this model we will use slightly different accuracy function.\nThe previous accuracy function compare our outputs with the single target, but since we have multiple targets, we need to aplly it differently.\nAfter we apply sigmoid to our activations, we need decide which are 1 and which are 0, the best way is to create some threshold, all values above it are 1’s, else == 0.\n\n\ndef accuracy_multi(inp, trg, thresh=0.5, sigmoid=True):\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp&gt;thresh)==trg.bool()).float().mean()\n\n\nThis function use the default threshold value, if we want to adjust this value within the same function, we will use a function in Python called partial\nIt allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments\n\n\n# partial function\ndef say_hello(name, say_what='hello'): return f'{say_what} {name}'\nsay_hello('Ismail'), say_hello('Ismail', 'hola')\n\n('hello Ismail', 'hola Ismail')\n\n\n\n# we can switch to another version of this function by calling partial\nf = partial(say_hello, say_what='Guten Tag')\nf('Salim'), f('Karim!')\n\n('Guten Tag Salim', 'Guten Tag Karim!')\n\n\n\nNow we can train our model as usual, we pick here 0.2 as threshold\n\n\nlearn =  vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr= 3e-3, freeze_epochs=4)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.943426\n0.692230\n0.235896\n00:39\n\n\n1\n0.823277\n0.564228\n0.285199\n00:31\n\n\n2\n0.604020\n0.199862\n0.827908\n00:32\n\n\n3\n0.359526\n0.124002\n0.944323\n00:30\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.131472\n0.116906\n0.944203\n00:31\n\n\n1\n0.116399\n0.106551\n0.951096\n00:31\n\n\n2\n0.096168\n0.104706\n0.951116\n00:31\n\n\n\n\n\n\nPicking the threshold is so important, if we pick too low we’ll often be failing to select correctly labeled objects, and if we pick to high we end up selecting only the objects that the model is strongly confident about.\nWe will grab all predictions and target using get_preds, then we will try few values for the thresh and see what get us the highest value.\n\n\npreds, targs= learn.get_preds()\n\n\n\n\n\n\n\n\n\nThe we can call the metrics directly, we just need to deactivate the sigmoid since it already apllied by default by get_preds on activations.\n\n\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\n\nAccording to this plot, the accuracy reach its highest when the thresh at 0.6 ____\n\n\n\n\n\n\nWe usualy think of deep learnig as couple of fields, each has its own architecture, problems, datatype.. for example there’s NLP, Vision, Regression, Tabular.\nBut the main difference among models used in these fields are basically the difference between dependent and independent variables used in those models, along side with its loss function.That means that there’s really a far wider array of models than just the simple domain-based split.\n\nwe can use text to generate image or vice versa, we can use continous values to predict videos/images/ texts..\n\nHere we will build a Regression Image model\n\nthe dependent variables are images\nwhile the independent variables are float values\n\n\n\n\n\nWe will use the Biwi Kinect Head Pose dataset for this section. We’ll begin by downloading the dataset as usual:\n\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n\n    \n      \n      100.00% [452321280/452316199 00:36&lt;00:00]\n    \n    \n\n\n\nPath.BASE_PATH = path\n\n\npath.ls().sorted()\n\n(#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj')...]\n\n\n\nThere are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won’t need them here). Let’s take a look inside one of these directories:\n\n\n(path/'01').ls().sorted()\n\n(#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt')...]\n\n\n\nInside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file:\n\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n\nPath('20/frame_00388_pose.txt')\n\n\n\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\nim.to_thumb(250)\n\n\n\n\n\n\n\n\n\nThe Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren’t important for our purposes, so we’ll just show the function we use to extract the head center point:\n\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nThis function return the coordinate of the center of the head of each image, so we can pass it as the get_y to DataBlock since it represent the independent variable for each image\n\n\nget_ctr(img_files[0])\n\ntensor([343.6303, 276.7759])\n\n\n\nThis dataset contains images of many person, each one has multiple images, so we can’t just randomly split the dataset, because we need the model to generelize on new people/images, and training the model on image of a person, and validate the results on a training set that contains images of the same person, will definitively cause Overfitting\nInstead what we do in this case, is to take all images that belong to one person, and define them as validation set.\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)))\n\n\nAs we see here we use PointBlock, this is what fastai use to coordinate data (tensor with 2 values)\nFor the splitting as we said before we took one person’s images 13 and put the all into validation dataset.\nWe use aug_transforms as transformers\nBefore doing any modeling, we should look at our data to confirm it seems okay:\n\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\n\n\n\nxb, yb= dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\n\nxb shape is [64,3,240,320]:\n\n64 is the number of items in each mini-batch\n3 represent number of channels, which in this case colors\n240*320 are the pixels of the image\n\n\n\n\n\n\nHere we create learner with help of vision_learner we pass to it:\n\ndls\nresnet18\ny_range(): this function define the range of our targets. In fastai this function is implemented using the sigmoid_range\n\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nThis is set as the final layer of the model\nNote that we didn’t define the loss function, but we already know that fastai will pick the right loss function for us depend on the type of data/model\n\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\n\nFastai picked MSELoss which stands for mean square error, which make sense since we have a regression problem.\nBut in case we want different loss we can pass it to vision_learner by using loss_func parameter.\nIn this type of model, we could pick the loss as metric we just need to take the square root of it)\nNow we need to pick a learning rate\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\n\n\n\n\nThen we will try 0.002 as learning rate\n\n\nlr = 0.002\nlearn.fine_tune(3, lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.137417\n0.008638\n02:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.009691\n0.000932\n02:10\n\n\n1\n0.003397\n0.000595\n02:10\n\n\n2\n0.002397\n0.000345\n02:10\n\n\n\n\n\n\nloss = (0.005764+0.001309+0.000556+0.000316)/4\nloss\n\n0.00198625\n\n\n\nmetric_err_rate = round(math.sqrt(0.002), 4)\nmetric_err_rate\n\n0.0447\n\n\n\nThe accuracy of the model 96% which is good. So by using a computer vision model and with transfer learning technics we manage to solve a regression problem with accuracy of 96%.\n\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "posts/Fastai_ch6/Ch6.html#multi-label-classification",
    "href": "posts/Fastai_ch6/Ch6.html#multi-label-classification",
    "title": "Chapter 6: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "As we briefly explain, multi-label classfication is when we predict more than category for one image or even zero category.\nIn fact the bear classfier we built earlier is a good example of multi-label calssification , the only exception is that our model doesn’t have the feature of returning zero class if the model isn’t confidently sure about neither of the classes\nIn practice it is more likely to see an images that match more than 1 categories or zero, but it’s rarely to see models being trained for that prorpose.\nFirst, let’s see what a multi-label dataset looks like, then we’ll explain how to get it ready for our model. we’ll see that the architecture of the model does not change from the last chapter; only the loss function does.\n\n\n\n\nFor this chapter we will work with Pascal Dataset which provide multi-label categories per image.\nFirst download the data\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n\n\n\n\n\n\n    \n      \n      100.00% [1637801984/1637796771 03:06&lt;00:00]\n    \n    \n\n\n\nThis dataset is differente from what we’ve seen till now, it’s not structured by filename or folder, instead comes with CSV(Comma-Separated Values) telling us what label is assigned to each image.\n\nwe will use pandas to see analyze the data\n\n\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n...\n...\n...\n...\n\n\n5006\n009954.jpg\nhorse person\nTrue\n\n\n5007\n009955.jpg\nboat\nTrue\n\n\n5008\n009958.jpg\nperson bicycle\nTrue\n\n\n5009\n009959.jpg\ncar\nFalse\n\n\n5010\n009961.jpg\ndog\nFalse\n\n\n\n\n5011 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n###Constructing a DataBlock\n\nNow we will go through the steps of creating DataLoaders objects from DataFrame.\nThe easiest way is to use DataBlock API.\nBut first we need to define each of these concepts:\n\nDataset: A collection that return a tuple with dependent and independent variales when we index into it ( in this case dataframe)\nDataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\n\nOn the top of these, fastai provides two classes for bringing training and validation set together:\n\nDatasets: a class that contains training dataset and validation dataset\nDataLoaders: a class that contains training DataLoader and validation DataLoader\n\nLet’s create a DataBlock with no parameters, then create Datasets object from in by passing the actual DataFrame we will use df\n\n\ndblock = DataBlock()\n\n\n# create datasets objects\ndsets = dblock.datasets(df)\n\n\nBy default the dataset is randomly splited to train and validation 80%/20%\n\n\nlen(dsets.train), len(dsets.valid)\n\n(4009, 1002)\n\n\n\nIf we call the first item from one of the Datasets it return a row of DataFrame twice, assuming that we have two things: input and target, which we will build later.\n\n\nx, y = dsets.train[0]\nx, y\n\n(fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object, fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object)\n\n\n\nThe dependent variable is the image name, and the independent variable is the label, so let’s grab them\n\n\nx['fname'], x['labels']\n\n('008663.jpg', 'car person')\n\n\n\nThe goal here is to tell Datablock how identify the x’s and y’s of the dataset.\nWe will use get_x and get_y functions\n\n\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n('005620.jpg', 'aeroplane')\n\n\n\nThe problem with lambdas is cannot be saved when we create a learner, this is why it’s better to avoid them.\n\n\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock =  DataBlock(get_x= get_x, get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n('002549.jpg', 'tvmonitor')\n\n\n\nfrom PIL import Image\n\n\nImage.open(path/'train'/'002549.jpg')\n\n\n\n\n\n\n\n\n\nIn order to open an image we need the path\nAs we know some images has more the one label, that why we need to split them by space.\nLet’s recreate the datablock by adding this 2 things (path, split)\n\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock =  DataBlock(get_x= get_x, get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n(Path('/root/.fastai/data/pascal_2007/train/002844.jpg'), ['train'])\n\n\n\nNow we can open the image just by calling the [0] of the index of that item in dataset\nTo open image and do the conversion to tensors, we will use block types to provide us with set of transforms: ImageBlock and MultiCategoryBlock\n\nwe used ImageBlock before, it open the image from the path\nbefore we used CategoryBlock which cannot be used here, because it returned a single integer, but here we have multiple labels for each image, thats why we need MultiCategoryBlock\n\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x=get_x,\n                   get_y=get_y)\ndsets= dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\ndsets.train[0][1]\n\nTensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nAs we see the list of categories contains zeros and one 1.\nThe zeros represent all the other categories that doesn’t match the image, and obviously the 1 represent the label\nThis is known as One-Hot Encoding\nLet’s see which category represent this particular image by using toch.where\n\n\nidxs = torch.where(dsets.train[77][1]==1.)[0]\ndsets.train.vocab[idxs]\n\n(#2) ['person','sofa']\n\n\n\nTill we use the randome splitter provided by default, instead of using the is_valid which can be used as splitter\n\n\ndf.is_valid\n\n0        True\n1        True\n2        True\n3       False\n4        True\n        ...  \n5006     True\n5007     True\n5008     True\n5009    False\n5010    False\nName: is_valid, Length: 5011, dtype: bool\n\n\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x,\n                   get_y=get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\nlen(dsets.train), len(dsets.valid)\n\n(2501, 2510)\n\n\n\nOne last we have to do before creating our dataloaders, is to make sure that all images are the same size by using RandomResizeCrop\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\n\ndls.show_batch(nrows=2, ncols=4)\n\n\n\n\n\n\n\n\n\n\n\n\nNow we need to create a Learner, we know that learner is defined by 4 things:\n\nmodel (resenet18)\ndataloaders, we already created it\nOptimizer (SGD)\nloss-function: we need to make sure that we create a suitable loss function for this type of model.First we create a learner and look at its activations\n\n\n\nlearn = vision_learner(dls, resnet18)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\nNow we bring one batch and deconstruct it with x and y, then call the model as function by passing the independent variable as parameter, which will return activations.\n\n\nx, y= to_cpu(dls.train.one_batch())\nactivs= learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\n\nactivs[2]\n\nTensorBase([ 2.1179, -0.0294,  0.7001, -0.3637,  0.9945,  3.5996, -3.0180,  1.5298,  0.8906, -0.3150,  0.7787,  0.9151,  3.0681, -4.6584,  1.9598, -0.6030, -1.8170,  2.2310,  1.1888, -0.0595],\n           grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nAs we see here the activations aren’t yet scaled between 0 and 1, so we need to use Sigmoid() to do that.\nThe loss we will use here is similar to the one we used in mnist dataset: mnist_loss, the only different is we will add log().\n\n\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, 1-inputs, inputs).log().mean()\n\n\nBecause we have a one-hot-encoded dependent variable, we can’t use nll_loss or softmax.\nSoftmax make all predictions sum to 1, and push one activation to be much larger that the others, due to use of exp, but in our case we may have more than one target we need to predict, so the sum of all activations to 1 will be am issue here\nIn other hand nll_loss as we saw returns the value of one activation, the that is corresponding to the single label. but we have have multiple labels!\nOne other benefit of this function binary_cross_entropy is that it uses the broadcasting technic, by apllying the logic -torch.where(targets==1, 1-inputs, inputs) to all labels.\n\nit’s like asking each image: is that a cat?, is that a chair? is that a person?.. and after each question calculating the different between the predicted value and the actual value and return it as loss.\n\nPytorch provide us with functions and modules that do exactly the same.\nF.binary_cross_entropy and nn.BCELoss calculate the cross-enropy on one-hot-encoded target, but without inculding the sigmoid()\nThe built-in sigmoid() version of these two are: F.binary_cross_entropy_with_logits and nn.BCEWithLogitsLoss.\nSo the equivalent built-in function to our binary_cross_entropy is nn.BCEWithLogitsLoss\n\n\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\n\nTensorMultiCategory(1.0342, grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nAlthough we don’t need to tell fastai to use this function as a loss, because it will pick nn.BCEWithLogitsLoss() automatically since we have multiple category labels. ___\nIn this model we will use slightly different accuracy function.\nThe previous accuracy function compare our outputs with the single target, but since we have multiple targets, we need to aplly it differently.\nAfter we apply sigmoid to our activations, we need decide which are 1 and which are 0, the best way is to create some threshold, all values above it are 1’s, else == 0.\n\n\ndef accuracy_multi(inp, trg, thresh=0.5, sigmoid=True):\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp&gt;thresh)==trg.bool()).float().mean()\n\n\nThis function use the default threshold value, if we want to adjust this value within the same function, we will use a function in Python called partial\nIt allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments\n\n\n# partial function\ndef say_hello(name, say_what='hello'): return f'{say_what} {name}'\nsay_hello('Ismail'), say_hello('Ismail', 'hola')\n\n('hello Ismail', 'hola Ismail')\n\n\n\n# we can switch to another version of this function by calling partial\nf = partial(say_hello, say_what='Guten Tag')\nf('Salim'), f('Karim!')\n\n('Guten Tag Salim', 'Guten Tag Karim!')\n\n\n\nNow we can train our model as usual, we pick here 0.2 as threshold\n\n\nlearn =  vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr= 3e-3, freeze_epochs=4)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.943426\n0.692230\n0.235896\n00:39\n\n\n1\n0.823277\n0.564228\n0.285199\n00:31\n\n\n2\n0.604020\n0.199862\n0.827908\n00:32\n\n\n3\n0.359526\n0.124002\n0.944323\n00:30\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.131472\n0.116906\n0.944203\n00:31\n\n\n1\n0.116399\n0.106551\n0.951096\n00:31\n\n\n2\n0.096168\n0.104706\n0.951116\n00:31\n\n\n\n\n\n\nPicking the threshold is so important, if we pick too low we’ll often be failing to select correctly labeled objects, and if we pick to high we end up selecting only the objects that the model is strongly confident about.\nWe will grab all predictions and target using get_preds, then we will try few values for the thresh and see what get us the highest value.\n\n\npreds, targs= learn.get_preds()\n\n\n\n\n\n\n\n\n\nThe we can call the metrics directly, we just need to deactivate the sigmoid since it already apllied by default by get_preds on activations.\n\n\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\n\nAccording to this plot, the accuracy reach its highest when the thresh at 0.6 ____"
  },
  {
    "objectID": "posts/Fastai_ch6/Ch6.html#regression",
    "href": "posts/Fastai_ch6/Ch6.html#regression",
    "title": "Chapter 6: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "We usualy think of deep learnig as couple of fields, each has its own architecture, problems, datatype.. for example there’s NLP, Vision, Regression, Tabular.\nBut the main difference among models used in these fields are basically the difference between dependent and independent variables used in those models, along side with its loss function.That means that there’s really a far wider array of models than just the simple domain-based split.\n\nwe can use text to generate image or vice versa, we can use continous values to predict videos/images/ texts..\n\nHere we will build a Regression Image model\n\nthe dependent variables are images\nwhile the independent variables are float values\n\n\n\n\n\nWe will use the Biwi Kinect Head Pose dataset for this section. We’ll begin by downloading the dataset as usual:\n\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n\n    \n      \n      100.00% [452321280/452316199 00:36&lt;00:00]\n    \n    \n\n\n\nPath.BASE_PATH = path\n\n\npath.ls().sorted()\n\n(#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj')...]\n\n\n\nThere are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won’t need them here). Let’s take a look inside one of these directories:\n\n\n(path/'01').ls().sorted()\n\n(#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt')...]\n\n\n\nInside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file:\n\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n\nPath('20/frame_00388_pose.txt')\n\n\n\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\nim.to_thumb(250)\n\n\n\n\n\n\n\n\n\nThe Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren’t important for our purposes, so we’ll just show the function we use to extract the head center point:\n\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nThis function return the coordinate of the center of the head of each image, so we can pass it as the get_y to DataBlock since it represent the independent variable for each image\n\n\nget_ctr(img_files[0])\n\ntensor([343.6303, 276.7759])\n\n\n\nThis dataset contains images of many person, each one has multiple images, so we can’t just randomly split the dataset, because we need the model to generelize on new people/images, and training the model on image of a person, and validate the results on a training set that contains images of the same person, will definitively cause Overfitting\nInstead what we do in this case, is to take all images that belong to one person, and define them as validation set.\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)))\n\n\nAs we see here we use PointBlock, this is what fastai use to coordinate data (tensor with 2 values)\nFor the splitting as we said before we took one person’s images 13 and put the all into validation dataset.\nWe use aug_transforms as transformers\nBefore doing any modeling, we should look at our data to confirm it seems okay:\n\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\n\n\n\nxb, yb= dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\n\nxb shape is [64,3,240,320]:\n\n64 is the number of items in each mini-batch\n3 represent number of channels, which in this case colors\n240*320 are the pixels of the image\n\n\n\n\n\n\nHere we create learner with help of vision_learner we pass to it:\n\ndls\nresnet18\ny_range(): this function define the range of our targets. In fastai this function is implemented using the sigmoid_range\n\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nThis is set as the final layer of the model\nNote that we didn’t define the loss function, but we already know that fastai will pick the right loss function for us depend on the type of data/model\n\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\n\nFastai picked MSELoss which stands for mean square error, which make sense since we have a regression problem.\nBut in case we want different loss we can pass it to vision_learner by using loss_func parameter.\nIn this type of model, we could pick the loss as metric we just need to take the square root of it)\nNow we need to pick a learning rate\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\n\n\n\n\nThen we will try 0.002 as learning rate\n\n\nlr = 0.002\nlearn.fine_tune(3, lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.137417\n0.008638\n02:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.009691\n0.000932\n02:10\n\n\n1\n0.003397\n0.000595\n02:10\n\n\n2\n0.002397\n0.000345\n02:10\n\n\n\n\n\n\nloss = (0.005764+0.001309+0.000556+0.000316)/4\nloss\n\n0.00198625\n\n\n\nmetric_err_rate = round(math.sqrt(0.002), 4)\nmetric_err_rate\n\n0.0447\n\n\n\nThe accuracy of the model 96% which is good. So by using a computer vision model and with transfer learning technics we manage to solve a regression problem with accuracy of 96%.\n\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "posts/Pydantic-Projects/WeatherCLI/First_Version.html",
    "href": "posts/Pydantic-Projects/WeatherCLI/First_Version.html",
    "title": "Building Weather CLI with Pydantic and Requests",
    "section": "",
    "text": "Goal: * Build a command-line tool to fetch and display weather information for a given city using a weather API.\nTech Stack: - Python: Core programming language. - Requests: For making HTTP requests to the weather API. - Python-dotenv: For securely loading environment variables (API keys). - Pydantic: For validating and parsing the API response. - UV: For handling dependencies.\n\n\n\nWhy? To avoid hardcoding sensitive data (e.g., API keys) in the code.\nHow? Create a .env file:\n\n\n  API_KEY=\"your_api_key_here\"\n  BASE_URL=\"https://api.openweathermap.org/data/2.5/weather\"\n  UNITS=\"metric\"\n\n\nLoad it into Python using dotenv:\n\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\napi_key = os.getenv(\"API_KEY\")\n  \n\n\n\n\nPydantic’s pydantic-settings library can use .env files internally without explicitly calling load_dotenv() by leveraging its built-in support for .env files. When you define a settings model using BaseSettings, it can automatically read environment variables from a .env file if you specify the env_file configuration in the Config class.\nFor example we could create an .env file like this:\n\nDATABASE_URL=postgresql://user:password@localhost/dbname\nSECRET_KEY=mysecretkey\nDEBUG=True\n\nAnd define this settings model:\n\n\nfrom pydantic_settings import BaseSettings\n\nclass AppConfig(BaseSettings):\n    database_url: str\n    secret_key: str\n    debug: bool\n\n    class Config:\n        # Specify the .env file\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\n# Instantiate the settings model\nconfig = AppConfig()\n\n\n\nWe could access the values like this: ```\nprint(config.database_url) print(config.secret_key) print(config.debug) ```\n\n\n\n\n\nSending GET Requests:\n\nUse requests.get() to make API calls with query parameters.\nExample:\n\n\n\nparams = {\"q\": city, \"appid\": settings.api_key, \"units\": settings.units}\nresponse = requests.get(settings.base_url, params=params)\n\n\nHandling Responses:\n\nUse .json() to parse the API’s JSON response into a Python dictionary.\nExample:\n\n\n\npython\ndata = response.json()\ntemp = data['main']['temp']\n\n\n\n\n\nWhy? Ensure the response structure matches expectations and prevent errors caused by invalid or missing fields.\nExample:\n\n\n\nclass WeatherResponse(BaseModel):\n    temp: float\n    description: str\n    humidity: int\n\nweather = WeatherResponse(\n    temp=data['main']['temp'],\n    description=data['weather'][0]['description'],\n    humidity=data['main']['humidity']\n  )\n  \n\n\n\n\n\nCommon Issues:\n\nInvalid API key → 401 Unauthorized.\nCity not found → 404 Not Found.\nNetwork issues → requests.exceptions.RequestException.\n\nSolutions:\n\nUse response.raise_for_status() to handle HTTP errors gracefully.\nWrap the API call in try...except to manage exceptions:\n\n\n\ntry:\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP error: {e}\")\n\n\n\n\n\n\n\nAdd tests for edge cases (e.g., city not found, network errors).\nExtend functionality (e.g., fetch weather forecasts, support multiple units).\nImprove user experience with clearer error messages."
  },
  {
    "objectID": "posts/Pydantic-Projects/WeatherCLI/First_Version.html#project-overview",
    "href": "posts/Pydantic-Projects/WeatherCLI/First_Version.html#project-overview",
    "title": "Building Weather CLI with Pydantic and Requests",
    "section": "",
    "text": "Goal: * Build a command-line tool to fetch and display weather information for a given city using a weather API.\nTech Stack: - Python: Core programming language. - Requests: For making HTTP requests to the weather API. - Python-dotenv: For securely loading environment variables (API keys). - Pydantic: For validating and parsing the API response. - UV: For handling dependencies.\n\n\n\nWhy? To avoid hardcoding sensitive data (e.g., API keys) in the code.\nHow? Create a .env file:\n\n\n  API_KEY=\"your_api_key_here\"\n  BASE_URL=\"https://api.openweathermap.org/data/2.5/weather\"\n  UNITS=\"metric\"\n\n\nLoad it into Python using dotenv:\n\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\napi_key = os.getenv(\"API_KEY\")\n  \n\n\n\n\nPydantic’s pydantic-settings library can use .env files internally without explicitly calling load_dotenv() by leveraging its built-in support for .env files. When you define a settings model using BaseSettings, it can automatically read environment variables from a .env file if you specify the env_file configuration in the Config class.\nFor example we could create an .env file like this:\n\nDATABASE_URL=postgresql://user:password@localhost/dbname\nSECRET_KEY=mysecretkey\nDEBUG=True\n\nAnd define this settings model:\n\n\nfrom pydantic_settings import BaseSettings\n\nclass AppConfig(BaseSettings):\n    database_url: str\n    secret_key: str\n    debug: bool\n\n    class Config:\n        # Specify the .env file\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\n# Instantiate the settings model\nconfig = AppConfig()\n\n\n\nWe could access the values like this: ```\nprint(config.database_url) print(config.secret_key) print(config.debug) ```\n\n\n\n\n\nSending GET Requests:\n\nUse requests.get() to make API calls with query parameters.\nExample:\n\n\n\nparams = {\"q\": city, \"appid\": settings.api_key, \"units\": settings.units}\nresponse = requests.get(settings.base_url, params=params)\n\n\nHandling Responses:\n\nUse .json() to parse the API’s JSON response into a Python dictionary.\nExample:\n\n\n\npython\ndata = response.json()\ntemp = data['main']['temp']\n\n\n\n\n\nWhy? Ensure the response structure matches expectations and prevent errors caused by invalid or missing fields.\nExample:\n\n\n\nclass WeatherResponse(BaseModel):\n    temp: float\n    description: str\n    humidity: int\n\nweather = WeatherResponse(\n    temp=data['main']['temp'],\n    description=data['weather'][0]['description'],\n    humidity=data['main']['humidity']\n  )\n  \n\n\n\n\n\nCommon Issues:\n\nInvalid API key → 401 Unauthorized.\nCity not found → 404 Not Found.\nNetwork issues → requests.exceptions.RequestException.\n\nSolutions:\n\nUse response.raise_for_status() to handle HTTP errors gracefully.\nWrap the API call in try...except to manage exceptions:\n\n\n\ntry:\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP error: {e}\")\n\n\n\n\n\n\n\nAdd tests for edge cases (e.g., city not found, network errors).\nExtend functionality (e.g., fetch weather forecasts, support multiple units).\nImprove user experience with clearer error messages."
  },
  {
    "objectID": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html",
    "href": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html",
    "title": "Hugging Face Course Notes: Chapter4",
    "section": "",
    "text": "The hugging Face Hub is the place where every Model, Dataset is deployed and stored.\nIn this chapter we will focus on how to:\n\nUse a fine-tuned model from the Hub\nShare and deploy a our model to the Hub\nBuild a model card\n\nAt its core a shared model is just a Git reposetory, which means that it can be cloned and used by others.\nWhen a new model is shared to the community a hosted inference API is deployed automatically, so anyone can test that model directly or build on top of it.\n\n\n\n\n\nAs we saw in previous chapters, using finetuned models from the Hub on our tasks is easy and can be achieved with few lines of code\n\n\nfrom transformers import pipeline\nunmasker = pipeline(\"fill-mask\", model = 'camembert-base')\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\n\n\n\n\n\n\n\n[{'score': 0.1466376781463623,\n  'token': 808,\n  'token_str': 'the',\n  'sequence': 'This course will teach you all about the models.'},\n {'score': 0.06081351637840271,\n  'token': 9098,\n  'token_str': 'this',\n  'sequence': 'This course will teach you all about this models.'}]\n\n\n\nOf course we need to pick a checkpoint that suitable for our task, otherwise we will get results that don’t male sense at all.\n\nin this case we pick camembert-base which is a good checkpoint for filling mask tasks.\n\nWe could also insentiate the checkpoint from the model calss directly:\n\n\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\nckpt = 'camembert-base'\ntokenizer = CamembertTokenizer.from_pretrained(ckpt)\nmodel = CamembertForMaskedLM.from_pretrained(ckpt)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nHowever its recommended to use the auto class to handel the insentitating of model and tokenizers:\n\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForMaskedLM.from_pretrained(ckpt)\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nWhat’s important is to inderstand how that specific model is trained, which dataset is used, and what’s its limitations and biases.\n\nall of this informastions should be mentioned in the model card (which we will build later)\n\n\n\n\n\n\nIn general there’s 3 ways to create a new model reposetories:\n\nUsing the push_to_hub API\nUsing the huggingface_hub Python library\nUsing the web interface\n\nOne the repo is created, we can add and edit files just like any other repo on github\n\n\n\n\nThe simplest way to create a model repo is to use push_to_hub API.\n\nbut first we need to get our credentials in order to use the API:\n\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\n\nWe used earlier the TrainingArguments class to pass hyper-parameters during the building of the training loop, the easiest way to push a model is by setting push_to_hub= True as an arguments:\n\n\nfrom transformers import Trainer, TrainingArguments\ntraining_arguments = TrainingArguments('test-train-0', save_strategy = 'epoch', push_to_hub= True)\n\n\nOnce the model is trained and the trainer.train() is called, the api will upload the model to the hub and save it in a repo with the name we pick test-train-0, but we can chose another name by passing hub_model_id=\"my_model_name\"\nOnce the training is complete we should do the final trainer_push_to_hub() to upload the last version of the model. This will also generate the model card, which contains all the metadata, hyperparameters and evaluation results.\n\n\n\n\nmodel card From Hugging Face\n\n\n\nThe push_to_hub() method can be applied on model, tokenizer, configs. It take care of both: creating the repo and pushing the model and tokenizer directly to that repository.\nNow let’s see exactly how this work:\n\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nckpt = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForMaskedLM.from_pretrained(ckpt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nNow we can take these model and build whatever we want with them, modify, add..and when we are satisfied with the results we can use push_to_hup() method:\n\n\nmodel.push_to_hub('test-ch4')\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/Smail/test-ch4/commit/2f5a775ac7e24540a43374fa820db4e225b195e2', commit_message='Upload BertForMaskedLM', commit_description='', oid='2f5a775ac7e24540a43374fa820db4e225b195e2', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nThis will create a new repo \"test-ch4\" in our profile and populate it with model files. We can do the same with tokenizer\n\n\ntokenizer.push_to_hub('test-ch4')\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/Smail/test-ch4/commit/e0b505500d75768b52d549adc8f67c3f40b015dd', commit_message='Upload tokenizer', commit_description='', oid='e0b505500d75768b52d549adc8f67c3f40b015dd', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nwe can also add the organization, tokenization key and other arguments that can be specified while pushing the model into the hub through API\n\n\n\n\n\nWe can also use the Huggingface_hub library that offer more tools that are simple and very effective to achieve various tasks such as pushing a model, adding files, deleting files, creating repos, editing, managing, getting informations etc.. *Similar to push_to_hub() method the package requires a key token to access the hub:\n\n\n!huggingface-cli login\n\n\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nToken: \nAdd token as git credential? (Y/n) n\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n\n\n\n!huggingface-cli whoami\n\nSmail\n\n\n\nThe huggingface_hub offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n\nfrom huggingface_hub import (\n    #User Management\n    login,\n    logout,\n    whoami,\n\n    # Repo creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n\n)\n\n\nWe could for example try to create a repo like this:\n\n\ncreate_repo('dummi_repo')\n\nRepoUrl('https://huggingface.co/Smail/dummi_repo', endpoint='https://huggingface.co', repo_type='model', repo_id='Smail/dummi_repo')\n\n\n\nOther arguments which may be useful are:\n\nprivate, in order to specify if the repository should be visible from others or not.\ntoken, if you would like to override the token stored in your cache by a given token.\nrepo_type, if you would like to create a dataset or a space instead of a model. Accepted values are “dataset” and “space”.\n\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled\n\n\n\n\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for Git Large File Storage) for larger files.\nIn general there is 3 ways to upload files to the HUB:\n\n\n\n\n\nthis approach doesn’t require installing git or git-lf on our system, it uses HTTP POST requests to push the files directly to the hub.\nIt’s limitation is the size of the file shouldn’t be larger than 5 GB.\nSince I work in a google colab, I will create a folder and work with it “locally”.\n\n\n!mkdir my_folder\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"/content/my_folder\")\ntokenizer.save_pretrained(\"/content/my_folder\")\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n('/content/my_folder/tokenizer_config.json',\n '/content/my_folder/special_tokens_map.json',\n '/content/my_folder/sentencepiece.bpe.model',\n '/content/my_folder/added_tokens.json',\n '/content/my_folder/tokenizer.json')\n\n\n\nfrom huggingface_hub import upload_file\nupload_file(\n    path_or_fileobj=\"/content/my_folder/config.json\",\n    path_in_repo=\"config.json\",\n    repo_id=\"Smail/dummi_repo\",\n)\n\nCommitInfo(commit_url='https://huggingface.co/Smail/dummi_repo/commit/1af5f861f0e0a3398d3856c69ff83bd5f04c372a', commit_message='Upload config.json with huggingface_hub', commit_description='', oid='1af5f861f0e0a3398d3856c69ff83bd5f04c372a', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nThis will upload the config.json file exists in path_to_file to the root of the repo as config.json.\n\n\n\n\n\nThe repository class abstract a local repo and handle all the work in agit like manner, it requires having git and git-lf installed in our system.\n\n\n!pip install git-LFS\n\nCollecting git-LFS\n  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\nInstalling collected packages: git-LFS\nSuccessfully installed git-LFS-1.6\n\n\n\n!git lfs install\n\nGit LFS initialized.\n\n\n\nfrom huggingface_hub import Repository\n\nrepo = Repository('/content', clone_from= 'Smail/dummi_repo')\n\n\nThis will create a folder in the path we decide and will import all the files in the repo from the hub.\nSince we didn’t create any files in that repo when we insantiate it, the only file we have is .gitattributes, from here we could deal with that repo as if it a git repo:\n\n\nrepo.git_pull()\nrepo.git_add()\nrepo.git_commit()\nrepo.git_push()\nrepo.git_tag()\n\n\nFirst we have to make the repo up-to-date with:\n\n\nrepo.git_pull()\n\n\nWe can now save the model and tokenizer in the directotry\n\n\nmodel.save_pretrained('my_folder')\ntokenizer.save_pretrained('my_folder')\n\n\nNow our local folder countains 2 new files and we could push them to the hub:\n\n\nrepo.git_add()\nrepo.git_commit('add model and tokenizer')\nrepo.git_push()\n\n\n\n\n\nThis method is very similar to the one before, its barbone method that in pure git and bash.\nFirst let’s install git and git-lf:\n\n\n!pip install git-LFS\n\nRequirement already satisfied: git-LFS in /usr/local/lib/python3.10/dist-packages (1.6)\n\n\n\nAnd the initialize git and lfs\n\n\n!git lfs install\n\nUpdated git hooks.\nGit LFS initialized.\n\n\n\nNow we can clone the repo from the hub the normal way:\n\n\n!git clone https://huggingface.co/Smail/test-model-2\n\nCloning into 'test-model-2'...\nremote: Enumerating objects: 4, done.\nremote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 4\nUnpacking objects:  25% (1/4)Unpacking objects:  50% (2/4)Unpacking objects:  75% (3/4)Unpacking objects: 100% (4/4)Unpacking objects: 100% (4/4), 1.12 KiB | 1.12 MiB/s, done.\n\n\n\nSince we are using bash, we could naigate the directory like this:\n\n\n!cd test-model-2 && ls\n\nREADME.md\n\n\n\nThis will chnage the directory the repo we just cloned, and print all the files in it.\nNow we can build a model like we did before and save all the files in the repo we cloned and push them to the hub:\n\n\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"/content/test-model-2\")\ntokenizer.save_pretrained(\"/content/test-model-2\")\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n('/content/test-model-2/tokenizer_config.json',\n '/content/test-model-2/special_tokens_map.json',\n '/content/test-model-2/sentencepiece.bpe.model',\n '/content/test-model-2/added_tokens.json',\n '/content/test-model-2/tokenizer.json')\n\n\n\n!ls\n\nmy_folder  sample_data  test-model-2\n\n\n\n!cd test-model-2 && ls\n\nadded_tokens.json  model.safetensors  sentencepiece.bpe.model  tokenizer_config.json\nconfig.json    README.md          special_tokens_map.json  tokenizer.json\n\n\n\nNow that we’ve saved some model and tokenizer artifacts, let’s take another look at the folder:\n\nIf you look at the file sizes (for example, with ls -lh), you should see that the model state dict file (pytorch_model.bin) is the only outlier, at more than 400 MB.\nWe can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git’s staging environment using the git add command:\n\n!cd test-model-2 && git add .\n\n\n!cd test-model-2 && git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   added_tokens.json\n    new file:   config.json\n    new file:   model.safetensors\n    new file:   sentencepiece.bpe.model\n    new file:   special_tokens_map.json\n    new file:   tokenizer.json\n    new file:   tokenizer_config.json\n\n\n\n\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its status command:\n\n\n!cd test-model-2 && git lfs status\n\nOn branch main\nObjects to be pushed to origin/main:\n\n\nObjects to be committed:\n\n    added_tokens.json (Git: 43734cd)\n    config.json (Git: 4b8db4b)\n    model.safetensors (LFS: 2785d2e)\n    sentencepiece.bpe.model (LFS: 988bc5a)\n    special_tokens_map.json (Git: b547935)\n    tokenizer.json (Git: 9a9362e)\n    tokenizer_config.json (Git: c49982e)\n\nObjects not staged for commit:\n\n\n\n\n\nWe can see that all files have Git as a handler, except pytorch_model.bin and sentencepiece.bpe.model, which have LFS. Great!\nLet’s proceed to the final steps, committing and pushing to the huggingface.co remote repository:\n\n\n!cd test-model-2 && git commit -m \"First model version\"\n\n[main adaa023] First model version\n 7 files changed, 128351 insertions(+)\n create mode 100644 added_tokens.json\n create mode 100644 config.json\n create mode 100644 model.safetensors\n create mode 100644 sentencepiece.bpe.model\n create mode 100644 special_tokens_map.json\n create mode 100644 tokenizer.json\n create mode 100644 tokenizer_config.json\n\n\n*Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:\n\n!cd test-model-2 && git push\n\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 2 threads\nCompressing objects: 100% (8/8), done.\nWriting objects: 100% (9/9), 592.06 KiB | 5.10 MiB/s, done.\nTotal 9 (delta 0), reused 0 (delta 0), pack-reused 0\nTo https://huggingface.co/Smail/test-model-2\n   6266b40..adaa023  main -&gt; main\n\n\n\n\n\n\n\nModel card plays a crucial role in open source model, it allow others to build an idea about the important element of the model without spending time and effort, it ensures reusability and reproducibility of the results.\nBy documenting the training and evaluation process we hepl other understand what they expect from the model, its limitations and capabilities, also we have to provide enough iformations about the data we train the model and how it was preprocessed.\nThe model card usually starts with a very brief, high-level overview of what the model is for, followed by additional details in the following sections:\n\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nEvaluation results\n\n\n\n\n\nThe model description provides basic details about the model. This includes the architecture, version, if it was introduced in a paper, if an original implementation is available, the author, and general information about the model. Any copyright should be attributed here. General information about training procedures, parameters, and important disclaimers can also be mentioned in this section.\n\n\n\n\n\nHere you describe the use cases the model is intended for, including the languages, fields, and domains where it can be applied. This section of the model card can also document areas that are known to be out of scope for the model, or where it is likely to perform suboptimally.\n\n\n\n\n\nThis section should include some examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes, and any other code you think might be helpful.\n\n\n\n\n\nThis part should indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome.\n\n\n\n\n\nIn this section you should describe all the relevant aspects of training that are useful from a reproducibility perspective. This includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, the batch size, the learning rate, and so on.\n\n\n\n\n\nHere you should describe the metrics you use for evaluation, and the different factors you are mesuring. Mentioning which metric(s) were used, on which dataset and which dataset split, makes it easy to compare you model’s performance compared to that of other models. These should be informed by the previous sections, such as the intended users and use cases.\n\n\n\n\n\nFinally, provide an indication of how well the model performs on the evaluation dataset. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses."
  },
  {
    "objectID": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#introduction",
    "href": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#introduction",
    "title": "Hugging Face Course Notes: Chapter4",
    "section": "",
    "text": "The hugging Face Hub is the place where every Model, Dataset is deployed and stored.\nIn this chapter we will focus on how to:\n\nUse a fine-tuned model from the Hub\nShare and deploy a our model to the Hub\nBuild a model card\n\nAt its core a shared model is just a Git reposetory, which means that it can be cloned and used by others.\nWhen a new model is shared to the community a hosted inference API is deployed automatically, so anyone can test that model directly or build on top of it."
  },
  {
    "objectID": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#using-pretrained-models",
    "href": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#using-pretrained-models",
    "title": "Hugging Face Course Notes: Chapter4",
    "section": "",
    "text": "As we saw in previous chapters, using finetuned models from the Hub on our tasks is easy and can be achieved with few lines of code\n\n\nfrom transformers import pipeline\nunmasker = pipeline(\"fill-mask\", model = 'camembert-base')\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\n\n\n\n\n\n\n\n[{'score': 0.1466376781463623,\n  'token': 808,\n  'token_str': 'the',\n  'sequence': 'This course will teach you all about the models.'},\n {'score': 0.06081351637840271,\n  'token': 9098,\n  'token_str': 'this',\n  'sequence': 'This course will teach you all about this models.'}]\n\n\n\nOf course we need to pick a checkpoint that suitable for our task, otherwise we will get results that don’t male sense at all.\n\nin this case we pick camembert-base which is a good checkpoint for filling mask tasks.\n\nWe could also insentiate the checkpoint from the model calss directly:\n\n\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\nckpt = 'camembert-base'\ntokenizer = CamembertTokenizer.from_pretrained(ckpt)\nmodel = CamembertForMaskedLM.from_pretrained(ckpt)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nHowever its recommended to use the auto class to handel the insentitating of model and tokenizers:\n\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForMaskedLM.from_pretrained(ckpt)\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nWhat’s important is to inderstand how that specific model is trained, which dataset is used, and what’s its limitations and biases.\n\nall of this informastions should be mentioned in the model card (which we will build later)"
  },
  {
    "objectID": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#sharing-a-pretrained-model",
    "href": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#sharing-a-pretrained-model",
    "title": "Hugging Face Course Notes: Chapter4",
    "section": "",
    "text": "In general there’s 3 ways to create a new model reposetories:\n\nUsing the push_to_hub API\nUsing the huggingface_hub Python library\nUsing the web interface\n\nOne the repo is created, we can add and edit files just like any other repo on github\n\n\n\n\nThe simplest way to create a model repo is to use push_to_hub API.\n\nbut first we need to get our credentials in order to use the API:\n\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\n\nWe used earlier the TrainingArguments class to pass hyper-parameters during the building of the training loop, the easiest way to push a model is by setting push_to_hub= True as an arguments:\n\n\nfrom transformers import Trainer, TrainingArguments\ntraining_arguments = TrainingArguments('test-train-0', save_strategy = 'epoch', push_to_hub= True)\n\n\nOnce the model is trained and the trainer.train() is called, the api will upload the model to the hub and save it in a repo with the name we pick test-train-0, but we can chose another name by passing hub_model_id=\"my_model_name\"\nOnce the training is complete we should do the final trainer_push_to_hub() to upload the last version of the model. This will also generate the model card, which contains all the metadata, hyperparameters and evaluation results.\n\n\n\n\nmodel card From Hugging Face\n\n\n\nThe push_to_hub() method can be applied on model, tokenizer, configs. It take care of both: creating the repo and pushing the model and tokenizer directly to that repository.\nNow let’s see exactly how this work:\n\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nckpt = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForMaskedLM.from_pretrained(ckpt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nNow we can take these model and build whatever we want with them, modify, add..and when we are satisfied with the results we can use push_to_hup() method:\n\n\nmodel.push_to_hub('test-ch4')\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/Smail/test-ch4/commit/2f5a775ac7e24540a43374fa820db4e225b195e2', commit_message='Upload BertForMaskedLM', commit_description='', oid='2f5a775ac7e24540a43374fa820db4e225b195e2', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nThis will create a new repo \"test-ch4\" in our profile and populate it with model files. We can do the same with tokenizer\n\n\ntokenizer.push_to_hub('test-ch4')\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/Smail/test-ch4/commit/e0b505500d75768b52d549adc8f67c3f40b015dd', commit_message='Upload tokenizer', commit_description='', oid='e0b505500d75768b52d549adc8f67c3f40b015dd', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nwe can also add the organization, tokenization key and other arguments that can be specified while pushing the model into the hub through API\n\n\n\n\n\nWe can also use the Huggingface_hub library that offer more tools that are simple and very effective to achieve various tasks such as pushing a model, adding files, deleting files, creating repos, editing, managing, getting informations etc.. *Similar to push_to_hub() method the package requires a key token to access the hub:\n\n\n!huggingface-cli login\n\n\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nToken: \nAdd token as git credential? (Y/n) n\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n\n\n\n!huggingface-cli whoami\n\nSmail\n\n\n\nThe huggingface_hub offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n\nfrom huggingface_hub import (\n    #User Management\n    login,\n    logout,\n    whoami,\n\n    # Repo creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n\n)\n\n\nWe could for example try to create a repo like this:\n\n\ncreate_repo('dummi_repo')\n\nRepoUrl('https://huggingface.co/Smail/dummi_repo', endpoint='https://huggingface.co', repo_type='model', repo_id='Smail/dummi_repo')\n\n\n\nOther arguments which may be useful are:\n\nprivate, in order to specify if the repository should be visible from others or not.\ntoken, if you would like to override the token stored in your cache by a given token.\nrepo_type, if you would like to create a dataset or a space instead of a model. Accepted values are “dataset” and “space”.\n\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled\n\n\n\n\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for Git Large File Storage) for larger files.\nIn general there is 3 ways to upload files to the HUB:\n\n\n\n\n\nthis approach doesn’t require installing git or git-lf on our system, it uses HTTP POST requests to push the files directly to the hub.\nIt’s limitation is the size of the file shouldn’t be larger than 5 GB.\nSince I work in a google colab, I will create a folder and work with it “locally”.\n\n\n!mkdir my_folder\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"/content/my_folder\")\ntokenizer.save_pretrained(\"/content/my_folder\")\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n('/content/my_folder/tokenizer_config.json',\n '/content/my_folder/special_tokens_map.json',\n '/content/my_folder/sentencepiece.bpe.model',\n '/content/my_folder/added_tokens.json',\n '/content/my_folder/tokenizer.json')\n\n\n\nfrom huggingface_hub import upload_file\nupload_file(\n    path_or_fileobj=\"/content/my_folder/config.json\",\n    path_in_repo=\"config.json\",\n    repo_id=\"Smail/dummi_repo\",\n)\n\nCommitInfo(commit_url='https://huggingface.co/Smail/dummi_repo/commit/1af5f861f0e0a3398d3856c69ff83bd5f04c372a', commit_message='Upload config.json with huggingface_hub', commit_description='', oid='1af5f861f0e0a3398d3856c69ff83bd5f04c372a', pr_url=None, pr_revision=None, pr_num=None)\n\n\n\nThis will upload the config.json file exists in path_to_file to the root of the repo as config.json.\n\n\n\n\n\nThe repository class abstract a local repo and handle all the work in agit like manner, it requires having git and git-lf installed in our system.\n\n\n!pip install git-LFS\n\nCollecting git-LFS\n  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\nInstalling collected packages: git-LFS\nSuccessfully installed git-LFS-1.6\n\n\n\n!git lfs install\n\nGit LFS initialized.\n\n\n\nfrom huggingface_hub import Repository\n\nrepo = Repository('/content', clone_from= 'Smail/dummi_repo')\n\n\nThis will create a folder in the path we decide and will import all the files in the repo from the hub.\nSince we didn’t create any files in that repo when we insantiate it, the only file we have is .gitattributes, from here we could deal with that repo as if it a git repo:\n\n\nrepo.git_pull()\nrepo.git_add()\nrepo.git_commit()\nrepo.git_push()\nrepo.git_tag()\n\n\nFirst we have to make the repo up-to-date with:\n\n\nrepo.git_pull()\n\n\nWe can now save the model and tokenizer in the directotry\n\n\nmodel.save_pretrained('my_folder')\ntokenizer.save_pretrained('my_folder')\n\n\nNow our local folder countains 2 new files and we could push them to the hub:\n\n\nrepo.git_add()\nrepo.git_commit('add model and tokenizer')\nrepo.git_push()\n\n\n\n\n\nThis method is very similar to the one before, its barbone method that in pure git and bash.\nFirst let’s install git and git-lf:\n\n\n!pip install git-LFS\n\nRequirement already satisfied: git-LFS in /usr/local/lib/python3.10/dist-packages (1.6)\n\n\n\nAnd the initialize git and lfs\n\n\n!git lfs install\n\nUpdated git hooks.\nGit LFS initialized.\n\n\n\nNow we can clone the repo from the hub the normal way:\n\n\n!git clone https://huggingface.co/Smail/test-model-2\n\nCloning into 'test-model-2'...\nremote: Enumerating objects: 4, done.\nremote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 4\nUnpacking objects:  25% (1/4)Unpacking objects:  50% (2/4)Unpacking objects:  75% (3/4)Unpacking objects: 100% (4/4)Unpacking objects: 100% (4/4), 1.12 KiB | 1.12 MiB/s, done.\n\n\n\nSince we are using bash, we could naigate the directory like this:\n\n\n!cd test-model-2 && ls\n\nREADME.md\n\n\n\nThis will chnage the directory the repo we just cloned, and print all the files in it.\nNow we can build a model like we did before and save all the files in the repo we cloned and push them to the hub:\n\n\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"/content/test-model-2\")\ntokenizer.save_pretrained(\"/content/test-model-2\")\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n('/content/test-model-2/tokenizer_config.json',\n '/content/test-model-2/special_tokens_map.json',\n '/content/test-model-2/sentencepiece.bpe.model',\n '/content/test-model-2/added_tokens.json',\n '/content/test-model-2/tokenizer.json')\n\n\n\n!ls\n\nmy_folder  sample_data  test-model-2\n\n\n\n!cd test-model-2 && ls\n\nadded_tokens.json  model.safetensors  sentencepiece.bpe.model  tokenizer_config.json\nconfig.json    README.md          special_tokens_map.json  tokenizer.json\n\n\n\nNow that we’ve saved some model and tokenizer artifacts, let’s take another look at the folder:\n\nIf you look at the file sizes (for example, with ls -lh), you should see that the model state dict file (pytorch_model.bin) is the only outlier, at more than 400 MB.\nWe can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git’s staging environment using the git add command:\n\n!cd test-model-2 && git add .\n\n\n!cd test-model-2 && git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   added_tokens.json\n    new file:   config.json\n    new file:   model.safetensors\n    new file:   sentencepiece.bpe.model\n    new file:   special_tokens_map.json\n    new file:   tokenizer.json\n    new file:   tokenizer_config.json\n\n\n\n\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its status command:\n\n\n!cd test-model-2 && git lfs status\n\nOn branch main\nObjects to be pushed to origin/main:\n\n\nObjects to be committed:\n\n    added_tokens.json (Git: 43734cd)\n    config.json (Git: 4b8db4b)\n    model.safetensors (LFS: 2785d2e)\n    sentencepiece.bpe.model (LFS: 988bc5a)\n    special_tokens_map.json (Git: b547935)\n    tokenizer.json (Git: 9a9362e)\n    tokenizer_config.json (Git: c49982e)\n\nObjects not staged for commit:\n\n\n\n\n\nWe can see that all files have Git as a handler, except pytorch_model.bin and sentencepiece.bpe.model, which have LFS. Great!\nLet’s proceed to the final steps, committing and pushing to the huggingface.co remote repository:\n\n\n!cd test-model-2 && git commit -m \"First model version\"\n\n[main adaa023] First model version\n 7 files changed, 128351 insertions(+)\n create mode 100644 added_tokens.json\n create mode 100644 config.json\n create mode 100644 model.safetensors\n create mode 100644 sentencepiece.bpe.model\n create mode 100644 special_tokens_map.json\n create mode 100644 tokenizer.json\n create mode 100644 tokenizer_config.json\n\n\n*Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:\n\n!cd test-model-2 && git push\n\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 2 threads\nCompressing objects: 100% (8/8), done.\nWriting objects: 100% (9/9), 592.06 KiB | 5.10 MiB/s, done.\nTotal 9 (delta 0), reused 0 (delta 0), pack-reused 0\nTo https://huggingface.co/Smail/test-model-2\n   6266b40..adaa023  main -&gt; main"
  },
  {
    "objectID": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#building-model-card",
    "href": "posts/HuggingFace_4/Hugging_Face_course_Notes_Chapter4.html#building-model-card",
    "title": "Hugging Face Course Notes: Chapter4",
    "section": "",
    "text": "Model card plays a crucial role in open source model, it allow others to build an idea about the important element of the model without spending time and effort, it ensures reusability and reproducibility of the results.\nBy documenting the training and evaluation process we hepl other understand what they expect from the model, its limitations and capabilities, also we have to provide enough iformations about the data we train the model and how it was preprocessed.\nThe model card usually starts with a very brief, high-level overview of what the model is for, followed by additional details in the following sections:\n\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nEvaluation results\n\n\n\n\n\nThe model description provides basic details about the model. This includes the architecture, version, if it was introduced in a paper, if an original implementation is available, the author, and general information about the model. Any copyright should be attributed here. General information about training procedures, parameters, and important disclaimers can also be mentioned in this section.\n\n\n\n\n\nHere you describe the use cases the model is intended for, including the languages, fields, and domains where it can be applied. This section of the model card can also document areas that are known to be out of scope for the model, or where it is likely to perform suboptimally.\n\n\n\n\n\nThis section should include some examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes, and any other code you think might be helpful.\n\n\n\n\n\nThis part should indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome.\n\n\n\n\n\nIn this section you should describe all the relevant aspects of training that are useful from a reproducibility perspective. This includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, the batch size, the learning rate, and so on.\n\n\n\n\n\nHere you should describe the metrics you use for evaluation, and the different factors you are mesuring. Mentioning which metric(s) were used, on which dataset and which dataset split, makes it easy to compare you model’s performance compared to that of other models. These should be informed by the previous sections, such as the intended users and use cases.\n\n\n\n\n\nFinally, provide an indication of how well the model performs on the evaluation dataset. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Blogs",
    "section": "",
    "text": "Understanting Big-O Notation\n\n\n\n\n\n\nPython\n\n\nData-Structure-&-Algorithms\n\n\n\n\n\n\n\n\n\nNov 23, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Build a Large Language Model\n\n\n\n\n\n\nLLM\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNov 23, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Weather CLI with Pydantic and Requests\n\n\n\n\n\n\nPython\n\n\nRequests\n\n\nPydantic\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Book-Tracker CLI with Pydantic\n\n\n\n\n\n\nPython\n\n\nPydantic\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Build a Large Language Model\n\n\n\n\n\n\nLLM\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Course Notes: Chapter5\n\n\n\n\n\n\nHugging Face\n\n\nPytorch\n\n\nDeep Learning\n\n\nNLP\n\n\nLLM\n\n\nHuggingFaceHub\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Course Notes: Chapter4\n\n\n\n\n\n\nHugging Face\n\n\nPytorch\n\n\nDeep Learning\n\n\nNLP\n\n\nLLM\n\n\nHuggingFaceHub\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Choice with Swag Dataset\n\n\n\n\n\n\nHuggingFace\n\n\nPytorch\n\n\nNLP\n\n\nDatasets\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Course Notes: Chapter2\n\n\n\n\n\n\nHugging-Face\n\n\nNLP\n\n\nLLMs\n\n\nPytorch\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Course Notes: Chapter3\n\n\n\n\n\n\nHugging-Face\n\n\nNLP\n\n\nLLMs\n\n\nPytorch\n\n\nDatasets\n\n\nSwag\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Course Notes: Chapter1\n\n\n\n\n\n\nHugging-Face\n\n\nNLP\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nKaggling Tutorial #0: Download A Dataset from Kaggle Using API-Key\n\n\n\n\n\n\nKaggle\n\n\nColab\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding QAs System Using Haystack\n\n\n\n\n\n\nHaystack\n\n\nLLMs\n\n\nQAs\n\n\nNLP\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nUtilizing Existing FAQs for Question Answering\n\n\n\n\n\n\nHaystack\n\n\nLLMs\n\n\nQAs\n\n\nNLP\n\n\nPandas\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Questionnaire\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: Questionnaire\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nPandas\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nNumpy\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: Questionnaire\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Questionnaire\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nNumpy\n\n\nPandas\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nNumpy\n\n\nPandas\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Questionnaire and Further Research\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Questionnaire\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Deep learning for coders with fastai and pytorch\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DSA_NeetCode/Big-O Notation/Big-O.html",
    "href": "posts/DSA_NeetCode/Big-O Notation/Big-O.html",
    "title": "Understanting Big-O Notation",
    "section": "",
    "text": "Big-O notation describes how the runtime or the space requirements of an algorithm Grow Relative to the input size N. In othe words how the algorithm perform as the n becomes very large."
  },
  {
    "objectID": "posts/DSA_NeetCode/Big-O Notation/Big-O.html#what-is-big-o-notation",
    "href": "posts/DSA_NeetCode/Big-O Notation/Big-O.html#what-is-big-o-notation",
    "title": "Understanting Big-O Notation",
    "section": "",
    "text": "Big-O notation describes how the runtime or the space requirements of an algorithm Grow Relative to the input size N. In othe words how the algorithm perform as the n becomes very large."
  },
  {
    "objectID": "posts/DSA_NeetCode/Big-O Notation/Big-O.html#classes-of-big-o-notation",
    "href": "posts/DSA_NeetCode/Big-O Notation/Big-O.html#classes-of-big-o-notation",
    "title": "Understanting Big-O Notation",
    "section": "Classes of Big-O Notation:",
    "text": "Classes of Big-O Notation:\n1. Constant Time: ( O(1) ) - Description: The runtime does not depend on the size of the input (( n )). - Example Use Cases: - Accessing an element in an array by its index. - Assigning a value to a variable. - Why It’s Efficient: The algorithm performs a fixed number of operations, no matter how large the input is.\n\ndef get_first_element(arr):\n    return arr[0]  # Always takes constant time.\n\n\n2. Logarithmic Time: ( O(n) )\n\nDescription: The runtime increases logarithmically as the input size grows. Often seen in divide-and-conquer algorithms.\nExample Use Cases:\n\nBinary search.\nEfficient data structures like binary search trees.\n\nWhy It’s Efficient: The input size is repeatedly divided by 2 (or another factor), so the algorithm performs significantly fewer operations as ( n ) grows.\nExample Code: Binary Search:\n\n\n\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return 'NoT Found!'\n\n\narray = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nbinary_search(array, 99)\n\n'NoT Found!'\n\n\n\n\n3. Linear Time: ( O(n) )\n\nDescription: The runtime increases proportionally with the input size (( n )).\nExample Use Cases:\n\nIterating through an array or list.\nSearching for an element in an unsorted list.\n\nWhy It’s Common: Many real-world algorithms involve processing each element once.\nExample Code: Linear Search:\n\n\ndef find_element(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\n\n\n4. Quasilinear Time: ( O(n n) )\n\nDescription: Often seen in efficient sorting algorithms, where the input is processed ( n ) times with a logarithmic component.\nExample Use Cases:\n\nMerge sort, quicksort (average case).\nHeap sort.\n\nWhy It’s Efficient for Sorting: Dividing and merging elements (logarithmic factor) is combined with processing all elements (( n )).\n\nExample Code: Merge Sort:\n\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    while left and right:\n        if left[0] &lt; right[0]:\n            result.append(left.pop(0))\n        else:\n            result.append(right.pop(0))\n    result.extend(left or right)\n    return result\n\n\n\n\n5. Quadratic Time: ( O(n^2) )\n\nDescription: Runtime grows quadratically with the input size. Often caused by nested loops.\nExample Use Cases:\n\nComparing every pair of elements (e.g., bubble sort).\nBrute-force solutions.\n\nWhy It’s Inefficient: For large ( n ), the runtime grows very quickly.\n\nExample Code: Bubble Sort:\n\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr\n\n\n\n6. Exponential Time: ( O(2^n) )\n\nDescription: Runtime doubles with every additional input. This is usually seen in recursive algorithms solving combinatorial problems.\nExample Use Cases:\n\nGenerating all subsets of a set.\nRecursive solutions without optimization (e.g., Fibonacci numbers).\n\nWhy It’s Expensive: Very inefficient for large ( n ); often impractical.\n\nExample Code: Recursive Fibonacci:\n\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n\n\n7. Factorial Time: ( O(n!) )\n\nDescription: Runtime grows faster than any other common complexity. Seen in brute-force solutions to permutation or combination problems.\nExample Use Cases:\n\nTraveling Salesman Problem (TSP) via brute force.\n\nWhy It’s Impractical: Even small inputs become infeasible (e.g., ( n=10 ), ( 10! = 3,628,800 )).\n\nExample Code: Brute-Force Permutations:\n\nfrom itertools import permutations\n\ndef all_permutations(arr):\n    return list(permutations(arr))\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nBig-O\nExample Algorithm\nSuitability\n\n\n\n\n( O(1) )\nArray access, hash table lookup\nExcellent.\n\n\n( O(n) )\nBinary search, tree operations\nVery efficient.\n\n\n( O(n) )\nLinear search, one loop\nEfficient for large ( n ).\n\n\n( O(n n) )\nMerge sort, quicksort\nGood for large ( n ).\n\n\n( O(n^2) )\nNested loops, bubble sort\nAvoid if possible.\n\n\n( O(2^n) )\nRecursive subset generation\nImpractical for large ( n ).\n\n\n( O(n!) )\nBrute-force permutations\nRarely practical.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define input size range\nn = np.linspace(1, 20, 100)\n\n# Define Big-O functions\nO_1 = np.ones_like(n)\nO_log_n = np.log2(n)\nO_n = n\nO_n_log_n = n * np.log2(n)\nO_n2 = n ** 2\nO_2n = 2 ** n\n\n# Create a figure\nplt.figure(figsize=(10, 6))\n\n# Plot each Big-O function\nplt.plot(n, O_1, label=r\"$O(1)$ (Constant)\", linewidth=2)\nplt.plot(n, O_log_n, label=r\"$O(\\log n)$ (Logarithmic)\", linewidth=2)\nplt.plot(n, O_n, label=r\"$O(n)$ (Linear)\", linewidth=2)\nplt.plot(n, O_n_log_n, label=r\"$O(n \\log n)$ (Quasilinear)\", linewidth=2)\nplt.plot(n, O_n2, label=r\"$O(n^2)$ (Quadratic)\", linewidth=2)\nplt.plot(n, O_2n, label=r\"$O(2^n)$ (Exponential)\", linewidth=2, linestyle=\"dashed\")\n\n# Customize the plot\nplt.ylim(0, 50)  # Focus on a manageable y-axis range\nplt.xlim(1, 20)  # Show for input size 1 to 20\nplt.xlabel(\"Input Size (n)\", fontsize=12)\nplt.ylabel(\"Operations\", fontsize=12)\nplt.title(\"Big-O Notation Growth Rates\", fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/Pydantic-Projects/BookTracker/First_version.html",
    "href": "posts/Pydantic-Projects/BookTracker/First_version.html",
    "title": "Building Book-Tracker CLI with Pydantic",
    "section": "",
    "text": "In this notebook I will create small Book-Tracking CLI that used Pydantic a data validation library.\nThis project serves as a practical exercise to explore Pydantic’s capabilities in a real-world context, focusing on creating, managing, and persisting data models.\n\n\n\n\nData Validation and Type Enforcement:\n\nPydantic uses Python’s type annotations to validate data, ensuring it conforms to specified types and constraints.\n\nCustom Validators and Field Constraints:\n\nYou can define custom validation logic and additional constraints (like min/max lengths, regex patterns) to enforce more complex data requirements.\n\nSerialization and Deserialization:\n\nPydantic models can be easily converted to and from JSON, simplifying data handling in various formats.\n\nDetailed Error Handling:\n\nPydantic provides comprehensive error messages, making it easier to identify and fix data validation issues.\n\nIntegration with Modern Frameworks:\n\nPydantic is widely used in frameworks like FastAPI for request and response validation, ensuring compatibility and leveraging best practices.\n\nEnhanced Code Readability and Productivity:\n\nBy using type annotations and Pydantic models, your code becomes more self-documenting and reduces boilerplate, allowing you to focus on core logic.\n\n\n\n\n\nData Integrity: Ensures data is always in the correct format and meets specified constraints.\nRobustness: Detailed error messages and validation mechanisms help handle edge cases effectively.\nDeveloper Productivity: Simplifies data validation and serialization, improving overall development efficiency.\nIntegration: Seamlessly integrates with modern Python frameworks, enhancing application robustness and maintainability.\n\n\n\n\n\nIn order to showcase the added value that Pydantic provide, I design a project where we will accept data from users store it and use it by the program. This type of situation is where we need to make sure that the data is consistent in each step of the pipeline.\nBook-Tracker is a simple Command-Line Interface (CLI) application for managing a personal book tracker. The application allows users to:\n\nAdd new books to their collection.\nList all books.\nUpdate the reading status of a book.\nDelete books.\n\n\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal, List\nfrom pathlib import Path\nimport json\n\n\n\n\n\nIn Pydantic, BaseModel is a class that serves as the foundation for defining data models. When you create a new model, you subclass BaseModel and define the fields (attributes) of your model using Python type annotations.\nIn our case we will use the BaseModel to create a Book model for all book objects, this will help us to apply certain constraint on each book created by user.\nWith help of Field we can add validation rules for model fields and specify constraints such as default values, minimum and maximum lengths, regex patterns, and more.\n\n\nclass Book(BaseModel):\n    title: str= Field(..., min_length= 1, description= \"title of the book\")\n    author: str= Field(..., min_length= 1, description= \"author of the book\")\n    genre: Optional[str]= Field(None, description=\"genre of the book\")\n    status: Literal['reading', 'to-read', 'finished']= Field(..., min_length= 1, description= 'status of the book')\n\n\nhere we specify the type of each attribute within the model Book and some metadata.\nthis model will be used to create all book objects.\n\n\nBOOK_FILES = Path(\"book.json\")\n\n\ndef load_books() -&gt; List[Book]:\n    if BOOK_FILES.exists():\n        with open(BOOK_FILES, 'r') as file:\n            data = json.load(file)\n            return [Book(**item) for item in data]\n    return []            \n\n\ndef save_books(books: List[Book]):\n    with open(BOOK_FILES, 'w') as f:\n        json.dump([book.model_dump() for book in books], f, indent= 4)\n\nTo manage the book data persistently, we store it in a JSON file. We use two functions: - load_books(): Reads books from the JSON file and returns them as a list of Book objects. - save_books(): Saves the current list of Book objects to the JSON file.\nThe program gracefully handles cases where the JSON file is missing or corrupted.\n\ndef add_book():\n    title = input(\"enter book title: \")\n    author = input(\"enter author of the book: \")\n    genre = input(\"[Optional] enter genre of the book: \")\n\n    while True:\n        print(\"status of the book: 1. reading - 2.to-read -3.finished\")\n        status = input(\"enter your option: \")\n        if status in ['1', '2', '3']:\n            status_map = {'1':'reading', '2':'to-read', '3':'finished'}\n            status = status_map[status]\n            break\n        else:\n            print(\"enter valid option: 1, 2 or 3\")\n                    \n    try:\n         book = Book(title=title, author=author, genre=genre, status=status)\n         print(book)\n         print(type(book))\n         books = load_books()\n         books.append(book)\n         save_books(books)\n         print(f\"Added. {title} by {author}\")\n\n    except ValueError as e:\n        print(f\"Error. {e}\")\n\n\n\n\n\n\n    \n\n\nThe add_book() function is responsible for gathering user input to add a new book to the tracker.\nOnce the inputs are gathered, the function attempts to create a Book instance using Pydantic’s model.\n\n\ndef list_books():\n    books = load_books()\n    if books:\n        print(\"List of Books:\")\n        print()\n        for i, book in enumerate(books, 1):\n            print(f\"{i}. {book.title} by {book.author} - Status: {book.status}\")\n    else:\n        print(\"List of books is empty\")\n        \n\n\nThe list_books() function displays all the books currently stored in the tracker\n\n\ndef update_status():\n    books = load_books()\n    list_books()\n    if not books:\n        print(\"list of books is empty\")\n    index= int(input(\"Choose book to update: \")) -1\n    if 0&lt;= index &lt;= len(books):\n        print(\"Choose udpate option: \")\n        print(\"1. reading\")\n        print(\"2. to-read\")\n        print(\"3. finished\")\n        \n        choice = input(\"Your option: \")\n        status_map = {\n                        '1': 'reading',\n                        '2': 'to-read',\n                        '3': 'finished'\n                     }\n        new_status = status_map.get(choice)\n        books[index].status = new_status\n        save_books(books)\n    else:\n        print(\"enter valid book\")\n        \n\n\nThe update_status() function allow user to update reading status of a targeted book.\n\n\ndef delete_book():\n    books = load_books()\n    list_books()\n    if not books:\n        print(\"Book list is empty\")\n        return\n    else:\n        index = int(input(\"Choose a book to delete: \")) - 1\n        if 0&lt;= index &lt;= len(books):\n            deleted_book = books.pop(index)\n            save_books(books)\n            print(f\"{deleted_book} is deleted from list of book\")\n        else:\n            print(\"Invalid book index\")\n            return\n            \n            \n        \n\n\nThe delete_book() function is called when the user want to remove a book from their list of books,\n\n\ndef main():\n    print(\"Book Tracker CLI, Pick an Option: \")\n    print(\"1. Add a Book\")\n    print(\"2. List of Books\")\n    print(\"3. Update status\")\n    print(\"4. delete Book\")\n    print(\"5. Exit\")\n\n    while True:\n        choice = input(\"Enter your choice: \")\n        if choice == '1':\n            add_book()\n        elif choice == '2':\n            list_books()\n        elif choice == '3':\n            update_status()\n        elif choice == \"4\":\n            delete_book()\n            \n        elif choice == '5':\n            print(\"Exiting program..\")\n            break\n        else:\n            print(\"Invalid choice, please select again: \")\n          \nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\n\nWorking on the Personal Book Tracker CLI has been a rewarding experience in building a functional Python project while learning to use Pydantic for data validation. This small project showcases how concepts like structured data models, file handling, and user interaction can be combined to create a practical application.\n\n\n\nWhile the Book Tracker CLI is functional, there are several ways to expand and improve it:\nAdd a Note-Taking Feature\n- Allow users to attach notes or summaries to each book.\n- Save these notes as .md files to encourage reflective note-taking and structured documentation.\nSearch and Filter Options\n- Add functionality to search for books by title or author.\n- Include filters for viewing books based on status (to-read, reading, or finished) or genre.\nInteractive Features\n- Provide a menu-driven interface for better navigation.\n- Add confirmation prompts for sensitive actions like deleting a book.\nIntegrate a Database\n- Replace the JSON file with a lightweight database like SQLite for better performance and scalability."
  },
  {
    "objectID": "posts/Pydantic-Projects/BookTracker/First_version.html#pydantic-what-why",
    "href": "posts/Pydantic-Projects/BookTracker/First_version.html#pydantic-what-why",
    "title": "Building Book-Tracker CLI with Pydantic",
    "section": "",
    "text": "Data Validation and Type Enforcement:\n\nPydantic uses Python’s type annotations to validate data, ensuring it conforms to specified types and constraints.\n\nCustom Validators and Field Constraints:\n\nYou can define custom validation logic and additional constraints (like min/max lengths, regex patterns) to enforce more complex data requirements.\n\nSerialization and Deserialization:\n\nPydantic models can be easily converted to and from JSON, simplifying data handling in various formats.\n\nDetailed Error Handling:\n\nPydantic provides comprehensive error messages, making it easier to identify and fix data validation issues.\n\nIntegration with Modern Frameworks:\n\nPydantic is widely used in frameworks like FastAPI for request and response validation, ensuring compatibility and leveraging best practices.\n\nEnhanced Code Readability and Productivity:\n\nBy using type annotations and Pydantic models, your code becomes more self-documenting and reduces boilerplate, allowing you to focus on core logic.\n\n\n\n\n\nData Integrity: Ensures data is always in the correct format and meets specified constraints.\nRobustness: Detailed error messages and validation mechanisms help handle edge cases effectively.\nDeveloper Productivity: Simplifies data validation and serialization, improving overall development efficiency.\nIntegration: Seamlessly integrates with modern Python frameworks, enhancing application robustness and maintainability.\n\n\n\n\n\nIn order to showcase the added value that Pydantic provide, I design a project where we will accept data from users store it and use it by the program. This type of situation is where we need to make sure that the data is consistent in each step of the pipeline.\nBook-Tracker is a simple Command-Line Interface (CLI) application for managing a personal book tracker. The application allows users to:\n\nAdd new books to their collection.\nList all books.\nUpdate the reading status of a book.\nDelete books.\n\n\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal, List\nfrom pathlib import Path\nimport json\n\n\n\n\n\nIn Pydantic, BaseModel is a class that serves as the foundation for defining data models. When you create a new model, you subclass BaseModel and define the fields (attributes) of your model using Python type annotations.\nIn our case we will use the BaseModel to create a Book model for all book objects, this will help us to apply certain constraint on each book created by user.\nWith help of Field we can add validation rules for model fields and specify constraints such as default values, minimum and maximum lengths, regex patterns, and more.\n\n\nclass Book(BaseModel):\n    title: str= Field(..., min_length= 1, description= \"title of the book\")\n    author: str= Field(..., min_length= 1, description= \"author of the book\")\n    genre: Optional[str]= Field(None, description=\"genre of the book\")\n    status: Literal['reading', 'to-read', 'finished']= Field(..., min_length= 1, description= 'status of the book')\n\n\nhere we specify the type of each attribute within the model Book and some metadata.\nthis model will be used to create all book objects.\n\n\nBOOK_FILES = Path(\"book.json\")\n\n\ndef load_books() -&gt; List[Book]:\n    if BOOK_FILES.exists():\n        with open(BOOK_FILES, 'r') as file:\n            data = json.load(file)\n            return [Book(**item) for item in data]\n    return []            \n\n\ndef save_books(books: List[Book]):\n    with open(BOOK_FILES, 'w') as f:\n        json.dump([book.model_dump() for book in books], f, indent= 4)\n\nTo manage the book data persistently, we store it in a JSON file. We use two functions: - load_books(): Reads books from the JSON file and returns them as a list of Book objects. - save_books(): Saves the current list of Book objects to the JSON file.\nThe program gracefully handles cases where the JSON file is missing or corrupted.\n\ndef add_book():\n    title = input(\"enter book title: \")\n    author = input(\"enter author of the book: \")\n    genre = input(\"[Optional] enter genre of the book: \")\n\n    while True:\n        print(\"status of the book: 1. reading - 2.to-read -3.finished\")\n        status = input(\"enter your option: \")\n        if status in ['1', '2', '3']:\n            status_map = {'1':'reading', '2':'to-read', '3':'finished'}\n            status = status_map[status]\n            break\n        else:\n            print(\"enter valid option: 1, 2 or 3\")\n                    \n    try:\n         book = Book(title=title, author=author, genre=genre, status=status)\n         print(book)\n         print(type(book))\n         books = load_books()\n         books.append(book)\n         save_books(books)\n         print(f\"Added. {title} by {author}\")\n\n    except ValueError as e:\n        print(f\"Error. {e}\")\n\n\n\n\n\n\n    \n\n\nThe add_book() function is responsible for gathering user input to add a new book to the tracker.\nOnce the inputs are gathered, the function attempts to create a Book instance using Pydantic’s model.\n\n\ndef list_books():\n    books = load_books()\n    if books:\n        print(\"List of Books:\")\n        print()\n        for i, book in enumerate(books, 1):\n            print(f\"{i}. {book.title} by {book.author} - Status: {book.status}\")\n    else:\n        print(\"List of books is empty\")\n        \n\n\nThe list_books() function displays all the books currently stored in the tracker\n\n\ndef update_status():\n    books = load_books()\n    list_books()\n    if not books:\n        print(\"list of books is empty\")\n    index= int(input(\"Choose book to update: \")) -1\n    if 0&lt;= index &lt;= len(books):\n        print(\"Choose udpate option: \")\n        print(\"1. reading\")\n        print(\"2. to-read\")\n        print(\"3. finished\")\n        \n        choice = input(\"Your option: \")\n        status_map = {\n                        '1': 'reading',\n                        '2': 'to-read',\n                        '3': 'finished'\n                     }\n        new_status = status_map.get(choice)\n        books[index].status = new_status\n        save_books(books)\n    else:\n        print(\"enter valid book\")\n        \n\n\nThe update_status() function allow user to update reading status of a targeted book.\n\n\ndef delete_book():\n    books = load_books()\n    list_books()\n    if not books:\n        print(\"Book list is empty\")\n        return\n    else:\n        index = int(input(\"Choose a book to delete: \")) - 1\n        if 0&lt;= index &lt;= len(books):\n            deleted_book = books.pop(index)\n            save_books(books)\n            print(f\"{deleted_book} is deleted from list of book\")\n        else:\n            print(\"Invalid book index\")\n            return\n            \n            \n        \n\n\nThe delete_book() function is called when the user want to remove a book from their list of books,\n\n\ndef main():\n    print(\"Book Tracker CLI, Pick an Option: \")\n    print(\"1. Add a Book\")\n    print(\"2. List of Books\")\n    print(\"3. Update status\")\n    print(\"4. delete Book\")\n    print(\"5. Exit\")\n\n    while True:\n        choice = input(\"Enter your choice: \")\n        if choice == '1':\n            add_book()\n        elif choice == '2':\n            list_books()\n        elif choice == '3':\n            update_status()\n        elif choice == \"4\":\n            delete_book()\n            \n        elif choice == '5':\n            print(\"Exiting program..\")\n            break\n        else:\n            print(\"Invalid choice, please select again: \")\n          \nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/Pydantic-Projects/BookTracker/First_version.html#conclusion",
    "href": "posts/Pydantic-Projects/BookTracker/First_version.html#conclusion",
    "title": "Building Book-Tracker CLI with Pydantic",
    "section": "",
    "text": "Working on the Personal Book Tracker CLI has been a rewarding experience in building a functional Python project while learning to use Pydantic for data validation. This small project showcases how concepts like structured data models, file handling, and user interaction can be combined to create a practical application.\n\n\n\nWhile the Book Tracker CLI is functional, there are several ways to expand and improve it:\nAdd a Note-Taking Feature\n- Allow users to attach notes or summaries to each book.\n- Save these notes as .md files to encourage reflective note-taking and structured documentation.\nSearch and Filter Options\n- Add functionality to search for books by title or author.\n- Include filters for viewing books based on status (to-read, reading, or finished) or genre.\nInteractive Features\n- Provide a menu-driven interface for better navigation.\n- Add confirmation prompts for sensitive actions like deleting a book.\nIntegrate a Database\n- Replace the JSON file with a lightweight database like SQLite for better performance and scalability."
  },
  {
    "objectID": "posts/Fastai_ch6/Questionnaire.html",
    "href": "posts/Fastai_ch6/Questionnaire.html",
    "title": "Chapter 6: Questionnaire",
    "section": "",
    "text": "Q1:\nHow could multi-label classification improve the usability of the bear classifier?\nMulti-label classfication gives the models in production the ability to return 0 prediction if the user inputs an image that doesn't contains any of the classes we trained the model to predict. In the case of bear classification inference, if we upload an image that doesn't contains any bear, our model still predict  a class.\nQ2:\nHow do we encode the dependent variable in a multi-label classification problem?\nIn multi-label classification we use One-Hot encoders, which is represented as an list with zeros and ones, where one(s) represent the label of that particular image.\nQ3:\nHow do you access the rows and columns of a DataFrame as if it was a matrix?\nWe could use .iloc or loc to access any row or colum of a DataFrame\nQ4:\nHow do you get a column by name from a DataFrame?\n\nimport pandas as pd\n\n\ndic = {'col1':[1, 2, 3], 'col2':[4, 5, 6]}\n\n\ndf =  pd.DataFrame(dic)\ndf\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\n\n# access column by name:\ndf.col1\n\n0    1\n1    2\n2    3\nName: col1, dtype: int64\n\n\n\n# or this way\ndf['col2']\n\n0    4\n1    5\n2    6\nName: col2, dtype: int64\n\n\nQ5:\nWhat is the difference between a Dataset and DataLoader?\nDataset could be any collection the return a tuple with dependent and independent variables when we index to it (DataFrame in our case)\nDataLoader is an iterator that provide the stream of mini batch, shuffling the datapoints..\nQ6:\nWhat does a Datasets object normally contain?\nDatasets objects contains Training Dataset and Validation Dataset\nQ7:\nWhat does a DataLoaders object normally contain?\nDataloaders contains training DataLoader and Validation DataLoader\nQ8:\nWhat does lambda do in Python?\nIt provides a shortcut for declaring small anonymous functions\nQ9:\nWhat are the methods to customize how the independent and dependent variables are created with the data block API?\nWe could use functions to customize how to define the independent and dependent variable, then assign these functions to get_x and get_y \nQ10:\nWhy is softmax not an appropriate output activation function when using a one hot encoded target?\nSoftmax pushes the model to pick one class among others, while when using one-hot encoding with multi-label classification we have more the one class per image.\nQ12:\nWhat is the difference between nn.BCELoss and nn.BCEWithLogitsLoss?\nnn.BCELoss has no sigmoid while nn.BCEWithLogitsLoss includes sigmoid in it.\nQ13:\nWhen is it okay to tune a hyperparameter on the validation set?\nWhen the results seems to have a smooth curve, what indicates the results change  based on changes to the hyper-parameters.\nQ14:\nHow is y_range implemented in fastai?\nfunction define the range of our targets. In fastai this function is implemented using the `sigmoid_range`\nQ16:\nWhat is a regression problem? What loss function should you use for such a problem?\nRegression problem is where we have to predict a continous value.\nUsually in this kind of problem the mean squared error is used\nQ17:\nWhat do you need to do to make sure the fastai library applies the same data augmentation to your input images and your target point coordinates?\nThe PointBlock class needs to be passed to the blocks parameter."
  },
  {
    "objectID": "posts/Build a Large Language Model/Chapter-1/Build_A_Large_Language_Model_Chapter_1_.html",
    "href": "posts/Build a Large Language Model/Chapter-1/Build_A_Large_Language_Model_Chapter_1_.html",
    "title": "Chapter 1: Build a Large Language Model",
    "section": "",
    "text": "In this series on notebooks I will share my learning/note-taking of the book Build a Large Lunguage Model(From Scratch) by the Author Sebastian Raschka.\n\n\n\n\n├── chapters  \n│   ├── chapter1_understanding_LLMs: high-level introduction to the fundamental concepts behind LLMs.  \n│   ├── chapter2_text_data: It covers the process ofpreparing text for LLM training, including splitting text into word and subword tokens.  \n│   ├── chapter3_attention_mechanisms:  It introduces a basicself-attention framework and progresses to an enhanced self-attention mechanism.  \n│   ├── chapter4_GPT_model: focuses on coding a GPT-like LLM that can be trained to generatehuman-like text.  \n│   ├── chapter5_pretraining: implements the pretraining process of LLMs.  \n│   ├── chapter6_text_classification:  introduces different LLM fine-tuning approaches.  \n│   ├── chapter7_instruction_following:  explores the instruction fine-tuning process of LLMs.  \n└──  \n\nThe aim of this chapter is to introduce the foundational concepts of large language models (LLMs) and the advancements in deep learning that made them possible\n\nthis chapter doesn’t contain any code.\n\nLarge language models (LLMs), like OpenAI’s ChatGPT, are deep neural networks that revolutionized natural language processing (NLP) in recent years.\nTraditional NLP methods excelled in tasks like spam classification and simple pattern recognition but struggled with complex tasks requiring advanced understanding and generation abilities.\nContemporary LLMs can handle sophisticated language tasks, such as writing an email from keywords, which was challenging for earlier models.\nWhen we say language models “understand,” we mean they can produce text that seems coherent and contextually appropriate, not that they have human-like awareness or true comprehension.\nThe transformer architecture and large datasets have driven the shift in NLP, enabling more advanced language understanding and interaction.\n\n\n\n\nLLM’s are neural network designed to understand and produce huma-like text.\nLarge in LLM refer to the size of the datasets those model trained on, but also on the size of parameters ( 100’s of billions)\n\nParameters are adjusted weights during training to predict next word in sentence.\n\nThe architecture of an LLM is called transformers which apply the attention mechanism to different parts of the input while performing the next word prediction. ### Applications of LLM’s:\nLLM’s can be used in many contexts to perform different tasks:\n\nmachine translation\nsentiments analysis\ntext generation\n..\n\n\n\n\n\n\nBuilding LLM form scratch allow us to understand the mechanics and limitations of language models, and provide us with skills set required for pretraining or fine-tuning phase.\nCustom-built LLM outperform general purpose one.\n\nMany companies prefer to build their own domain-specific llm to keep their private data in-home and not share it with third party.\ndeveloping small lm open the door for deployment on devices like laptops or even mobiles rather than huge servers.\n\ncreating LLM is a process where pre-training and fine-tuning takes place.\n\npre indicates that it is the first phase, model is trained on huge chunk of data where it learns basic knowledge and broad pattern of the language.\nthe fine-tuning phase is where the model get further training but on very specific task and get its knowledge narrowed.\n\nFine-tuning can be devised in 2 category:\n\nInstruction fine-tuning: where the model get trained one pair of instruction =&gt; output dataset.\nWhere classification tuning the data consist of text and associated class label.\n\n\n\n\n\n\nAll modern LLM rely on Transformer architecture which was presented for the first time in this famous paper: Attention is all you need.\nTransformer consist of two submodal: 1-encoder and 2-decoder. - encoder module process the input text into some numerical representation that capture meaning.\n- decoder uses the numerical values and generate text\n\nthe key component of the transformer architecture is attention mechanism, we will talk about it later.\nTransformer Variants:\n- Models like BERT and GPT are based on the original transformer architecture but adapt it for different tasks.\n- BERT’s Training Strategy: BERT uses a masked word prediction approach, where it predicts missing words in a sentence, making it suitable for tasks like text classification and sentiment analysis.\n- GPT vs. BERT: GPT is designed for generative tasks, whereas BERT excels in tasks requiring understanding of context, like sentiment prediction and document categorization.\n- BERT’s Real-world Application: Platforms like X (formerly Twitter) use BERT for tasks such as detecting toxic content.\nGPT Focus: GPT utilizes the Decoder portion of the transformer architecture and is designed for text generation tasks.\nZero-shot and Few-shot Learning: GPT models excel in zero-shot learning, meaning they can handle tasks without specific prior examples. They also perform well in few-shot learning, where they learn from a small number of provided examples.\nVersatility: While GPT models are optimized for text completion, they exhibit broad adaptability and can tackle a wide range of tasks, showcasing their flexibility in natural language processing.\n\n\n\n\n\nDiverse Training Data: Large datasets used for training GPT- and BERT-like models contain billions of words, covering a broad range of topics and languages (both natural and programming).\nComprehensive Corpus: These datasets are designed to ensure comprehensive exposure to diverse linguistic and contextual patterns.\n\n\n\n\n\nGPT Origin: GPT was introduced in the paper Improving Language Understanding by Generative Pre-Training by Radford et al. from OpenAI.\nGPT-3: A scaled-up version of the original GPT with more parameters and a larger training dataset.\nChatGPT’s Base Model: The initial ChatGPT model was derived by fine-tuning GPT-3 on a large instruction dataset, using methods from OpenAI’s InstructGPT paper.\nModel Versatility: Despite being trained on a simple next-word prediction task, GPT models excel in various tasks like text completion, spelling correction, classification, and language translation.\nSelf-Supervised Learning: The next-word prediction task is a type of self-supervised learning, where the model uses the structure of the data itself for training.\nLabel Creation: Labels are generated dynamically, with the next word in a sentence or document serving as the prediction target.\nTraining on Massive Datasets: This approach enables the use of large, unlabeled text datasets for training, as explicit labeling of data is unnecessary.\n\n\n\n\n\nNow we understand the basic theory behind LLM and how they were introduced, its time to build them from scratch.\n\n &gt;Source: Book: Build A Large Language Model by Sebastian Raschka"
  },
  {
    "objectID": "posts/Build a Large Language Model/Chapter-1/Build_A_Large_Language_Model_Chapter_1_.html#structure-of-the-book",
    "href": "posts/Build a Large Language Model/Chapter-1/Build_A_Large_Language_Model_Chapter_1_.html#structure-of-the-book",
    "title": "Chapter 1: Build a Large Language Model",
    "section": "",
    "text": "├── chapters  \n│   ├── chapter1_understanding_LLMs: high-level introduction to the fundamental concepts behind LLMs.  \n│   ├── chapter2_text_data: It covers the process ofpreparing text for LLM training, including splitting text into word and subword tokens.  \n│   ├── chapter3_attention_mechanisms:  It introduces a basicself-attention framework and progresses to an enhanced self-attention mechanism.  \n│   ├── chapter4_GPT_model: focuses on coding a GPT-like LLM that can be trained to generatehuman-like text.  \n│   ├── chapter5_pretraining: implements the pretraining process of LLMs.  \n│   ├── chapter6_text_classification:  introduces different LLM fine-tuning approaches.  \n│   ├── chapter7_instruction_following:  explores the instruction fine-tuning process of LLMs.  \n└──  \n\nThe aim of this chapter is to introduce the foundational concepts of large language models (LLMs) and the advancements in deep learning that made them possible\n\nthis chapter doesn’t contain any code.\n\nLarge language models (LLMs), like OpenAI’s ChatGPT, are deep neural networks that revolutionized natural language processing (NLP) in recent years.\nTraditional NLP methods excelled in tasks like spam classification and simple pattern recognition but struggled with complex tasks requiring advanced understanding and generation abilities.\nContemporary LLMs can handle sophisticated language tasks, such as writing an email from keywords, which was challenging for earlier models.\nWhen we say language models “understand,” we mean they can produce text that seems coherent and contextually appropriate, not that they have human-like awareness or true comprehension.\nThe transformer architecture and large datasets have driven the shift in NLP, enabling more advanced language understanding and interaction.\n\n\n\n\nLLM’s are neural network designed to understand and produce huma-like text.\nLarge in LLM refer to the size of the datasets those model trained on, but also on the size of parameters ( 100’s of billions)\n\nParameters are adjusted weights during training to predict next word in sentence.\n\nThe architecture of an LLM is called transformers which apply the attention mechanism to different parts of the input while performing the next word prediction. ### Applications of LLM’s:\nLLM’s can be used in many contexts to perform different tasks:\n\nmachine translation\nsentiments analysis\ntext generation\n..\n\n\n\n\n\n\nBuilding LLM form scratch allow us to understand the mechanics and limitations of language models, and provide us with skills set required for pretraining or fine-tuning phase.\nCustom-built LLM outperform general purpose one.\n\nMany companies prefer to build their own domain-specific llm to keep their private data in-home and not share it with third party.\ndeveloping small lm open the door for deployment on devices like laptops or even mobiles rather than huge servers.\n\ncreating LLM is a process where pre-training and fine-tuning takes place.\n\npre indicates that it is the first phase, model is trained on huge chunk of data where it learns basic knowledge and broad pattern of the language.\nthe fine-tuning phase is where the model get further training but on very specific task and get its knowledge narrowed.\n\nFine-tuning can be devised in 2 category:\n\nInstruction fine-tuning: where the model get trained one pair of instruction =&gt; output dataset.\nWhere classification tuning the data consist of text and associated class label.\n\n\n\n\n\n\nAll modern LLM rely on Transformer architecture which was presented for the first time in this famous paper: Attention is all you need.\nTransformer consist of two submodal: 1-encoder and 2-decoder. - encoder module process the input text into some numerical representation that capture meaning.\n- decoder uses the numerical values and generate text\n\nthe key component of the transformer architecture is attention mechanism, we will talk about it later.\nTransformer Variants:\n- Models like BERT and GPT are based on the original transformer architecture but adapt it for different tasks.\n- BERT’s Training Strategy: BERT uses a masked word prediction approach, where it predicts missing words in a sentence, making it suitable for tasks like text classification and sentiment analysis.\n- GPT vs. BERT: GPT is designed for generative tasks, whereas BERT excels in tasks requiring understanding of context, like sentiment prediction and document categorization.\n- BERT’s Real-world Application: Platforms like X (formerly Twitter) use BERT for tasks such as detecting toxic content.\nGPT Focus: GPT utilizes the Decoder portion of the transformer architecture and is designed for text generation tasks.\nZero-shot and Few-shot Learning: GPT models excel in zero-shot learning, meaning they can handle tasks without specific prior examples. They also perform well in few-shot learning, where they learn from a small number of provided examples.\nVersatility: While GPT models are optimized for text completion, they exhibit broad adaptability and can tackle a wide range of tasks, showcasing their flexibility in natural language processing.\n\n\n\n\n\nDiverse Training Data: Large datasets used for training GPT- and BERT-like models contain billions of words, covering a broad range of topics and languages (both natural and programming).\nComprehensive Corpus: These datasets are designed to ensure comprehensive exposure to diverse linguistic and contextual patterns.\n\n\n\n\n\nGPT Origin: GPT was introduced in the paper Improving Language Understanding by Generative Pre-Training by Radford et al. from OpenAI.\nGPT-3: A scaled-up version of the original GPT with more parameters and a larger training dataset.\nChatGPT’s Base Model: The initial ChatGPT model was derived by fine-tuning GPT-3 on a large instruction dataset, using methods from OpenAI’s InstructGPT paper.\nModel Versatility: Despite being trained on a simple next-word prediction task, GPT models excel in various tasks like text completion, spelling correction, classification, and language translation.\nSelf-Supervised Learning: The next-word prediction task is a type of self-supervised learning, where the model uses the structure of the data itself for training.\nLabel Creation: Labels are generated dynamically, with the next word in a sentence or document serving as the prediction target.\nTraining on Massive Datasets: This approach enables the use of large, unlabeled text datasets for training, as explicit labeling of data is unnecessary.\n\n\n\n\n\nNow we understand the basic theory behind LLM and how they were introduced, its time to build them from scratch.\n\n &gt;Source: Book: Build A Large Language Model by Sebastian Raschka"
  },
  {
    "objectID": "posts/Fastai_ch4/Questionnaire.html",
    "href": "posts/Fastai_ch4/Questionnaire.html",
    "title": "Chapter 4: Questionnaire",
    "section": "",
    "text": "Q1:\nHow is a grayscale image represented on a computer? How about a color image? ___ * Grayscale image is way of turning an array/tensor to grayscale value on each pixel of that image, the values went from 0 to 256, the darker the pixel the closer to 256.\nQ2:\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n___\n* MNIST_SAMPLE contains two folders Train and Valid.\n* This method of structuring dataset help the community to compare the results between models by setting the same framework.\nQ3:\nExplain how the “pixel similarity” approach to classifying digits works.\n___\n* First we turn images in tensor, then we stack them together. * we take the mean value of each pixel for all images, this will give us an image that each pixel of it represent the mean of all datset. * Then we classify images by comparing the mean absolute error between that image and the ideal3/ideal7 and see which return low distance.\nQ4:\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n___\n\n# list with odd and even numbers\nlisst =  [1, 2, 3, 4, 5, 31, 17, 70]\n# using list-comprehension to select only odd numbers\ndouble_odd = [2*i for i in lisst if i%2==1]\ndouble_odd\n\n[2, 6, 10, 62, 34]\n\n\nQ5:\nWhat is a “rank-3 tensor”?\n___ * It’s a tensor with 3 dimensions, each can be represented as an array (array of array of array), it’s basically a cube.\nQ6:\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\n____\n* Rank represent the dimesion of the tensorm while shape tells how many elemen there’s in each dimension.\nQ7:\nWhat are RMSE and L1 norm?\n___\n* RMSE also called L2 stands dor Root Mean Square Error, while L1 is Least Absolute Error. * This functions are basically the same, we used them to measure distance.\nQ9:\nCreate a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.\n___\n\nimport torch\n\n\nr3_tens = torch.Tensor(list(range(1,10))).view(3,3)\nr3_tens\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\ndouble_tens= 2*r3_tens\ndouble_tens\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\ndouble_tens[1:, 1:]\n\ntensor([[10., 12.],\n        [16., 18.]])\n\n\nQ10:\nWhat is broadcasting?\n___\n* It refers to mathematical operation between different dimensions arrays and tensors.\nQ11:\nAre metrics generally calculated using the training set, or the validation set? Why?\n_____ * Metrics are calculated on validation set, so we have good measure of the model performance.\nQ12&13:\nWhat is SGD?\nWhy does SGD use mini-batches?\n____ * SGD or Stochastic Gradient Descent is an optimization function that help us to update weights and minimize the loss. * SGD updates gradients after each mini batch, otherwise it will take a lot of time if we decide to update the gradients after going through all the dataset, or the model won’t learn much if we decide to updates the gradients after each data point.\nQ14:\nWhat are the seven steps in SGD for machine learning?\n___\n* Initialize the parameters * Calculate the predictions * Calculate the loss * Calculate the gradients * Step the weights * Redo the whole process from step 2 * Stop\nQ15:\nHow do we initialize the weights in a model?\n___\n* Usually we initialize weights by picking random values\nQ16:\nWhat is “loss”?\n___\n* The loss is function that uses the model in order to optimizes it’s predictions\nQ17:\nWhat’s the gradients?\n___\n* The gradients are values that dictate how much should we change the weights in order to minimize the loss\nQ18:\nWhy can’t we always use a high learning rate?\n___\n* Picking a large learning rate will get the loss worse.\nQ19:\nDo you need to know how to calculate gradients yourself? ___\n* It’s important to understand the math behind each concept in Deep Learning, but we don’t need to do everything by ourself, we could use frameworks like pytorch and fastai.\nQ20:\nWhy can’t we use accuracy as a loss function?\n___\n* Loss function changes as the weights changes, but the accuracy only changes when the predictions change.\nQ21:\nDraw the sigmoid function. What is special about its shape?\n___\n* Sigmoid takes an input and return a number always between 0 and 1.\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\n\n\n\nQ22:\nWhat is the difference between a loss function and a metric?\n___\n* Loss is what model uses to optimize the predidictions, while metrics is what we (the ML practitioner) use to understand the performance of the model.\nQ23:\nWhat is the function to calculate new weights using a learning rate?\n___\n* The optimizer function\nQ24:\nWhat does the DataLoader class do?\n___ * Can be used to iterate through data, create batches, transform data..\nQ25:\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n____\n\npredictions = linear_model(x)\nloss = mnist_loss(predictions, y)\nloss.backward()\nfor parameter in parameters:\n    parameter.data -= parameter.grad.data * learning_rate\n    parameter.grad = None\nQ26:\nCreate a function which, if passed two arguments [1,2,3,4] and ‘abcd’ , returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . What is special about that output data structure?\n___\n\nThis kind of datascructure is convinient for machine learning where we need to iterate through dataset.\n\n\ninputs = [1, 2, 3, 4]\nlabels = ['a', 'b', 'c', 'd']\ndef data_func(xb, yb):\n    return list(zip(xb, yb))\ndata_func(inputs, labels)\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nQ27:\nWhat does view in Pytorch do? ____\n* It changes the shape of the tensor without changing it content.\nQ28:\nWhat are the “bias” parameters in a neural network? Why do we need them?\n___\n* Bias allow us to all kind of multiplications without thinking if the inputs are zero in some cases.\nQ29:\nWhat does the @ operator do in Python?\n___\n* In python @ is used to do matrix multiplication.\nQ30:\nWhat does the backward method do?\n___ * Backward tells pytorch to calculate the change in the gradients at that point\nQ31:\nWhy do we have to zero the gradients?\n___ * zero gradients tell pytorch to not track the changes in gradients while we updates the weights.\nQ32:\nWhat information do we have to pass to Learner?\n___ * things we pass to Learner : - DataLoaders - architecture - loss_func - metrics\nQ33:\nShow Python or pseudocode for the basic steps of a training loop.\n____\n\n\n    def train_epoch(model, lr, params):\n        for xb,yb in dl:\n            calc_grad(xb, yb, model)\n            for p in params:\n                p.data -= p.grad*lr\n                p.grad.zero_()\n    for i in range(5):\n        train_epoch(model, lr, params)\n\n\nQ34:\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\n___ * ReLU stands from Rectified Linear Unit. This non-linear finction return any negative activations into zero. \nQ35:\nWhat is an “activation function”?\n___ * An activation function is a non-linear function that takes the outputs activations fron one layer of the neural network as inputs and output it after some kind of computation to another layer o NN."
  },
  {
    "objectID": "posts/Fastai_ch4/Questionnaire.html#questionnaire",
    "href": "posts/Fastai_ch4/Questionnaire.html#questionnaire",
    "title": "Chapter 4: Questionnaire",
    "section": "",
    "text": "Q1:\nHow is a grayscale image represented on a computer? How about a color image? ___ * Grayscale image is way of turning an array/tensor to grayscale value on each pixel of that image, the values went from 0 to 256, the darker the pixel the closer to 256.\nQ2:\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n___\n* MNIST_SAMPLE contains two folders Train and Valid.\n* This method of structuring dataset help the community to compare the results between models by setting the same framework.\nQ3:\nExplain how the “pixel similarity” approach to classifying digits works.\n___\n* First we turn images in tensor, then we stack them together. * we take the mean value of each pixel for all images, this will give us an image that each pixel of it represent the mean of all datset. * Then we classify images by comparing the mean absolute error between that image and the ideal3/ideal7 and see which return low distance.\nQ4:\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n___\n\n# list with odd and even numbers\nlisst =  [1, 2, 3, 4, 5, 31, 17, 70]\n# using list-comprehension to select only odd numbers\ndouble_odd = [2*i for i in lisst if i%2==1]\ndouble_odd\n\n[2, 6, 10, 62, 34]\n\n\nQ5:\nWhat is a “rank-3 tensor”?\n___ * It’s a tensor with 3 dimensions, each can be represented as an array (array of array of array), it’s basically a cube.\nQ6:\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\n____\n* Rank represent the dimesion of the tensorm while shape tells how many elemen there’s in each dimension.\nQ7:\nWhat are RMSE and L1 norm?\n___\n* RMSE also called L2 stands dor Root Mean Square Error, while L1 is Least Absolute Error. * This functions are basically the same, we used them to measure distance.\nQ9:\nCreate a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.\n___\n\nimport torch\n\n\nr3_tens = torch.Tensor(list(range(1,10))).view(3,3)\nr3_tens\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\ndouble_tens= 2*r3_tens\ndouble_tens\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\ndouble_tens[1:, 1:]\n\ntensor([[10., 12.],\n        [16., 18.]])\n\n\nQ10:\nWhat is broadcasting?\n___\n* It refers to mathematical operation between different dimensions arrays and tensors.\nQ11:\nAre metrics generally calculated using the training set, or the validation set? Why?\n_____ * Metrics are calculated on validation set, so we have good measure of the model performance.\nQ12&13:\nWhat is SGD?\nWhy does SGD use mini-batches?\n____ * SGD or Stochastic Gradient Descent is an optimization function that help us to update weights and minimize the loss. * SGD updates gradients after each mini batch, otherwise it will take a lot of time if we decide to update the gradients after going through all the dataset, or the model won’t learn much if we decide to updates the gradients after each data point.\nQ14:\nWhat are the seven steps in SGD for machine learning?\n___\n* Initialize the parameters * Calculate the predictions * Calculate the loss * Calculate the gradients * Step the weights * Redo the whole process from step 2 * Stop\nQ15:\nHow do we initialize the weights in a model?\n___\n* Usually we initialize weights by picking random values\nQ16:\nWhat is “loss”?\n___\n* The loss is function that uses the model in order to optimizes it’s predictions\nQ17:\nWhat’s the gradients?\n___\n* The gradients are values that dictate how much should we change the weights in order to minimize the loss\nQ18:\nWhy can’t we always use a high learning rate?\n___\n* Picking a large learning rate will get the loss worse.\nQ19:\nDo you need to know how to calculate gradients yourself? ___\n* It’s important to understand the math behind each concept in Deep Learning, but we don’t need to do everything by ourself, we could use frameworks like pytorch and fastai.\nQ20:\nWhy can’t we use accuracy as a loss function?\n___\n* Loss function changes as the weights changes, but the accuracy only changes when the predictions change.\nQ21:\nDraw the sigmoid function. What is special about its shape?\n___\n* Sigmoid takes an input and return a number always between 0 and 1.\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\n\n\n\nQ22:\nWhat is the difference between a loss function and a metric?\n___\n* Loss is what model uses to optimize the predidictions, while metrics is what we (the ML practitioner) use to understand the performance of the model.\nQ23:\nWhat is the function to calculate new weights using a learning rate?\n___\n* The optimizer function\nQ24:\nWhat does the DataLoader class do?\n___ * Can be used to iterate through data, create batches, transform data..\nQ25:\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n____\n\npredictions = linear_model(x)\nloss = mnist_loss(predictions, y)\nloss.backward()\nfor parameter in parameters:\n    parameter.data -= parameter.grad.data * learning_rate\n    parameter.grad = None\nQ26:\nCreate a function which, if passed two arguments [1,2,3,4] and ‘abcd’ , returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . What is special about that output data structure?\n___\n\nThis kind of datascructure is convinient for machine learning where we need to iterate through dataset.\n\n\ninputs = [1, 2, 3, 4]\nlabels = ['a', 'b', 'c', 'd']\ndef data_func(xb, yb):\n    return list(zip(xb, yb))\ndata_func(inputs, labels)\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nQ27:\nWhat does view in Pytorch do? ____\n* It changes the shape of the tensor without changing it content.\nQ28:\nWhat are the “bias” parameters in a neural network? Why do we need them?\n___\n* Bias allow us to all kind of multiplications without thinking if the inputs are zero in some cases.\nQ29:\nWhat does the @ operator do in Python?\n___\n* In python @ is used to do matrix multiplication.\nQ30:\nWhat does the backward method do?\n___ * Backward tells pytorch to calculate the change in the gradients at that point\nQ31:\nWhy do we have to zero the gradients?\n___ * zero gradients tell pytorch to not track the changes in gradients while we updates the weights.\nQ32:\nWhat information do we have to pass to Learner?\n___ * things we pass to Learner : - DataLoaders - architecture - loss_func - metrics\nQ33:\nShow Python or pseudocode for the basic steps of a training loop.\n____\n\n\n    def train_epoch(model, lr, params):\n        for xb,yb in dl:\n            calc_grad(xb, yb, model)\n            for p in params:\n                p.data -= p.grad*lr\n                p.grad.zero_()\n    for i in range(5):\n        train_epoch(model, lr, params)\n\n\nQ34:\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\n___ * ReLU stands from Rectified Linear Unit. This non-linear finction return any negative activations into zero. \nQ35:\nWhat is an “activation function”?\n___ * An activation function is a non-linear function that takes the outputs activations fron one layer of the neural network as inputs and output it after some kind of computation to another layer o NN."
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html",
    "href": "posts/Fastai_ch7/Ch7.html",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "In this chapter we will discuss more advanced technics for training image classifier models and getting better results.\nIn this chapter we will see:\n\nWhat is Normalization?\nData augmentation with Mixup\nProgressive Risizing\nTest Time Augmentation\n\nTo implement all those technics, we will build model from scartch an train it on a subset of ImageNet called Imagenette\n\n\n\n\nThis dataset is created by Fastai community, the goal of it is to have a dataset that can train models that generelize well on the larger version (ImageNet), which will help Machine learning practitioners to build and experiment many ideas and projects with less computation power.\nImagenette has 10 classes, which are very different from one another.\nLet’s download the dataset\n\n\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)\n\n\n\n\n\n\n    \n      \n      100.00% [1557168128/1557161267 00:38&lt;00:00]\n    \n    \n\n\n\nHere we create DataLoaders using the Resize technic we saw in CH5\n\n\ndblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64)\n\n\nNow we do the training just to have something to begin with\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.615762\n1.960073\n0.446229\n02:59\n\n\n1\n1.231709\n1.436732\n0.542943\n03:01\n\n\n2\n0.946307\n0.957584\n0.703883\n03:01\n\n\n3\n0.710163\n0.686722\n0.788648\n02:59\n\n\n4\n0.581876\n0.551411\n0.833831\n02:57\n\n\n\n\n\n\nNot bad result considering that we didn’t use a pretrained model.\nThe aim of this chapter is to increase the peformance of this base-line model by implementing different technics\n\n\n\n\n\nNormalization means having a dataset with mean of 0 and standard diviation of 1.\nMost images and computer vision libraries use values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is not going to have a mean of 0 and a standard deviation of 1.\nLet’s grab a batch of data and take the average of each axis except the channels axis (indx==1)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([0.4573, 0.4509, 0.4188], device='cuda:0'),\n TensorImage([0.2730, 0.2652, 0.2803], device='cuda:0'))\n\n\n\nAs expected, the mean and std is different than what we desire.\nFastai provide the method Normalize which can be applied on the whole batch, so we could add it to batch_tfms section of the datablock, we just need to tell fastai which are the mean and the standard deviation we want to use (in this case we will use ImageNet stats) but even without giving these stats fastai will calculate them from one batch and use them.\nNotice here we build the datablock withing a function that take batch_size and size as parameters, we will see way later\n\n\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\n\ndls = get_dls(64, 224)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([-0.0920, -0.0490,  0.0548], device='cuda:0'),\n TensorImage([1.1969, 1.1888, 1.2855], device='cuda:0'))\n\n\n\nThis Normalize method help us to get close to the desired values for mean and std\nLet’s how this effect the performance of the model:\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.617465\n1.945767\n0.462285\n02:58\n\n\n1\n1.272744\n2.252786\n0.462659\n02:58\n\n\n2\n0.947452\n1.084139\n0.643017\n02:58\n\n\n3\n0.722699\n0.795408\n0.765123\n03:00\n\n\n4\n0.610676\n0.577560\n0.819642\n02:58\n\n\n\n\n\n\nImplementing the normalization didn’t help the model get better results, because this technic works better when using a pretrained model.\n\n\n\n\n\nThe idea behind Progressive Resizing is to start training the model with small images, and end the training fase with large images.\nAs we have seen, the image size doesn’t play a role in the learning process.\nAlthough changing the image size in the middel of training will effect how the model behave in a way or another.\nThe best way is to deal with the transition from small image to larger images as if we do transfer learning. After changing the images size we should fine_tune method.\nWe can look to Progressive Resizing as a form of data augmentation, which mean the model will generalize well.\nTo implement progressive resizing it is most convenient if you first create a get_dls function which takes an image size and a batch size as we did in the section before, and returns your DataLoaders:\nNow you can create your DataLoaders with a small size and use fit_one_cycle in the usual way, training for a few less epochs than you might otherwise do:\n\n\ndls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n\nThen you can replace the DataLoaders inside the Learner, and fine-tune:\n\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n\nWe can increase the image size by a bit, untill we reach the normal size of the actual image in dataset.\nProgressive resizing can be good or bad depending many factors:\n\nFor transfer learning, if the model is already trained on similar task we will use it for, and on similar size of images, normally re-training it on smaller images will damage the performance.\nIn other hand if the model is trained on dataset with different size of images, and on dfferent task, using progressive resizing may help the model performance.\n\nThere’s no right answear for every situation, we just need to try and experiment different things.\n\n\n\n\n\nTill now all kind of data augmentations we implement are done on training set, while the validation set is taking the same images.\nThe idea behind Test Time Augmentation tta is to use some augmented images for the validation set to get predictions (for the same image) and average (or take the maximun) them.\nUsually fastai do center cropping for validation.\nThis method may cause the model to miss valuable lessons from edges of the cropped images.\nWe could avoid center cropping but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, then take the average of predictions or the maximum\nBy default, fastai will use the unaugmented center crop image plus four randomly augmented images.\nYou can pass any DataLoader to fastai’s tta method; by default, it will use your validation set:\n\n\npreds,targs = learn.tta()\naccuracy(preds, targs).item()\n\n\n\n\n\nThe idea of mixup is to take two data points (images) and mix them together with some percentage, then do the ine-hot encodings where the new image represented with the new values (percentages) instead of 0 or 1. The model here have to predict not only the right label, but also the percentage by which the label is represented in that image.\nWe can express this idea with code as:\n\n\nchurch = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\nshow_image(tchurch, ax=axs[0]);\nshow_image(tgas, ax=axs[1]);\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(5, 3e-3)\n\n\nTraining a model with mixup techninx make it way hareder for the model to learn, because the model need to predict 2 labels for each image instead of one, plus predicting the righ percentage of each label.\nOverfitting seems less likely to be a problem when mixup is used.\nMixup tend to produce good results with 80 epochs of training or more.\nOne other benefit of using Mixup is when we have labels == 0, 1, because of using softmax and sigmoid the output can never be 0 or 1, thus our loss can never be perfect. With mixup labels cannot be 0 or 1 unless we mix 2 images with the same label, the rest of the time our labels will be linear combinations like 0.4, 0.6 ..\n\n\n\n\n\nIn classfication problems we usually have labels like 0 and 1, so the model job is to return them accuaratly, even 0.999 is not good enough where the label is 1!. this cuase the model to update the weights in order to get closer and closer to the right and unique answear, what will lead to overfitting.\nThe solution to this problem is to smooth the labels, by replacing 1 with a bit smaller number, and 0 with a bit bigger number. this will encourage the model to be less confident, which will help to better generalization.\nLabel Smoothing can be expressed mathematically:\n\n0: \\(\\frac{\\epsilon}{N}\\) where N is number of classes we have and epsilon represent a parameter usually 0.1(it’s like saying we’re 10% less confident about the label)\n1: \\(1-\\epsilon + \\frac{\\epsilon}{N}\\)\n\nIn our Imagenette example where we have 10 classes, the targets become something like (here for a target that corresponds to the index 3):\n\n[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n\nTo use this in practice, we just have to change the loss function in our call to Learner:\n\n```python model = xresnet50(n_out=dls.c) learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3)"
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#imagenette",
    "href": "posts/Fastai_ch7/Ch7.html#imagenette",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "This dataset is created by Fastai community, the goal of it is to have a dataset that can train models that generelize well on the larger version (ImageNet), which will help Machine learning practitioners to build and experiment many ideas and projects with less computation power.\nImagenette has 10 classes, which are very different from one another.\nLet’s download the dataset\n\n\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)\n\n\n\n\n\n\n    \n      \n      100.00% [1557168128/1557161267 00:38&lt;00:00]\n    \n    \n\n\n\nHere we create DataLoaders using the Resize technic we saw in CH5\n\n\ndblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64)\n\n\nNow we do the training just to have something to begin with\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.615762\n1.960073\n0.446229\n02:59\n\n\n1\n1.231709\n1.436732\n0.542943\n03:01\n\n\n2\n0.946307\n0.957584\n0.703883\n03:01\n\n\n3\n0.710163\n0.686722\n0.788648\n02:59\n\n\n4\n0.581876\n0.551411\n0.833831\n02:57\n\n\n\n\n\n\nNot bad result considering that we didn’t use a pretrained model.\nThe aim of this chapter is to increase the peformance of this base-line model by implementing different technics"
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#normalization",
    "href": "posts/Fastai_ch7/Ch7.html#normalization",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "Normalization means having a dataset with mean of 0 and standard diviation of 1.\nMost images and computer vision libraries use values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is not going to have a mean of 0 and a standard deviation of 1.\nLet’s grab a batch of data and take the average of each axis except the channels axis (indx==1)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([0.4573, 0.4509, 0.4188], device='cuda:0'),\n TensorImage([0.2730, 0.2652, 0.2803], device='cuda:0'))\n\n\n\nAs expected, the mean and std is different than what we desire.\nFastai provide the method Normalize which can be applied on the whole batch, so we could add it to batch_tfms section of the datablock, we just need to tell fastai which are the mean and the standard deviation we want to use (in this case we will use ImageNet stats) but even without giving these stats fastai will calculate them from one batch and use them.\nNotice here we build the datablock withing a function that take batch_size and size as parameters, we will see way later\n\n\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\n\ndls = get_dls(64, 224)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([-0.0920, -0.0490,  0.0548], device='cuda:0'),\n TensorImage([1.1969, 1.1888, 1.2855], device='cuda:0'))\n\n\n\nThis Normalize method help us to get close to the desired values for mean and std\nLet’s how this effect the performance of the model:\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.617465\n1.945767\n0.462285\n02:58\n\n\n1\n1.272744\n2.252786\n0.462659\n02:58\n\n\n2\n0.947452\n1.084139\n0.643017\n02:58\n\n\n3\n0.722699\n0.795408\n0.765123\n03:00\n\n\n4\n0.610676\n0.577560\n0.819642\n02:58\n\n\n\n\n\n\nImplementing the normalization didn’t help the model get better results, because this technic works better when using a pretrained model."
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#progressive-resizing",
    "href": "posts/Fastai_ch7/Ch7.html#progressive-resizing",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "The idea behind Progressive Resizing is to start training the model with small images, and end the training fase with large images.\nAs we have seen, the image size doesn’t play a role in the learning process.\nAlthough changing the image size in the middel of training will effect how the model behave in a way or another.\nThe best way is to deal with the transition from small image to larger images as if we do transfer learning. After changing the images size we should fine_tune method.\nWe can look to Progressive Resizing as a form of data augmentation, which mean the model will generalize well.\nTo implement progressive resizing it is most convenient if you first create a get_dls function which takes an image size and a batch size as we did in the section before, and returns your DataLoaders:\nNow you can create your DataLoaders with a small size and use fit_one_cycle in the usual way, training for a few less epochs than you might otherwise do:\n\n\ndls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n\nThen you can replace the DataLoaders inside the Learner, and fine-tune:\n\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n\nWe can increase the image size by a bit, untill we reach the normal size of the actual image in dataset.\nProgressive resizing can be good or bad depending many factors:\n\nFor transfer learning, if the model is already trained on similar task we will use it for, and on similar size of images, normally re-training it on smaller images will damage the performance.\nIn other hand if the model is trained on dataset with different size of images, and on dfferent task, using progressive resizing may help the model performance.\n\nThere’s no right answear for every situation, we just need to try and experiment different things."
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#test-time-augmentation",
    "href": "posts/Fastai_ch7/Ch7.html#test-time-augmentation",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "Till now all kind of data augmentations we implement are done on training set, while the validation set is taking the same images.\nThe idea behind Test Time Augmentation tta is to use some augmented images for the validation set to get predictions (for the same image) and average (or take the maximun) them.\nUsually fastai do center cropping for validation.\nThis method may cause the model to miss valuable lessons from edges of the cropped images.\nWe could avoid center cropping but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, then take the average of predictions or the maximum\nBy default, fastai will use the unaugmented center crop image plus four randomly augmented images.\nYou can pass any DataLoader to fastai’s tta method; by default, it will use your validation set:\n\n\npreds,targs = learn.tta()\naccuracy(preds, targs).item()"
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#mixup",
    "href": "posts/Fastai_ch7/Ch7.html#mixup",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "The idea of mixup is to take two data points (images) and mix them together with some percentage, then do the ine-hot encodings where the new image represented with the new values (percentages) instead of 0 or 1. The model here have to predict not only the right label, but also the percentage by which the label is represented in that image.\nWe can express this idea with code as:\n\n\nchurch = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\nshow_image(tchurch, ax=axs[0]);\nshow_image(tgas, ax=axs[1]);\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(5, 3e-3)\n\n\nTraining a model with mixup techninx make it way hareder for the model to learn, because the model need to predict 2 labels for each image instead of one, plus predicting the righ percentage of each label.\nOverfitting seems less likely to be a problem when mixup is used.\nMixup tend to produce good results with 80 epochs of training or more.\nOne other benefit of using Mixup is when we have labels == 0, 1, because of using softmax and sigmoid the output can never be 0 or 1, thus our loss can never be perfect. With mixup labels cannot be 0 or 1 unless we mix 2 images with the same label, the rest of the time our labels will be linear combinations like 0.4, 0.6 .."
  },
  {
    "objectID": "posts/Fastai_ch7/Ch7.html#label-smoothing",
    "href": "posts/Fastai_ch7/Ch7.html#label-smoothing",
    "title": "Chapter 7: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "In classfication problems we usually have labels like 0 and 1, so the model job is to return them accuaratly, even 0.999 is not good enough where the label is 1!. this cuase the model to update the weights in order to get closer and closer to the right and unique answear, what will lead to overfitting.\nThe solution to this problem is to smooth the labels, by replacing 1 with a bit smaller number, and 0 with a bit bigger number. this will encourage the model to be less confident, which will help to better generalization.\nLabel Smoothing can be expressed mathematically:\n\n0: \\(\\frac{\\epsilon}{N}\\) where N is number of classes we have and epsilon represent a parameter usually 0.1(it’s like saying we’re 10% less confident about the label)\n1: \\(1-\\epsilon + \\frac{\\epsilon}{N}\\)\n\nIn our Imagenette example where we have 10 classes, the targets become something like (here for a target that corresponds to the index 3):\n\n[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n\nTo use this in practice, we just have to change the loss function in our call to Learner:\n\n```python model = xresnet50(n_out=dls.c) learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3)"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "",
    "text": "Transformer models are very large with Ms to 10s of Billions of parameters, which make the process of training and fine-tuning and deploying them very hard.\nHere comes the Hugging Face library which adress that problem, the goal is to provide a single API through which any transformer model can be loaded, trained and saved.\nWith Transformer library we can: - Download, load and use models for inference or fine-tuning with just couple lines of code - all models in the library are stored like any other model, at their core they are just a simple pytorch nn.Module class. - All components of the models are stored in one file, so no abstarctions or shared modules across files"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#preprocessing-with-a-tokenizer",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#preprocessing-with-a-tokenizer",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Preprocessing with a Tokenizer:",
    "text": "Preprocessing with a Tokenizer:\n\nIn order to convert raw text to its numerical form before we feed it to the model, we use Tokenizer.\nHere is how we tokenize any input words:\n\n\nfrom transformers import AutoTokenizer\nmdl_ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\ninputs = 'My birthday is today!'\noutputs = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\noutputs\n\n{'input_ids': tensor([[ 101, 2026, 5798, 2003, 2651,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n\n\n\nFirst we pick a model distilbert-base-uncased-finetuned-sst-2-english which is basically the same model our pipeline used to classify the sentence.\nWe use AutoTokenizer to get to tokenization method according to that model, because each model has its own method of tokenizing words.\nThen we feed the text to the tokenizer, and we pick which type of tensors we want to get returned\n\npt stands for pytorch\nother parameters will be covered later\n\nWe get a dictionary with 2 keys: input_ids and attention_mask\nattention_mask will be covered later, input_ids contains one list of integers."
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#going-through-the-model",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#going-through-the-model",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Going through the model:",
    "text": "Going through the model:\n\nWe can download the pretraind model same we did with tokenizer, by usin AutoModel class which also has from_pretrained method.\nWe just need to download the same model as used in tokenization process.\n\n\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(mdl_ckpt)\n\n\nThis architecture we just downloaded conatins onlly the base transformer module: given some inputs, it outputs what we call Hidden_state.\nFor each model inputs we will retrieve a high-dimensional vector representing the contextual understanding of that input by the model\nThese Hidden_states can be used as it is, but usually it will be feeded as input to another part of the model called the Head.\nEach Head is a task_specific head."
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#a-high-dimensional-vector",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#a-high-dimensional-vector",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "A high-dimensional vector?",
    "text": "A high-dimensional vector?\n\nUsually the model outputs a large vector with 3 dimensions:\n\nBatch-size: the number of sequence processed (in our case we pass only one sentence)\nSequence-length: The length of the numerical representation of the sequence (8 in our example)\nHidden size: The vector dimension of each model input.\n\nThe high-dimentionality of this vector comes from the last dimension, the hidden-size is very large dimension: usually ~700:\n\n\nouts = model(**outputs)\nouts.last_hidden_state.shape\n\ntorch.Size([1, 7, 768])"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#model-heads-making-sense-out-of-numbers",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#model-heads-making-sense-out-of-numbers",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Model heads: Making sense out of numbers:",
    "text": "Model heads: Making sense out of numbers:\n\nSo to wrap-up the whole process: First get inputs converted input ID then the embedding layer convert them into tokenized vectors.\nThe subsequent layers manipulate thes vectors using attention mechanism to produce a contextual understanding of that input in form of High-dimensional-vector.\n\n\n\n\nmodel\n\n\n\nThere rae many architecture available in the Transformers library, each is designed to tackle specific task.\nFor example if we want a model for a sequence classification head, we will use AutoModelForSequenceClassification instead of AutoModel.\n\n\ntext = ['do you feel any better today?', 'I feel warm and cosy in my house']\ntokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\ninps = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n\n\nfrom transformers import AutoModelForSequenceClassification\nmdl_ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(mdl_ckpt)\nouts = model(**inps)\nouts\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.2121,  0.4987],\n        [-3.9382,  4.1996]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\nouts.logits.shape\n\ntorch.Size([2, 2])\n\n\n\nIn this case we have 2 sentences and 2 labels negative positive.\nThe model will take the high dimensional vector as input and outputs a vector that match our task.\n\n\nPost processing:\n\nThe vector we get doesn’t make any sense as it is, so we need to make it meaningful for our task.\n\n\nouts.logits\n\ntensor([[-0.2121,  0.4987],\n        [-3.9382,  4.1996]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nthose are prediction for each sentence, and each prediction can be mapped to a label, so we need to know each label which, then convert those logits into some meaningful values.\nTo convert the logits into probabilies we will pass them through a softmax layer.\n\n\nimport torch\npreds = torch.nn.functional.softmax(outs.logits, dim=-1)\npreds\n\ntensor([[3.2942e-01, 6.7058e-01],\n        [2.9218e-04, 9.9971e-01]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nNow we need to know the label of each colomn:\n\n\nmodel.config.id2label\n\n{0: 'NEGATIVE', 1: 'POSITIVE'}\n\n\n\nSo the position [0] is negative where the position [1] positive"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#saving-the-model",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#saving-the-model",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Saving the model:",
    "text": "Saving the model:\n\nTo save a model we are satisfied with its prformance:\n\n\nmdl.save_pretrained('path')\n\n\n!ls 'path'\n\nconfig.json  pytorch_model.bin  special_tokens_map.json  tokenizer_config.json  vocab.txt\n\n\n\nThis saves 2 files:\n\nconfig.json: contains all attributes necessary to build the model architecture, and also it contains some metadata\npytorch_model.bin: contains the learnable weights."
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#using-a-transformer-model-for-inference",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#using-a-transformer-model-for-inference",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Using a Transformer model for inference:",
    "text": "Using a Transformer model for inference:\n\nTokenizer convert input words into input ID:\n\n\nsequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\ninps = tokenizer(sequences)\nencoded_sequences = inps.input_ids\nencoded_sequences\n\n[[101, 7592, 999, 102], [101, 4658, 1012, 102], [101, 3835, 999, 102]]\n\n\n\nThe output we get here is a list of list, the problem is that tensors accept only rectangular shapes.\nSo we nee to cenvert it into the targeted shape:\n\n\ninput = torch.tensor(encoded_sequences)\ninput\n\ntensor([[ 101, 7592,  999,  102],\n        [ 101, 4658, 1012,  102],\n        [ 101, 3835,  999,  102]])\n\n\n\nUsing the tensors as inputs to the model\n\nMaking use of this returned tensor is easy as pass it through the model:\n\n\noutputs= mdl(input)\noutputs\n\nBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n           3.9394e-01, -9.4770e-02],\n         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n           2.2992e-01, -4.1172e-02],\n         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n           2.8224e-01,  7.5566e-02],\n         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n           1.0441e+00, -6.1970e-03]],\n\n        [[ 3.6436e-01,  3.2464e-02,  2.0258e-01,  ...,  6.0111e-02,\n           3.2451e-01, -2.0995e-02],\n         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n           1.4553e-01, -3.7545e-02],\n         [ 3.3223e-01, -2.3271e-01,  9.4877e-02,  ..., -2.5268e-01,\n           3.2172e-01,  8.1079e-04],\n         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n           1.0526e+00, -5.6255e-01]],\n\n        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n           3.3564e-01,  2.8262e-01],\n         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n           2.0175e-01,  3.3275e-01],\n         [ 2.0160e-01,  1.5783e-01,  9.8974e-03,  ..., -3.8850e-01,\n           4.1308e-01,  3.9732e-01],\n         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n           1.0925e+00, -4.8456e-02]]], grad_fn=&lt;NativeLayerNormBackward0&gt;), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n       grad_fn=&lt;TanhBackward0&gt;), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#loading-and-saving",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#loading-and-saving",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Loading and Saving:",
    "text": "Loading and Saving:\n\nLoading and saving tokenizer is simple and very similar to how we load and save Models.\nBy using the same two methods: from_pretrained and save_pretrained.\nAlso we can load the tokenizer either by calling tokenizer class BertTokenizer or by just using AutoTokenizer, same as how we load models: AutoModel or BertModel:\n\n\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n\n# or the easy way\ntokenizer1 = AutoTokenizer.from_pretrained('bert-base-cased')\n\n\ntext = 'try to tokenize this text so I can see the difference between them!'\ninp = tokenizer(text, return_tensors='pt')\ninp1 = tokenizer1(text, return_tensors='pt')\n\n\ninp.input_ids, inp1.input_ids\n\n(tensor([[  101,  2222,  1106, 22559,  3708,  1142,  3087,  1177,   146,  1169,\n           1267,  1103,  3719,  1206,  1172,   106,   102]]),\n tensor([[  101,  2222,  1106, 22559,  3708,  1142,  3087,  1177,   146,  1169,\n           1267,  1103,  3719,  1206,  1172,   106,   102]]))\n\n\n\nSo both methods yield same results, but as we said before we prefer using the second method since its code agnostinc and can be applied with all model.\nSaving tokenizer is also similar to how we save models:\n\n\ntokenizer.save_pretrained('path')\n\n('path/tokenizer_config.json',\n 'path/special_tokens_map.json',\n 'path/vocab.txt',\n 'path/added_tokens.json')"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#encoding",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#encoding",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Encoding:",
    "text": "Encoding:\n\nThe process of translating words to numbers is called encoding.\nThe encoding is done through 2 steps:\n\nTokenization\nconversion to input IDs\n\n\n * The first we create token-word, ,ostly complete words, but in some cases the one word will be splited to 2 or more parts. - this sub parts can be dentified by the ## preffix. * Then we need to convert those tokens into input IDs in order to feed them to the model. * To do that the tokenizer pass this tokens through a Vocabulary. - When we instentiate the tokenizer with from_pretrained() we already download that vocabulary the we can match ewach token against an ID. - we need to use the same checkpoint during the training. * Here we will explore these 2 steps seperatly:\n\ntext  = 'Using a transformer network is simple'\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ntokens = tokenizer.tokenize(text)\ntokens\n\n['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n\n\n\nAs we see the word transformer get splited to 2 parts, the second one is represented with ## prefix\n\n\nids = tokenizer.convert_tokens_to_ids(tokens)\nids\n\n[7993, 170, 11303, 1200, 2443, 1110, 3014]"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#decoding",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#decoding",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Decoding:",
    "text": "Decoding:\n\nThe idea of decoding is the exact opsite of encoding, its the process of converting ids to their text/word form\n\n\ndecoded_ids = tokenizer.decode(ids)\ndecoded_ids\n\n'Using a transformer network is simple'\n\n\n\nWe get the text we begin with back by using the decode() method."
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#padding",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#padding",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Padding:",
    "text": "Padding:\n\nSuppose we have this list of list as ids:\n\n\nbatched_ids = [[200, 200], [200, 200, 100]]\n\n\nWe need to get this list of list in rectangular shape before we convert it into tensor.\nThis is where we use padding\n\n\npadding_id = 100\nbatched_ids = [[200, 200, padding_id], [200, 200, 100]]\n\n\nThe padding token ID can be found in tokenizer.pad_token_id.\nNow let’s do a simple experience to see differences between 3 batches after going through a model: - first list - second list - both lists batched and padding applied\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(mdl_chkpt)\nsequence1_ids = [[200, 200]]\nsequence2_ids = [[200, 200, 200]]\nbatched_ids = [[200, 200,tokenizer.pad_token_id ], [200, 200, 200]]\n\n\nprint(model(torch.tensor(sequence1_ids)).logits)\nprint(model(torch.tensor(sequence2_ids)).logits)\nprint(model(torch.tensor(batched_ids)).logits)\n\ntensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)\ntensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)\ntensor([[ 1.3374, -1.2163],\n        [ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nThe thing we could observe here is that we get same logits for the second sequence ID compared with batched_ids but not the first one?\nThe first sequence id is where we applied padding, and as we know transformer models are very sensetive to any context of the words (in this case context of the ids) so the element that we added in order to get a rectangular shape, is also get computed by the transformer model, which influence the final prediction.\nWe need to tell the model to ignore these padding values during the computation\n\n\nAttention mask:\n\nAttention mask is what tell the model during the predecting phase to ignore padding values and not including them while computing the attention mechanism\n\n\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\n\nattention_mask = [\n    [1, 1, 1],\n    [1, 1, 0],\n]\n\noutputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\nprint(outputs.logits)\n\ntensor([[ 1.5694, -1.3895],\n        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nNow we get the same logits"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#longer-sequences",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#longer-sequences",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Longer sequences:",
    "text": "Longer sequences:\n\nTransformer model cannot handle very long sentences, usually they have between 512 and 1024 tokens as maximum length for a sentence.\nI we have a situation where we need to deal with very large sequence we either:\n\nuse models that can handle long sentences\nuse truncation method\n\nTruncation is a way of making sequences of the same batch the same length, either by picking the length of the longest sequence or the short one"
  },
  {
    "objectID": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#special-tokens",
    "href": "posts/HuggingFace_2/HuggingFace_NLP_course_Notes_2.html#special-tokens",
    "title": "Hugging Face Course Notes: Chapter2",
    "section": "Special Tokens",
    "text": "Special Tokens\n\nIf we look closely to the input ID’s we get, we can spot a small difference from what we got earlier\nThe tokenizer added 2 ID’s to the list, one in the begining and another at the end.\n\nthey alwayas have the same value: 101 and 102\n\n\n\nseq = \"I've been waiting for a HuggingFace course my whole life.\"\ntoks1 = tokenizer(seq)\ntoks2 = tokenizer.tokenize(seq)\nids1= toks1.input_ids\nids2 = tokenizer.convert_tokens_to_ids(toks2)\nprint(f'normal: {ids1}')\nprint(f'hard_coded: {ids2}')\n\nnormal: [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\nhard_coded: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n\n\n\nNow we will decode the different types of ID’s we will get different decoded sentences:\n\n\nprint(f'normal: {tokenizer.decode(ids1)}')\nprint(f'hard-coded: {tokenizer.decode(ids2)}')\n\nnormal: [CLS] i've been waiting for a huggingface course my whole life. [SEP]\nhard-coded: i've been waiting for a huggingface course my whole life.\n\n\n\nTokenizer added special tokens to the sentence [CLS] and [SEP], because the model was trained with this kind of architecture"
  },
  {
    "objectID": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html",
    "href": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html",
    "title": "Hugging Face Course Notes: Chapter5",
    "section": "",
    "text": "!pip install datasets\n\nCollecting datasets\n  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.7/536.7 kB 8.0 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\nRequirement already satisfied: pyarrow&gt;=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 14.0 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 16.7 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;=2023.10.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\nRequirement already satisfied: huggingface-hub&gt;=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.19.4-&gt;datasets) (4.9.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;datasets) (1.16.0)\nInstalling collected packages: dill, multiprocess, datasets\nSuccessfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16\n!pip install transformers\n\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors&gt;=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.19.3-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.19.3-&gt;transformers) (4.9.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2024.2.2)\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer"
  },
  {
    "objectID": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#intrudoction",
    "href": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#intrudoction",
    "title": "Hugging Face Course Notes: Chapter5",
    "section": "Intrudoction:",
    "text": "Intrudoction:\n\nIn chapter 3 we learn how to use the library datsets by loading datest from the hub and building a compute_metrics function, and using Dataset.map() function, however this functionalities doesn’t represent the whole picture about dataset library.\nIn this chapetr we will go deeper and try understand :\n\nHow to load a dataset when it’s not available on the Hub\nHow to slice and dice a dataset\nWhat to do when the datset is large\nWhat is “memory mapping” and Apache row?\nHow to create our own dataset and push it to the hub?"
  },
  {
    "objectID": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#what-if-my-dataset-isnt-on-the-hub",
    "href": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#what-if-my-dataset-isnt-on-the-hub",
    "title": "Hugging Face Course Notes: Chapter5",
    "section": "What if my dataset isn’t on the Hub?",
    "text": "What if my dataset isn’t on the Hub?\n\nMost of the time we will deal with situation when the dataset we want to work with isn’t on the HUB, In this section we’ll show you how huggingface Datasets can be used to load datasets that aren’t available on the Hugging Face Hub.\n\n\nWorking with local and remote datasets:\n\nHugging face Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n\nload_dataset(\"csv\", data_files=\"my_file.csv\")\nload_dataset(\"text\", data_files=\"my_file.txt\")\nload_dataset(\"json\", data_files=\"my_file.jsonl\")\nload_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")\n\n\n\n* As shown , for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files.\n* First we will deal with loading a dataset from local files; later we’ll see how to do the same with remote files.\n\n\n\n### Loading a local dataset\n\n* Here we'll use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale dataset for question answering in Italian.\n\n    * The training and test splits are hosted on GitHub, so we can download them with a simple wget command:\n\n\n::: {#cell-9 .cell outputId='6433a784-3bd2-4b27-f0fc-3135e17e61bb'}\n``` {.python .cell-code}\n!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n\n--2024-02-27 13:44:06--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\nResolving github.com (github.com)... 140.82.114.4\nConnecting to github.com (github.com)|140.82.114.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n--2024-02-27 13:44:06--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7725286 (7.4M) [application/octet-stream]\nSaving to: ‘SQuAD_it-train.json.gz’\n\nSQuAD_it-train.json 100%[===================&gt;]   7.37M  --.-KB/s    in 0.1s    \n\n2024-02-27 13:44:06 (68.4 MB/s) - ‘SQuAD_it-train.json.gz’ saved [7725286/7725286]\n\n--2024-02-27 13:44:06--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n--2024-02-27 13:44:07--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1051245 (1.0M) [application/octet-stream]\nSaving to: ‘SQuAD_it-test.json.gz’\n\nSQuAD_it-test.json. 100%[===================&gt;]   1.00M  --.-KB/s    in 0.07s   \n\n2024-02-27 13:44:07 (15.1 MB/s) - ‘SQuAD_it-test.json.gz’ saved [1051245/1051245]\n\n\n:::\n\nWe need decompress them frist:\n\n\n!gzip -dkv SQuAD_it-*.json.gz\n\nSQuAD_it-test.json.gz:   87.5% -- created SQuAD_it-test.json\nSQuAD_it-train.json.gz:  82.3% -- created SQuAD_it-train.json\n\n\n\n!rm -rf SQuAD_it-*.json.gz\n\n\nNow we can download our dataset from local file as if it from the hub:\n\n\ndataset = load_dataset('json', data_files= \"SQuAD_it-train.json\", field= \"data\" )\n\n\n\n\n\nBe default downloading local file creates a DatasetDict with train split:\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n\n\n\nWe can view that we have 442 rows, let’s see one of them:\n\n\ndataset['train'][0]\n\n\nGreat, we’ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once.\nTo do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:\n\n\n# the proper way to load dataset from local file:\ndata_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\nsquad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\nsquad_it_dataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 48\n    })\n})\n\n\n\nThis is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\nThe loading scripts in Datasets acually support the automatic decompression of the input files, we could have skipped the use of gzip by pointing the data_files argument directly to the compressed files:\n\n\ndata_files = {'train': 'SQuAD_it-train.json.gz', 'test': 'SQuAD_it-test.json.gz'}\ndataset = load_dataset('json', data_files=data_files, field= 'data')\ndataset\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 48\n    })\n})\n\n\n\nThe automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point data_files to the compressed files and you’re good to go!\n\n\n\nLoading a remote dataset:\n\nLoading remote files is very similar to loading locally, we just need to point to data_file the url where the data is stored instead to providing the path to lacal files.\nFor example the SQuAD_it dataset is stored on github so we could build our dataset from that url directly:\n\n\ndata_files = {'train':'https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz',\n              'test': 'https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz'}\ndataset = load_dataset('json', data_files=data_files, field='data')"
  },
  {
    "objectID": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#time-to-slice-and-dice",
    "href": "posts/HuggingFace_5/Hugging_Face_Cource_Ch5.html#time-to-slice-and-dice",
    "title": "Hugging Face Course Notes: Chapter5",
    "section": "Time to slice and dice:",
    "text": "Time to slice and dice:\n\nIn this section we will explore various features Datasets provide in order to clean and prepare the dataset for the next steps.\n\n\nSlicing and dicing our data:\n\nLike Pandas Datasets provides several functions to manipulate the content of DatasetDict Data object, we already use the .map() method.\nHere we will use Drug Review Dataset from UC Irvine Machine Learning Repository which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction.\nFirst we need to download it and unzip it:\n\n\n!wget https://archive.ics.uci.edu/static/public/462/drug+review+dataset+drugs+com.zip\n\n--2024-02-27 14:22:46--  https://archive.ics.uci.edu/static/public/462/drug+review+dataset+drugs+com.zip\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified\nSaving to: ‘drug+review+dataset+drugs+com.zip’\n\ndrug+review+dataset     [       &lt;=&gt;          ]  41.00M  30.7MB/s    in 1.3s    \n\n2024-02-27 14:22:48 (30.7 MB/s) - ‘drug+review+dataset+drugs+com.zip’ saved [42989872]\n\n\n\n\n!unzip drug+review+dataset+drugs+com.zip\n\nArchive:  drug+review+dataset+drugs+com.zip\n  inflating: drugsComTest_raw.tsv    \n  inflating: drugsComTrain_raw.tsv   \n\n\n\nWe will use csv arguments here even though we have tsv files, we just need to specifying the delimiter argument in the load_dataset() function as follows:\n\n\ndata_files = {'train': 'drugsComTrain_raw.tsv',\n              'test': 'drugsComTest_raw.tsv'}\ndataset = load_dataset('csv', data_files=data_files, delimiter= '\\t')\n\n\n\n\n\n\n\n\nA good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you’re working with. In Datasets, we can create a random sample by chaining the Dataset.shuffle() and Dataset.select() functions together:\n\n\ndrug_sample = dataset['train'].shuffle(seed= 42).select(range(1000))\ndrug_sample[:2]\n\n{'Unnamed: 0': [87571, 178045],\n 'drugName': ['Naproxen', 'Duloxetine'],\n 'condition': ['Gout, Acute', 'ibromyalgia'],\n 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"'],\n 'rating': [9.0, 3.0],\n 'date': ['September 2, 2015', 'November 7, 2011'],\n 'usefulCount': [36, 13]}\n\n\n\nHere we fixed the seed for reproducibility.\n.select() method works with iterator so we provide a range()\nThen we slice that sample the python way.\nFrom this sample we can already see a few quirks in our dataset:\n\nThe Unnamed: 0 column looks suspiciously like an anonymized ID for each patient.\nThe condition column includes a mix of uppercase and lowercase labels.\nThe reviews are of varying length and contain a mix of Python line separators () as well as HTML character codes like &#039;.\n\nWe will adress each issue with Dataset library:\nFirst the unnamed column may be just Id fro each patient, so we check if each one of those IDs is unique or not:\n\n\nfor split in dataset.keys():\n    assert len(dataset[split]) == len(dataset[split].unique(\"Unnamed: 0\"))\n\n\nIt’s better to rename that column to something more meaningfull:\n\n\ndataset = dataset.rename_column('Unnamed: 0', 'patient_id')\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 161297\n    })\n    test: Dataset({\n        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 53766\n    })\n})\n\n\n\nNow we have to lower case all the condition values. This can be easily achieved by .map() method:\n\n\ndef lower_case(example):\n  return {'condition': example['condition'].lower()}\n\n\ndataset.map(lower_case)\n\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-27-b34dfc422493&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 dataset.map(lower_case)\n\n/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\n    866             cache_file_names = {k: None for k in self}\n    867         return DatasetDict(\n--&gt; 868             {\n    869                 k: dataset.map(\n    870                     function=function,\n\n/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py in &lt;dictcomp&gt;(.0)\n    867         return DatasetDict(\n    868             {\n--&gt; 869                 k: dataset.map(\n    870                     function=function,\n    871                     with_indices=with_indices,\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\n    591             self: \"Dataset\" = kwargs.pop(\"self\")\n    592         # apply actual function\n--&gt; 593         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    594         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    595         for dataset in datasets:\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\n    556         }\n    557         # apply actual function\n--&gt; 558         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    559         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    560         # re-apply format to the output\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\n   3103                     desc=desc or \"Map\",\n   3104                 ) as pbar:\n-&gt; 3105                     for rank, done, content in Dataset._map_single(**dataset_kwargs):\n   3106                         if done:\n   3107                             shards_done += 1\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in _map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\n   3456                     _time = time.time()\n   3457                     for i, example in shard_iterable:\n-&gt; 3458                         example = apply_function_on_filtered_inputs(example, i, offset=offset)\n   3459                         if update_data:\n   3460                             if i == 0:\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)\n   3359             if with_rank:\n   3360                 additional_args += (rank,)\n-&gt; 3361             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n   3362             if isinstance(processed_inputs, LazyDict):\n   3363                 processed_inputs = {\n\n&lt;ipython-input-26-98b6e79f37ab&gt; in lower_case(example)\n      1 def lower_case(example):\n----&gt; 2   return {'condition': example['condition'].lower()}\n\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\n\n\nIts seems like some of condition values are NoneType, which the map function cannot support.\nLet’s drop these rows using Dataset.filter(), which works in a similar way to Dataset.map() and expects a function that receives a single example of the dataset.\nWe will use the lambda function:\n\n\ndataset = dataset.filter(lambda x: x['condition'] is not None)\n\n\n\n\n\n\n\n\nNow lets aplly .map() method on lower_case() funtion:\n\n\ndataset = dataset.map(lower_case)\n\n\n\n\n\n\n\n\n\nCreating new columns:\n\nWhen dealing with reviews datasets, its good practice to count the number of words in each review, so lets create a function that achieve that goal:\n\n\ndef compute_review_length(example):\n  return {'review_length': len(example['review'].split())}\n\n\ncompute_review_length() returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when compute_review_length() is passed to Dataset.map(), it will be applied to all the rows in the dataset to create a new review_length column:\n\n\ndataset = dataset.map(compute_review_length)\n\n\n\n\n\n\n\n\ndataset['train'][2]\n\n{'patient_id': 92703,\n 'drugName': 'Lybrel',\n 'condition': 'birth control',\n 'review': '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n 'rating': 5.0,\n 'date': 'December 14, 2009',\n 'usefulCount': 17,\n 'review_length': 134}\n\n\n\nAs expected, we can see a review_length column has been added to our training set. We can sort this new column with Dataset.sort() to see what the extreme values look like:\n\n\ndataset['train'].sort('review_length')[:3]\n\n{'patient_id': [111469, 13653, 53602],\n 'drugName': ['Ledipasvir / sofosbuvir',\n  'Amphetamine / dextroamphetamine',\n  'Alesse'],\n 'condition': ['hepatitis c', 'adhd', 'birth control'],\n 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n 'rating': [10.0, 10.0, 10.0],\n 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n 'usefulCount': [41, 3, 0],\n 'review_length': [1, 1, 1]}\n\n\n\nWe could delete rows that countains review with less than 30 words, by using the combination of lambda function + .filter():\n\n\ndataset = dataset.filter(lambda x: x['review_length'] &gt; 30)\n\n\n\n\n\n\n\n\nThe last problemm we have to deal with is html characters, We can use Python’s html module to unescape these characters, like so:\n\n\nimport html\n\n\nWe’ll use Dataset.map() to unescape all the HTML characters in our corpus:\n\n\ndataset = dataset.map(lambda x: {'review': html.unescape(x['review'])})\n\n\n\n\n\n\n\n\n\nThe map() method’s superpowers:\n\nThe last mapping we apply on dataset takes almost 40s to execute, this duration can be reduced if we pass an argument to the .map() method: batched=True, this will create batches and lets the code applied in many elements at once, we could also construct the code in list comprehension instead of regular for loop, which also add preformance the operation:\n\n\nnew_dataset = dataset.map(lambda x: {'review': [html.unescape(o) for o in x['review']]}, batched= True)\n\n\n\n\n\n\n\n\nThis time our map() function execute in 1s, 40x time faster!\nUsing .map() with batched set to true is very powerful tool that will help us later with tokenizer()\nNow lets tokenize our dataset since we are done EDA part:\n\n\nmdl = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(mdl)\ndef tokenizer_function(example):\n  return tokenizer(example['review'], truncation= True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokenized_ds = dataset.map(tokenizer_function, batched=True)\n\n\n\n\n\n\n\n\nAll of this functionality condensed into a single method is already pretty amazing, but there’s more! With Dataset.map() and batched=True you can change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example, and we will need to do this as part of the preprocessing for several of the NLP tasks we’ll undertake in Chapter 7.\nIn machine learning, an example is usually defined as the set of features that we feed to the model. In some contexts, these features will be the set of columns in a Dataset, but in others (like here and for question answering), multiple features can be extracted from a single example and belong to a single column.\nLet’s have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return all the chunks of the texts instead of just the first one. This can be done with return_overflowing_tokens=True:\n\n\ndef tokenize_split(example):\n  return tokenizer(\n      example['review'],\n      truncation= True,\n      max_length= 128,\n      return_overflowing_tokens= True\n  )\n\n\nLet’s test this before we pass it to the map() function:\n\n\nres = tokenize_split(dataset['train'][0])\n[len(inp) for inp in res['input_ids']]\n\n[128, 45]\n\n\n\nSo, our first example in the training set became two features because it was tokenized to more than the maximum number of tokens we specified: the first one of length 128 and the second one of length 45. Now let’s do this for all elements of the dataset!\n\n\ntokenized_ds = dataset.map(tokenize_split, batched= True)\n\n\n\n\n\n---------------------------------------------------------------------------\nArrowInvalid                              Traceback (most recent call last)\n&lt;ipython-input-47-4c142a3791ef&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 tokenized_ds = dataset.map(tokenize_split, batched= True)\n\n/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\n    866             cache_file_names = {k: None for k in self}\n    867         return DatasetDict(\n--&gt; 868             {\n    869                 k: dataset.map(\n    870                     function=function,\n\n/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py in &lt;dictcomp&gt;(.0)\n    867         return DatasetDict(\n    868             {\n--&gt; 869                 k: dataset.map(\n    870                     function=function,\n    871                     with_indices=with_indices,\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\n    591             self: \"Dataset\" = kwargs.pop(\"self\")\n    592         # apply actual function\n--&gt; 593         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    594         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    595         for dataset in datasets:\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\n    556         }\n    557         # apply actual function\n--&gt; 558         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    559         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    560         # re-apply format to the output\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\n   3103                     desc=desc or \"Map\",\n   3104                 ) as pbar:\n-&gt; 3105                     for rank, done, content in Dataset._map_single(**dataset_kwargs):\n   3106                         if done:\n   3107                             shards_done += 1\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py in _map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\n   3499                                 writer.write_table(pa.Table.from_pandas(batch))\n   3500                             else:\n-&gt; 3501                                 writer.write_batch(batch)\n   3502                         num_examples_progress_update += num_examples_in_batch\n   3503                         if time.time() &gt; _time + config.PBAR_REFRESH_TIME_INTERVAL:\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\n    567                 inferred_features[col] = typed_sequence.get_inferred_type()\n    568         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\n--&gt; 569         pa_table = pa.Table.from_arrays(arrays, schema=schema)\n    570         self.write_table(pa_table, writer_batch_size)\n    571 \n\n/usr/local/lib/python3.10/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_arrays()\n\n/usr/local/lib/python3.10/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.validate()\n\n/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\n\nArrowInvalid: Column 8 named input_ids expected length 1000 but got length 1447\n\n\n\n\nLooking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. here those 1,000 examples gave 1,463 new features, resulting in a shape error.\nThe problem is that we’re trying to mix two different datasets of different sizes: the drug_dataset columns will have a certain number of examples (the 1,000 in our error), but the tokenized_dataset we are building will have more (the 1,463 in the error message; it is more than 1,000 because we are tokenizing long reviews into more than one example by using return_overflowing_tokens=True). That doesn’t work for a Dataset, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset. We can do the former with the remove_columns argument:\n\n\ntokenized_ds = dataset.map(tokenize_split, batched= True, remove_columns= dataset['train'].column_names)\n\n\n\n\n\n\n\n\nlen(tokenized_ds['train'])\n\n204198\n\n\n\nlen(dataset['train'])\n\n138514"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "Since we now familiar the whole process of creating deep learning model, using pre-built model, building them from scratch, handling data, and putting these model into web apps, we will now to go deeper and keep focus on details that make model accurate and reliable.\nIt takes many tweaks and parameters changing in order to “polish” a model.\nIn order to achieve this goal we need to be familiar with many concepts and technics, different types of layers, regularization methods, optimizers, how to put layers together into architectures, labeling techniques, and much more.\n\n\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 30.4 MB/s \n     |████████████████████████████████| 1.3 MB 60.3 MB/s \n     |████████████████████████████████| 5.3 MB 53.0 MB/s \n     |████████████████████████████████| 441 kB 66.2 MB/s \n     |████████████████████████████████| 1.6 MB 54.6 MB/s \n     |████████████████████████████████| 115 kB 75.5 MB/s \n     |████████████████████████████████| 163 kB 67.3 MB/s \n     |████████████████████████████████| 212 kB 71.5 MB/s \n     |████████████████████████████████| 127 kB 76.1 MB/s \n     |████████████████████████████████| 115 kB 62.1 MB/s \n     |████████████████████████████████| 7.6 MB 58.8 MB/s \nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n\n\nIn real world scenarios, the first thing we do is we get in contact with data, usualy at this phase we know nothing about the dataset. We then start to look how to extract the data we want from it, and what the data looks like, and how it is structured.\nUsually data is provided in one of two ways:\n\nIndividual files representing items of data, possibly organized into folder or with filenames representing information about those items\n\ntext documents\n\nimages\n\n\nA table of data in which each row is an item and may include filenames providing connections between the data in the table and data in other formats\n\nCSV files\n\n\n\nExceptions:\n\nDomains like Genomics\n\nbinary database formats\n\nnetwork streams\n\n\n\n# download the dataset\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:02&lt;00:00]\n    \n    \n\n\n\n#get the path as variable, and see what inside\nPath.BASE_PATH = path\npath.ls()\n\n(#2) [Path('images'),Path('annotations')]\n\n\n\nAs we notice here, the data is provided with 2 directories:\n\nimages\nannotations\n\n\n\n#take a look at what inside the images directory\n(path/'images').ls()\n\n(#7393) [Path('images/german_shorthaired_87.jpg'),Path('images/Russian_Blue_200.jpg'),Path('images/Siamese_25.jpg'),Path('images/japanese_chin_48.jpg'),Path('images/miniature_pinscher_161.jpg'),Path('images/pug_150.jpg'),Path('images/pug_120.jpg'),Path('images/pug_63.jpg'),Path('images/samoyed_143.jpg'),Path('images/yorkshire_terrier_190.jpg')...]\n\n\n\nWhen we took a look at these names, we see some paterns: we already know from chapter1 that cats name are uppercase, here we see that after the breed’s name there is a (_) then a number, and finally the extension.\nThis may help us to write some code that extract the breed from a single Path.\n\n\n#pick one \nfname = (path/\"images\").ls()[0]\nfname\n\nPath('images/german_shorthaired_87.jpg')\n\n\n\nThe best way to work with strings and extract patterns from them is to use Regex, which stands for Regular Expression.\n\n\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n\n['german_shorthaired']\n\n\n\nNow we need to label the whole dataset using this code.\nFastai comes with many classes for labeling, in this case when we need to label with help of regex we could use RegexLabeller class within DataBlaock API.\n\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n\n\n\n\n\nFastai has this method of presizing the images in a way that conserve its quality after the data augmentation, so it helps the model to learn more lessons from data, and also it helps our dataset to be more varies\nThe idea behind the presizing is we crop the image and resize it to 460 by 460 first, which is a big size by deep learning norms, this operation is done on CPU, then we do the data augmentation in batches, by cropping a rotated random part of that 460^2 image, and taking the cropped image then resize again to a 224 by 224 image, all this operation are done on batch level, which mean on GPU.**\n\n\nitem_tfms=Resize(460),  \nbatch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n![REsize_method](1.png)\n\n* Usually all the augmentation operations we do on a image reduce the quality of the image, but with this approach we could say we can preserve big part of informations of that image so the model can learn better \n\n### Checking and Debugging a DataBlock\n\n* `DataBlock` is just a blueprint for orginizing data before we feed it to the model, you have no guarantee that your template is going to work on your data source as you intend.\n* So, before training a model you should always check your data. You can do this using the show_batch method:\n\n::: {#cell-23 .cell outputId='0f1711e2-f2c6-4703-ed64-1ea456f04a2d' execution_count=10}\n``` {.python .cell-code}\ndls.show_batch(nrows=1, ncols=4 ,unique= True)\n\n\n\n\n\n\n\n:::\n\nIn case we made a mistake in the process of creating datablock, we could use .summary to track the problem.Here we didn’t resize the images in one formm, so couldn’t use the batch transform\nAs we see here the .summary gives us precise diagnostic of the problem:**\n\nat least two tensors in the batch are not the same size.\n\n\n\n\n\nOnce we feel like the datablock is well created, we better begin train the model, and use it as a tool of cleaning the data. If there’s a problem with data or the model, we better know that before we lost lot of time and energy on data cleaning even before testing the model.\n\n\n# train the model\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.526362\n0.348364\n0.111637\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.482150\n0.309586\n0.093369\n01:16\n\n\n1\n0.327810\n0.229392\n0.064953\n01:17\n\n\n\n\n\n\n\n\n\nAs we saw before in Chapter 2 the best tool to clean the data is basically the model itself\nAfter creating the datablock and dataloader we better train the model and get some feedback so we know if something is wrong very early, and if not we start to use the model as tool to investigate the data\nUsually before we train the model we have to decide the function that will update the parameters, a Loss Function. But here we didn’t create any loss function?\nIf we didn’t decide the way by which we update the paramters, Fastai by default will chose a loss function for us.\n\nthe chosen loss function will suite the kind of model we build, and the type of dataset we have.\n\n\n\n# check the loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\nThe loss function used to train this model is Cross Entropy Loss\n\n\n\n\n\n\nThe Cross-Entropy Loss is a function similar to what we saw in the previous chapter, when we created the mnist_loss function:\n\ndef mnist_loss(predictions, targets):\n    prediction = prediction.sigmoid()\n    return torch.where(targets==1, 1-prediction, predictions).mean()\n\nThe problem with this function is, it only takes 2 categories(3, 7) but here we have 37 types of breeds.but here we have multiple classes.\n\nit can takes more than 2 categories\n\n\n\n\n\nIn order to understand the cross-entropy loss, let’s grab a batch of data\n\n\nx,y = dls.one_batch()\n\n\nIt return the activations of dependent and independent variable of one mini-batch\n\n\n# independent variable\ny, len(y)\n\n(TensorCategory([ 8, 34,  6, 33, 12, 15, 32, 14,  5, 27, 20, 32, 18, 24,  8, 29,  5,  4,  0, 28, 10, 12, 16, 25, 29,  3, 34, 27, 30, 15,  6, 15, 27, 34, 14, 21,  5, 17, 31, 26, 13, 35, 17, 35, 23, 14,\n                 35, 35,  8,  7, 21,  0, 22, 17, 19, 26, 16,  5, 15, 27, 11, 22, 34, 18], device='cuda:0'),\n 64)\n\n\n\nIt return 64 number, each represent on of the 37 breeds index\n\n\n# all indepent variables\ndls.vocab, len(dls.vocab)\n\n(['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier'],\n 37)\n\n\n\nHere we use get_preds to get the predictions for each image in the dataset or mini-batch (like in this example).\n\n\npreds,_ = learn.get_preds(dl=[(x,y)])\n\n\n\n\n\n\n\n\n\n# prediction for image [0] in the mini-batch\npreds[0]\n\nTensorBase([1.0919e-05, 6.0375e-08, 8.5680e-05, 7.1281e-07, 1.8754e-06, 4.7956e-07, 1.1708e-04, 1.4080e-06, 9.9972e-01, 9.4731e-07, 6.8318e-08, 1.7055e-07, 1.9338e-06, 7.7372e-07, 1.7931e-07,\n            1.5261e-07, 1.1363e-08, 2.3353e-07, 1.3689e-07, 4.5361e-07, 7.1548e-08, 4.9425e-06, 1.0026e-05, 2.6908e-07, 3.5724e-07, 3.1201e-07, 1.0177e-08, 2.7219e-08, 1.6157e-06, 3.5337e-08,\n            1.8094e-06, 3.3425e-05, 2.3514e-08, 3.0318e-06, 1.4910e-07, 1.9582e-07, 9.5147e-08])\n\n\n\nThe 37 predictions refer to the probability of each breed to match the image[0].\n\nif we sum() them up they add up to 1:\n\n\n\npreds[0].sum()\n\nTensorBase(1.0000)\n\n\n\nIn order to transform the activations of our model into prediction, we use Soft-Max\n\n\n\n\n\nAs we said before, Softmax is similar to sigmoid function we use before, but it only can handel more than 2 classes.\n\n\n# reminder of sigmoid\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n\n\n\nThis function allow us to predict whether a activation number is pointing to each category of the two, by calculating which activation is big and by much. But in our case today we have 37 category, which means by this logic we need a activation for each one.\nFirst let’s create a similar situation where we have only 2 categories, but we won’t solve it as it’s a binary problem (it's 3) but as 2 categories problem, each has it’s activation, and their probabilty sum up to 1.\n\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\n\nWe can’t just take the sigmoid of this directly, since we don’t get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):\n\n\nsigmoid(acts)\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\n\nEevn though we try different approach to solve the same problem, we still have some similarities.\nWe will use sigmoid on each activation.\nAnd we still need to substract an activation from another beacuse that represent how much the model sure about an image is assigned to each category, thats for first column.\nIn the second colun we just use 1 - prediction (activation of the second column)\n\n\ndiffs = acts[:, 0] - acts[:, 1]\n\n\n# create the sigmoid function of both categories\nsigm_ver = torch.stack([diffs.sigmoid(), 1-diffs.sigmoid()], dim=1)\n\n\nWe can express the softmax function as:\n\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\n\nLet’s check that softmax returns the same values as sigmoid for the first column, and those values subtracted from 1 for the second column:\n\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts, sigm_ver\n\n(tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]), tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]))\n\n\n\nSoftmax calculate the \\(exp^{x}\\) and divide it by sum \\(exp^{x}\\) of all activations of other categories.\n\nthe exp make sure the biggest activation is way bigger than others\ndividing by the sum is what make softmax values add up to 1\n\n\n\n\n\n\nIn the previous chapter when we created mnis_loss, we used torch.where to select between the input and 1-input.\nWith softmax, we will use indexing.\n\n\n# the pretended targets\ntargs = tensor([0, 1, 0, 1, 1, 0])\n\n\n# create an index\nidx = range(6)\n\n\n# the softmax activations\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nHere we make the targs decide which activation we pick in each row\n\n\nsm_acts[idx, targs]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\n\nlet’s display what we just did :\n\n\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targs'] = targs\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targs]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n\n\n\n\n3\n7\ntargs\nidx\nresult\n\n\n\n\n0.602469\n0.397531\n0\n0\n0.602469\n\n\n0.502065\n0.497935\n1\n1\n0.497935\n\n\n0.133188\n0.866811\n0\n2\n0.133188\n\n\n0.996640\n0.003360\n1\n3\n0.003360\n\n\n0.595949\n0.404051\n1\n4\n0.404051\n\n\n0.366118\n0.633882\n0\n5\n0.366118\n\n\n\n\n\n\nBut idea here is not to use it in a simple binary problem, because torch.where could did the same job here, but is to use it in order to solve a multi-categorie problem\n\nPyTorch provides a function that does exactly the same thing as sm_acts[range(n), targ] (except it takes the negative, because when applying the log afterward, we will have negative numbers), called nll_loss (NLL stands for negative log likelihood):\n\n-sm_acts[idx, targs]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targs, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\n\n\n\n\n\ncross-entropy-loss-function.png\n\n\n\nThe using of logarithms allow us to do all kind of multiplications without carring about the size of the output.\n\nthe nature of log functions make them increase lineary when the underlying signal increase exponentialy.\nlog(a*b) = log(a)+log(b)\nthe log of a number approaches negative infinity when the number approaches zero\n\nIn our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is “good” (closer to 1) and a large value when the prediction is “bad” (closer to 0).\nNotice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class. One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training.\n\nCalculating the loss pay attention only to the high softmax value.\n\n\n\n\n\nAfter taking the log of the softmax, we can then call the negative log likelihood.\n\nfirst : log_softmax\nthen : nll_loss\nor : nn.CrossEntropyLoss()\n\n\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targs)\n\ntensor(1.8045)\n\n\n\nnn.CrossEntropyLoss()(acts, targs)\n\ntensor(1.8045)\n\n\n\nThe nn.CrossEntrpyLoss() make do all the steps for us, but if we want to go through all those steps one by one softmas+log then negative log we could do it also:\n\n\nF.nll_loss(nn.Softmax()(acts).log(), targs,)\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\ntensor(1.8045)\n\n\n\nAdding the reduction='none' to this functions will return the loss of each row, if we didn’t add this aparameter the fuction will return the mean loss of all rows.\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\n\n\n\n\nAs we saw in chapter 3 it’s hard for us to interpret the loss function, since it’s some the computers use in order to updates the parameters and optimize the performance.\nBut we can use some kind of demonstration that shows where the model did good, and where did bad.\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIts was easy to understand what happened when they were only 3 classes in bears model, but here we have 37 breeds.\n\nthats why we will useinterp.most_confused(min_val=5) to output to most bad decisions the model taked\n\n\n\ninterp.most_confused(min_val=5)\n\n\n\n\n\n\n\n\n[('Ragdoll', 'Birman', 6)]\n\n\n\nThe best way to understand what happend is to google the names of each breed and see why the model confused it with the other breed, so we know that the model is in the right track\n\n\n\n\n\nAt this point all we can do is improve the model by correcting some detaills that may optimize the final prefromance\n\n\n\n\nOne way of improving our model is by picking the right learning rate.\n\nit will help to get faster result per epoch\nminimize the loss and updating parameters with less steps\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.615709\n5.479418\n0.529093\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n3.068122\n1.406392\n0.443843\n01:15\n\n\n\n\n\n\nHere we pick a learning rate 0.1 which is 5 times bigger than the last one 0.002 and we get bad results error rate at: 0.5\n\nbig learning rate may reduce the computation needed for the training process but the model performance will be bad\n\nAlso if we pick a small learning rate it will take forever to achieve something.\nThe answear for this dilemma is The Learning Rate Finder\n\nFastai library adopte this method created by the resaercher Leslie Smith in a paper in 2015.\n\nthe idea of Smith is to start with a small learning rate (very small), and use it for one mini-batch, see how much the loss changed, and then start increasing the learning rate by some percentage (doubling it since its very small anyway)\nrepeate this process again(track the loss, double the learning rate ..) until the loss get worse.\nat this point we just pick a learning rate smaller than the one that causes the loss to get worse.\n\nFastai course advice is either:\n\none order of magnitude less than where the minimun loss was achieved(divide by 10)\nthe last point where the loss was clearly decreasing\n\nBoth point are giving the same value usually.\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n\nMinimum/10: 1.00e-02, steepest point: 4.37e-03\n\n\n\nThe plot shows that the loss between 10e-6 and 10e-3 almost didn’t change, but after it start to decrease until it reachs the minimum at 10e-1.\nWe don’t want a learning rate bigger than 10e-1 because there where the loss get worse, and we don’t need learning rate at 10e-1 because at this value we’ve left the stage where the loss was decreasing.\n\nwe need to pick the learning rate where the just start to decrease all the way to the minimum: 1e-3\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.327572\n0.370063\n0.120433\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.570779\n0.429716\n0.131935\n01:15\n\n\n1\n0.322137\n0.246614\n0.073072\n01:17\n\n\n\n\n\n\nThe error rate get better 10 times just by using the learning rate methode. Loss also get better by this percentage.\n\n\n\n\n\nWe are familiar with the idea of Transfer Learning, where we use a pretrainned model on our dataset, by fine tuning it in a way that keep all the learned weights and use them in our task.\nWe know tha Convolutional Neural Network consist of many linear layers, and between each two of them there’s a nonlinear activation function (ReLU for example), followed by the final layer with an activation function such as Softmax. The final layer uses a matrix with enough columns such that the ouput size is has the number of classes our model trained to predict(assuming we have a classfication task) This final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset.\nSo we first delete it when we start the transfer learning process, and replace it with a new linear layer with the correct number of outputs that matches our desired task(in this case 37 breeds, so 37 activations)\nThis new linear layer have total randome set of weights, but that doesn’t mean we should set all weights randomly even for the pretrained part.\n\nAll of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the Zeiler and Fergus paper, the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\n\nWe want to build a model such as preserve all the learned weights, and apply them on our dataset, so only adjust them as required for the specifics of our particular task.\nSo, the idea is to keep the pretrained part’s weights intact, and only update the weights of the added part. This process is called Freezing\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nOf Course this is just the default approach, fine_tune has many parameters that allow us to apply different tweaks for each specific situation.\nFor now, let’s do this process manually without using fine_tune\n\n\n# check fine_tune source acode\nlearn.fine_tune??\n\n\nFirst we create our learner from the dls and arch using vision_learner\n\nby default vision_learner will freeze the pre-trained part of the model (freeze the params)\n\nThen train the added layer with randome weights for number of epochs with a learning rate we pick\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.114612\n0.429605\n0.134641\n01:10\n\n\n1\n0.538649\n0.245115\n0.083221\n01:10\n\n\n2\n0.313570\n0.207912\n0.065629\n01:10\n\n\n\n\n\n\nNow we need to unfreeze the model:\n\n\nlearn.unfreeze()\n\n\nNow we run lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn’t appropriate any more:\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.999999747378752e-06)\n\n\n\n\n\n\n\n\n\n\nAs we see here the graph is different than what we saw before when we use randome weights to train the model, because that the model has been trained already.\nThe approach to pick the right lr here is to chose a point before the sharp increase.\n\n\n34se3a\n\n\nlearn.fit_one_cycle(6 , lr_max=4.786300905834651e-06)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.260985\n0.207057\n0.062246\n01:16\n\n\n1\n0.244476\n0.201592\n0.064276\n01:18\n\n\n2\n0.240523\n0.193317\n0.058187\n01:15\n\n\n3\n0.234724\n0.189429\n0.054127\n01:15\n\n\n4\n0.227204\n0.188406\n0.056157\n01:21\n\n\n5\n0.209963\n0.187695\n0.056157\n01:16\n\n\n\n\n\n\n\n\n\nAfter training the model for 6 epochs we get eror_rate at 6% which is fine, but we could do better.\nThe thing we could optimize here is to rethink the learning rate again.\n\npicking one learning rate value for the whole neural network isn’t a good idea.\nthe model is consisted of 2 parts as we know:\n\nthe pre-trained part contained good parameters that has been trained for many epochs\nthe last layer which we trained ourself for not more than 10 (3+6)\n\nso idea here is we shouldn’t trait both parts as if they are the same by picking one learning rate for the whole model\ninstead we could go with a small lr value for the first part, then aplly a slightly bigger one for the last layer.\n\nThis technic is devloped by Jason Yosinski and his team. They shows in 2014 that with transfer learning, different layer should be trained at different speed. \n\nFastai adopt this idea by using slice, which is a built-in object that let you pass 2 values:\n\nthe first define the learning rate of the earlier layer\nthe second for the last layers\n\nThe layers in between will have learning rates that are multiplicatively equidistant throughout that range\n\nLet’s see this technic in action\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(14, lr_max=slice(1e-6,1e-4))\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.129876\n0.383506\n0.121786\n01:10\n\n\n1\n0.516040\n0.284697\n0.092693\n01:13\n\n\n2\n0.328486\n0.217860\n0.071042\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.269753\n0.210582\n0.071719\n01:15\n\n\n1\n0.256654\n0.203205\n0.067659\n01:17\n\n\n2\n0.245040\n0.196284\n0.066982\n01:14\n\n\n3\n0.222485\n0.197652\n0.066306\n01:14\n\n\n4\n0.186354\n0.193144\n0.062923\n01:17\n\n\n5\n0.185777\n0.189425\n0.060217\n01:15\n\n\n6\n0.150886\n0.190105\n0.060893\n01:15\n\n\n7\n0.146768\n0.186121\n0.057510\n01:18\n\n\n8\n0.134524\n0.177772\n0.054804\n01:15\n\n\n9\n0.135853\n0.180999\n0.058187\n01:15\n\n\n10\n0.127154\n0.178239\n0.056834\n01:18\n\n\n11\n0.110540\n0.179652\n0.056157\n01:15\n\n\n12\n0.122252\n0.180609\n0.056834\n01:14\n\n\n13\n0.105743\n0.180926\n0.054804\n01:17\n\n\n\n\n\n\nWe can plot the training and the validation loss\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right amount of epoch you will train the model on is also something we should address properly.\nWe need to keep eye on the train/val loss as shown above, but also on error rate (or any metric we pick).\nIf the loss and the netric are getting better significantly at the end of training, that’s mean we didn’t train for too long\nThe loss is just something we use to allow the optimizer to have something it can different and optimize, it’s not something we really should care about in practice.\n\nif the loss of the validation get worse at during the training because the model is getting over confident, only later it get worse because of overfitting, in practice we care only about the later issue\nIn case of overfitting, the easy solution is to retrain from scratch again, and this time select a total number of epochs based on where your previous best results were found\n\nIt’s not all about epochs, we could add more parameters to the model to get better result\n\n\n\n\n\nIn general, more parameters handle the date more accuratly.\nUsing a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error.\n\nThe way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=\n\nThe other downside of deeper architectures is that they take quite a bit longer to train.\n\nOne technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training.\nTo enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).\n\nYou can’t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let’s try a ResNet-50 now with mixed precision:\n\n\nfrom fastai.callback.fp16 import *\nlearn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.314968\n0.331779\n0.112314\n01:07\n\n\n1\n0.600175\n0.297889\n0.089310\n01:09\n\n\n2\n0.424932\n0.264503\n0.078484\n01:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.294915\n0.276637\n0.076455\n01:08\n\n\n1\n0.303050\n0.266962\n0.077131\n01:10\n\n\n2\n0.241821\n0.301895\n0.086604\n01:08\n\n\n3\n0.144625\n0.222015\n0.060217\n01:08\n\n\n4\n0.082382\n0.166509\n0.056834\n01:10\n\n\n5\n0.060593\n0.161509\n0.060893\n01:08\n\n\n\n\n\n\nWe get better results, at less epochs, and less time per epochs only by usung deeper architecture.\n\nbut it’s allways better to start with small model, before scaling-up"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html#from-dogs-and-cats-to-pet-breeds",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html#from-dogs-and-cats-to-pet-breeds",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "In real world scenarios, the first thing we do is we get in contact with data, usualy at this phase we know nothing about the dataset. We then start to look how to extract the data we want from it, and what the data looks like, and how it is structured.\nUsually data is provided in one of two ways:\n\nIndividual files representing items of data, possibly organized into folder or with filenames representing information about those items\n\ntext documents\n\nimages\n\n\nA table of data in which each row is an item and may include filenames providing connections between the data in the table and data in other formats\n\nCSV files\n\n\n\nExceptions:\n\nDomains like Genomics\n\nbinary database formats\n\nnetwork streams\n\n\n\n# download the dataset\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:02&lt;00:00]\n    \n    \n\n\n\n#get the path as variable, and see what inside\nPath.BASE_PATH = path\npath.ls()\n\n(#2) [Path('images'),Path('annotations')]\n\n\n\nAs we notice here, the data is provided with 2 directories:\n\nimages\nannotations\n\n\n\n#take a look at what inside the images directory\n(path/'images').ls()\n\n(#7393) [Path('images/german_shorthaired_87.jpg'),Path('images/Russian_Blue_200.jpg'),Path('images/Siamese_25.jpg'),Path('images/japanese_chin_48.jpg'),Path('images/miniature_pinscher_161.jpg'),Path('images/pug_150.jpg'),Path('images/pug_120.jpg'),Path('images/pug_63.jpg'),Path('images/samoyed_143.jpg'),Path('images/yorkshire_terrier_190.jpg')...]\n\n\n\nWhen we took a look at these names, we see some paterns: we already know from chapter1 that cats name are uppercase, here we see that after the breed’s name there is a (_) then a number, and finally the extension.\nThis may help us to write some code that extract the breed from a single Path.\n\n\n#pick one \nfname = (path/\"images\").ls()[0]\nfname\n\nPath('images/german_shorthaired_87.jpg')\n\n\n\nThe best way to work with strings and extract patterns from them is to use Regex, which stands for Regular Expression.\n\n\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n\n['german_shorthaired']\n\n\n\nNow we need to label the whole dataset using this code.\nFastai comes with many classes for labeling, in this case when we need to label with help of regex we could use RegexLabeller class within DataBlaock API.\n\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html#presizing",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html#presizing",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "Fastai has this method of presizing the images in a way that conserve its quality after the data augmentation, so it helps the model to learn more lessons from data, and also it helps our dataset to be more varies\nThe idea behind the presizing is we crop the image and resize it to 460 by 460 first, which is a big size by deep learning norms, this operation is done on CPU, then we do the data augmentation in batches, by cropping a rotated random part of that 460^2 image, and taking the cropped image then resize again to a 224 by 224 image, all this operation are done on batch level, which mean on GPU.**\n\n\nitem_tfms=Resize(460),  \nbatch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n![REsize_method](1.png)\n\n* Usually all the augmentation operations we do on a image reduce the quality of the image, but with this approach we could say we can preserve big part of informations of that image so the model can learn better \n\n### Checking and Debugging a DataBlock\n\n* `DataBlock` is just a blueprint for orginizing data before we feed it to the model, you have no guarantee that your template is going to work on your data source as you intend.\n* So, before training a model you should always check your data. You can do this using the show_batch method:\n\n::: {#cell-23 .cell outputId='0f1711e2-f2c6-4703-ed64-1ea456f04a2d' execution_count=10}\n``` {.python .cell-code}\ndls.show_batch(nrows=1, ncols=4 ,unique= True)\n\n\n\n\n\n\n\n:::\n\nIn case we made a mistake in the process of creating datablock, we could use .summary to track the problem.Here we didn’t resize the images in one formm, so couldn’t use the batch transform\nAs we see here the .summary gives us precise diagnostic of the problem:**\n\nat least two tensors in the batch are not the same size.\n\n\n\n\n\nOnce we feel like the datablock is well created, we better begin train the model, and use it as a tool of cleaning the data. If there’s a problem with data or the model, we better know that before we lost lot of time and energy on data cleaning even before testing the model.\n\n\n# train the model\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.526362\n0.348364\n0.111637\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.482150\n0.309586\n0.093369\n01:16\n\n\n1\n0.327810\n0.229392\n0.064953\n01:17\n\n\n\n\n\n\n\n\n\nAs we saw before in Chapter 2 the best tool to clean the data is basically the model itself\nAfter creating the datablock and dataloader we better train the model and get some feedback so we know if something is wrong very early, and if not we start to use the model as tool to investigate the data\nUsually before we train the model we have to decide the function that will update the parameters, a Loss Function. But here we didn’t create any loss function?\nIf we didn’t decide the way by which we update the paramters, Fastai by default will chose a loss function for us.\n\nthe chosen loss function will suite the kind of model we build, and the type of dataset we have.\n\n\n\n# check the loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\nThe loss function used to train this model is Cross Entropy Loss"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html#cross-entropy-loss",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html#cross-entropy-loss",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "The Cross-Entropy Loss is a function similar to what we saw in the previous chapter, when we created the mnist_loss function:\n\ndef mnist_loss(predictions, targets):\n    prediction = prediction.sigmoid()\n    return torch.where(targets==1, 1-prediction, predictions).mean()\n\nThe problem with this function is, it only takes 2 categories(3, 7) but here we have 37 types of breeds.but here we have multiple classes.\n\nit can takes more than 2 categories\n\n\n\n\n\nIn order to understand the cross-entropy loss, let’s grab a batch of data\n\n\nx,y = dls.one_batch()\n\n\nIt return the activations of dependent and independent variable of one mini-batch\n\n\n# independent variable\ny, len(y)\n\n(TensorCategory([ 8, 34,  6, 33, 12, 15, 32, 14,  5, 27, 20, 32, 18, 24,  8, 29,  5,  4,  0, 28, 10, 12, 16, 25, 29,  3, 34, 27, 30, 15,  6, 15, 27, 34, 14, 21,  5, 17, 31, 26, 13, 35, 17, 35, 23, 14,\n                 35, 35,  8,  7, 21,  0, 22, 17, 19, 26, 16,  5, 15, 27, 11, 22, 34, 18], device='cuda:0'),\n 64)\n\n\n\nIt return 64 number, each represent on of the 37 breeds index\n\n\n# all indepent variables\ndls.vocab, len(dls.vocab)\n\n(['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier'],\n 37)\n\n\n\nHere we use get_preds to get the predictions for each image in the dataset or mini-batch (like in this example).\n\n\npreds,_ = learn.get_preds(dl=[(x,y)])\n\n\n\n\n\n\n\n\n\n# prediction for image [0] in the mini-batch\npreds[0]\n\nTensorBase([1.0919e-05, 6.0375e-08, 8.5680e-05, 7.1281e-07, 1.8754e-06, 4.7956e-07, 1.1708e-04, 1.4080e-06, 9.9972e-01, 9.4731e-07, 6.8318e-08, 1.7055e-07, 1.9338e-06, 7.7372e-07, 1.7931e-07,\n            1.5261e-07, 1.1363e-08, 2.3353e-07, 1.3689e-07, 4.5361e-07, 7.1548e-08, 4.9425e-06, 1.0026e-05, 2.6908e-07, 3.5724e-07, 3.1201e-07, 1.0177e-08, 2.7219e-08, 1.6157e-06, 3.5337e-08,\n            1.8094e-06, 3.3425e-05, 2.3514e-08, 3.0318e-06, 1.4910e-07, 1.9582e-07, 9.5147e-08])\n\n\n\nThe 37 predictions refer to the probability of each breed to match the image[0].\n\nif we sum() them up they add up to 1:\n\n\n\npreds[0].sum()\n\nTensorBase(1.0000)\n\n\n\nIn order to transform the activations of our model into prediction, we use Soft-Max\n\n\n\n\n\nAs we said before, Softmax is similar to sigmoid function we use before, but it only can handel more than 2 classes.\n\n\n# reminder of sigmoid\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n\n\n\nThis function allow us to predict whether a activation number is pointing to each category of the two, by calculating which activation is big and by much. But in our case today we have 37 category, which means by this logic we need a activation for each one.\nFirst let’s create a similar situation where we have only 2 categories, but we won’t solve it as it’s a binary problem (it's 3) but as 2 categories problem, each has it’s activation, and their probabilty sum up to 1.\n\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\n\nWe can’t just take the sigmoid of this directly, since we don’t get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):\n\n\nsigmoid(acts)\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\n\nEevn though we try different approach to solve the same problem, we still have some similarities.\nWe will use sigmoid on each activation.\nAnd we still need to substract an activation from another beacuse that represent how much the model sure about an image is assigned to each category, thats for first column.\nIn the second colun we just use 1 - prediction (activation of the second column)\n\n\ndiffs = acts[:, 0] - acts[:, 1]\n\n\n# create the sigmoid function of both categories\nsigm_ver = torch.stack([diffs.sigmoid(), 1-diffs.sigmoid()], dim=1)\n\n\nWe can express the softmax function as:\n\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\n\nLet’s check that softmax returns the same values as sigmoid for the first column, and those values subtracted from 1 for the second column:\n\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts, sigm_ver\n\n(tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]), tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]))\n\n\n\nSoftmax calculate the \\(exp^{x}\\) and divide it by sum \\(exp^{x}\\) of all activations of other categories.\n\nthe exp make sure the biggest activation is way bigger than others\ndividing by the sum is what make softmax values add up to 1\n\n\n\n\n\n\nIn the previous chapter when we created mnis_loss, we used torch.where to select between the input and 1-input.\nWith softmax, we will use indexing.\n\n\n# the pretended targets\ntargs = tensor([0, 1, 0, 1, 1, 0])\n\n\n# create an index\nidx = range(6)\n\n\n# the softmax activations\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nHere we make the targs decide which activation we pick in each row\n\n\nsm_acts[idx, targs]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\n\nlet’s display what we just did :\n\n\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targs'] = targs\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targs]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n\n\n\n\n3\n7\ntargs\nidx\nresult\n\n\n\n\n0.602469\n0.397531\n0\n0\n0.602469\n\n\n0.502065\n0.497935\n1\n1\n0.497935\n\n\n0.133188\n0.866811\n0\n2\n0.133188\n\n\n0.996640\n0.003360\n1\n3\n0.003360\n\n\n0.595949\n0.404051\n1\n4\n0.404051\n\n\n0.366118\n0.633882\n0\n5\n0.366118\n\n\n\n\n\n\nBut idea here is not to use it in a simple binary problem, because torch.where could did the same job here, but is to use it in order to solve a multi-categorie problem\n\nPyTorch provides a function that does exactly the same thing as sm_acts[range(n), targ] (except it takes the negative, because when applying the log afterward, we will have negative numbers), called nll_loss (NLL stands for negative log likelihood):\n\n-sm_acts[idx, targs]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targs, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\n\n\n\n\n\ncross-entropy-loss-function.png\n\n\n\nThe using of logarithms allow us to do all kind of multiplications without carring about the size of the output.\n\nthe nature of log functions make them increase lineary when the underlying signal increase exponentialy.\nlog(a*b) = log(a)+log(b)\nthe log of a number approaches negative infinity when the number approaches zero\n\nIn our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is “good” (closer to 1) and a large value when the prediction is “bad” (closer to 0).\nNotice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class. One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training.\n\nCalculating the loss pay attention only to the high softmax value.\n\n\n\n\n\nAfter taking the log of the softmax, we can then call the negative log likelihood.\n\nfirst : log_softmax\nthen : nll_loss\nor : nn.CrossEntropyLoss()\n\n\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targs)\n\ntensor(1.8045)\n\n\n\nnn.CrossEntropyLoss()(acts, targs)\n\ntensor(1.8045)\n\n\n\nThe nn.CrossEntrpyLoss() make do all the steps for us, but if we want to go through all those steps one by one softmas+log then negative log we could do it also:\n\n\nF.nll_loss(nn.Softmax()(acts).log(), targs,)\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\ntensor(1.8045)\n\n\n\nAdding the reduction='none' to this functions will return the loss of each row, if we didn’t add this aparameter the fuction will return the mean loss of all rows.\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html#model-interpretation",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html#model-interpretation",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "As we saw in chapter 3 it’s hard for us to interpret the loss function, since it’s some the computers use in order to updates the parameters and optimize the performance.\nBut we can use some kind of demonstration that shows where the model did good, and where did bad.\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIts was easy to understand what happened when they were only 3 classes in bears model, but here we have 37 breeds.\n\nthats why we will useinterp.most_confused(min_val=5) to output to most bad decisions the model taked\n\n\n\ninterp.most_confused(min_val=5)\n\n\n\n\n\n\n\n\n[('Ragdoll', 'Birman', 6)]\n\n\n\nThe best way to understand what happend is to google the names of each breed and see why the model confused it with the other breed, so we know that the model is in the right track"
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html#improving-our-model",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html#improving-our-model",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "At this point all we can do is improve the model by correcting some detaills that may optimize the final prefromance\n\n\n\n\nOne way of improving our model is by picking the right learning rate.\n\nit will help to get faster result per epoch\nminimize the loss and updating parameters with less steps\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.615709\n5.479418\n0.529093\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n3.068122\n1.406392\n0.443843\n01:15\n\n\n\n\n\n\nHere we pick a learning rate 0.1 which is 5 times bigger than the last one 0.002 and we get bad results error rate at: 0.5\n\nbig learning rate may reduce the computation needed for the training process but the model performance will be bad\n\nAlso if we pick a small learning rate it will take forever to achieve something.\nThe answear for this dilemma is The Learning Rate Finder\n\nFastai library adopte this method created by the resaercher Leslie Smith in a paper in 2015.\n\nthe idea of Smith is to start with a small learning rate (very small), and use it for one mini-batch, see how much the loss changed, and then start increasing the learning rate by some percentage (doubling it since its very small anyway)\nrepeate this process again(track the loss, double the learning rate ..) until the loss get worse.\nat this point we just pick a learning rate smaller than the one that causes the loss to get worse.\n\nFastai course advice is either:\n\none order of magnitude less than where the minimun loss was achieved(divide by 10)\nthe last point where the loss was clearly decreasing\n\nBoth point are giving the same value usually.\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n\nMinimum/10: 1.00e-02, steepest point: 4.37e-03\n\n\n\nThe plot shows that the loss between 10e-6 and 10e-3 almost didn’t change, but after it start to decrease until it reachs the minimum at 10e-1.\nWe don’t want a learning rate bigger than 10e-1 because there where the loss get worse, and we don’t need learning rate at 10e-1 because at this value we’ve left the stage where the loss was decreasing.\n\nwe need to pick the learning rate where the just start to decrease all the way to the minimum: 1e-3\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.327572\n0.370063\n0.120433\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.570779\n0.429716\n0.131935\n01:15\n\n\n1\n0.322137\n0.246614\n0.073072\n01:17\n\n\n\n\n\n\nThe error rate get better 10 times just by using the learning rate methode. Loss also get better by this percentage.\n\n\n\n\n\nWe are familiar with the idea of Transfer Learning, where we use a pretrainned model on our dataset, by fine tuning it in a way that keep all the learned weights and use them in our task.\nWe know tha Convolutional Neural Network consist of many linear layers, and between each two of them there’s a nonlinear activation function (ReLU for example), followed by the final layer with an activation function such as Softmax. The final layer uses a matrix with enough columns such that the ouput size is has the number of classes our model trained to predict(assuming we have a classfication task) This final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset.\nSo we first delete it when we start the transfer learning process, and replace it with a new linear layer with the correct number of outputs that matches our desired task(in this case 37 breeds, so 37 activations)\nThis new linear layer have total randome set of weights, but that doesn’t mean we should set all weights randomly even for the pretrained part.\n\nAll of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the Zeiler and Fergus paper, the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\n\nWe want to build a model such as preserve all the learned weights, and apply them on our dataset, so only adjust them as required for the specifics of our particular task.\nSo, the idea is to keep the pretrained part’s weights intact, and only update the weights of the added part. This process is called Freezing\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nOf Course this is just the default approach, fine_tune has many parameters that allow us to apply different tweaks for each specific situation.\nFor now, let’s do this process manually without using fine_tune\n\n\n# check fine_tune source acode\nlearn.fine_tune??\n\n\nFirst we create our learner from the dls and arch using vision_learner\n\nby default vision_learner will freeze the pre-trained part of the model (freeze the params)\n\nThen train the added layer with randome weights for number of epochs with a learning rate we pick\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.114612\n0.429605\n0.134641\n01:10\n\n\n1\n0.538649\n0.245115\n0.083221\n01:10\n\n\n2\n0.313570\n0.207912\n0.065629\n01:10\n\n\n\n\n\n\nNow we need to unfreeze the model:\n\n\nlearn.unfreeze()\n\n\nNow we run lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn’t appropriate any more:\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.999999747378752e-06)\n\n\n\n\n\n\n\n\n\n\nAs we see here the graph is different than what we saw before when we use randome weights to train the model, because that the model has been trained already.\nThe approach to pick the right lr here is to chose a point before the sharp increase.\n\n\n34se3a\n\n\nlearn.fit_one_cycle(6 , lr_max=4.786300905834651e-06)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.260985\n0.207057\n0.062246\n01:16\n\n\n1\n0.244476\n0.201592\n0.064276\n01:18\n\n\n2\n0.240523\n0.193317\n0.058187\n01:15\n\n\n3\n0.234724\n0.189429\n0.054127\n01:15\n\n\n4\n0.227204\n0.188406\n0.056157\n01:21\n\n\n5\n0.209963\n0.187695\n0.056157\n01:16\n\n\n\n\n\n\n\n\n\nAfter training the model for 6 epochs we get eror_rate at 6% which is fine, but we could do better.\nThe thing we could optimize here is to rethink the learning rate again.\n\npicking one learning rate value for the whole neural network isn’t a good idea.\nthe model is consisted of 2 parts as we know:\n\nthe pre-trained part contained good parameters that has been trained for many epochs\nthe last layer which we trained ourself for not more than 10 (3+6)\n\nso idea here is we shouldn’t trait both parts as if they are the same by picking one learning rate for the whole model\ninstead we could go with a small lr value for the first part, then aplly a slightly bigger one for the last layer.\n\nThis technic is devloped by Jason Yosinski and his team. They shows in 2014 that with transfer learning, different layer should be trained at different speed. \n\nFastai adopt this idea by using slice, which is a built-in object that let you pass 2 values:\n\nthe first define the learning rate of the earlier layer\nthe second for the last layers\n\nThe layers in between will have learning rates that are multiplicatively equidistant throughout that range\n\nLet’s see this technic in action\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(14, lr_max=slice(1e-6,1e-4))\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.129876\n0.383506\n0.121786\n01:10\n\n\n1\n0.516040\n0.284697\n0.092693\n01:13\n\n\n2\n0.328486\n0.217860\n0.071042\n01:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.269753\n0.210582\n0.071719\n01:15\n\n\n1\n0.256654\n0.203205\n0.067659\n01:17\n\n\n2\n0.245040\n0.196284\n0.066982\n01:14\n\n\n3\n0.222485\n0.197652\n0.066306\n01:14\n\n\n4\n0.186354\n0.193144\n0.062923\n01:17\n\n\n5\n0.185777\n0.189425\n0.060217\n01:15\n\n\n6\n0.150886\n0.190105\n0.060893\n01:15\n\n\n7\n0.146768\n0.186121\n0.057510\n01:18\n\n\n8\n0.134524\n0.177772\n0.054804\n01:15\n\n\n9\n0.135853\n0.180999\n0.058187\n01:15\n\n\n10\n0.127154\n0.178239\n0.056834\n01:18\n\n\n11\n0.110540\n0.179652\n0.056157\n01:15\n\n\n12\n0.122252\n0.180609\n0.056834\n01:14\n\n\n13\n0.105743\n0.180926\n0.054804\n01:17\n\n\n\n\n\n\nWe can plot the training and the validation loss\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right amount of epoch you will train the model on is also something we should address properly.\nWe need to keep eye on the train/val loss as shown above, but also on error rate (or any metric we pick).\nIf the loss and the netric are getting better significantly at the end of training, that’s mean we didn’t train for too long\nThe loss is just something we use to allow the optimizer to have something it can different and optimize, it’s not something we really should care about in practice.\n\nif the loss of the validation get worse at during the training because the model is getting over confident, only later it get worse because of overfitting, in practice we care only about the later issue\nIn case of overfitting, the easy solution is to retrain from scratch again, and this time select a total number of epochs based on where your previous best results were found\n\nIt’s not all about epochs, we could add more parameters to the model to get better result\n\n\n\n\n\nIn general, more parameters handle the date more accuratly.\nUsing a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error.\n\nThe way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=\n\nThe other downside of deeper architectures is that they take quite a bit longer to train.\n\nOne technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training.\nTo enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).\n\nYou can’t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let’s try a ResNet-50 now with mixed precision:\n\n\nfrom fastai.callback.fp16 import *\nlearn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.314968\n0.331779\n0.112314\n01:07\n\n\n1\n0.600175\n0.297889\n0.089310\n01:09\n\n\n2\n0.424932\n0.264503\n0.078484\n01:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.294915\n0.276637\n0.076455\n01:08\n\n\n1\n0.303050\n0.266962\n0.077131\n01:10\n\n\n2\n0.241821\n0.301895\n0.086604\n01:08\n\n\n3\n0.144625\n0.222015\n0.060217\n01:08\n\n\n4\n0.082382\n0.166509\n0.056834\n01:10\n\n\n5\n0.060593\n0.161509\n0.060893\n01:08\n\n\n\n\n\n\nWe get better results, at less epochs, and less time per epochs only by usung deeper architecture.\n\nbut it’s allways better to start with small model, before scaling-up"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "",
    "text": "Do you need these for deep learning?\n\nLots of math\n\nFalse\n\nLots of data\n\nFalse\n\nLots of expensive computers\n\nFalse\n\nA PhD\n\nFalse\n\n\n\n\n\nName five areas where deep learning is now the best in the world.\n\nMedical research\nRobotics\nAssurance\nLinguistics/ Natural Lunguage Processing\nBiology\n\n\n\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nPercepton\n\n\n\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nMinsky in his book Perceptons shows the limitation of the device “perceptons” and how it cannot solve any comlex problem, the ai community did agreed with Minsky, but the did not pay attention to the solution he suggested, which is a 2 layer model.\nIn 80’s the 2 layer models were usual in ai labs. In theory a 2 layer model can solve any problem, but in practice the layers were too big and consum a lot of computation power, the solution to this is to add more layers, but this insight was not acknowledged, what leat to the 2 winter of NN.\n\n\n\nWhat is a GPU?\nIt’s a Graphic Prossessing Unit, which is used to do many computation tasks in parallel, which help to accelerate to compution of big tasks by cutting them into small tasks and compute them in parallel\n\n\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nto write a programm that recognize images in a photo we need to write a long set of rules that resume any possible photo and image, tell the computer exactly how to deal with any of them, which is way more complicated that what we can do. That’s way we use machine learning to solve those kinfd of problems, just by showing the model data and help it to learn from it.\n\n\n\nWhat did Samuel mean by “Weight Assignment”?\nWhat term do we normally use in deep learning for what Samuel called “Weights”?\nDraw a picture that summarizes Arthur Samuel’s view of a machine learning model\nweight Assignment is refer to the parameters of the model, these are what we call today weights and bias. they are set of value we assign to each data point, what make the optimization of the loss possible.\n\n\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nThis is a highly-researched topic known as interpretability of deep learning models. the natur of deep learning “deep” make it hard to really understand the way the model solve each problem, specially if the model has many layers, what makes it even hard to know exactly which layer is responsable of the procces of learning which part.\n\n\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nUniversal approximation theorem\n\n\n\nWhat do you need in order to train a model?\nIn order to train a model, we need architecture for the given problem, we first need data(+labels), then we need set of values (paramaters), then we need some kind of metric to know if our model did good or bad (loss function), and we need a way to updates these parameters in order to optimize the loss function.\n\n\n\nHow could a feedback loop impact the rollout of a predictive policing model?\nIn a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop\n\n\n\nDo we always have to use 224x224 pixel images with the cat recognition model?\nNo.\n\n\n\nWhat is the difference between classification and regression?\nClassification problem is when we need to decide between 2(or more) classes, the prediction in this problem isn’t quantity, where regression problem is focused on predecting numeric quantity.\n\n\n\nWhat is a validation set? What is a test set? Why do we need them?\nValidation set is small portion of the data set that we preserve from the training fase in order to prevent the model from memorizing the data instead of learning from it. The validation set allow us to measure the performance of the model on data that the model didn’t see before. Same we can say about test set, which is another preserved protion of data that we use in the final fase of the training in order to have a real idea of model performance.\n\n\n\nWhat will fastai do if you don’t provide a validation set?\nit will automatically create a validation set of 20% of our dataset.\nvalid_pct=0.2\n\n\n\nWhat is overfitting?\nis when the model is memorizing answears instead of learning from data.\n\n\n\nWhat is a metric? How does it differ to “loss”?\nMetric is what tells us how the model perform, in other way the loss is what we should minimize in order to optimize the model perfromance.as we will see later, sometimes, we could use the metric as loss.\n\n\n\nHow can pretrained models help?\npertrained model can be used again, we just need to fin it in our probem.\na pretrained model is a model that learned many lesson from the prior problem, and already has good set of paramters, these parameters(weights+biases) are what we seek in the proccess of using a pretrained model. this procces is called fine-tunning, which mean less money and time consuming.\n\n\n\nWhat is the “head” of a model?\nWhen using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model.\n\n\n\nWhat is an “architecture”?\nThe architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.\n\n\n\nWhat are “hyperparameters”?\nTraining models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters."
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#questionnaire",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#questionnaire",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "",
    "text": "Do you need these for deep learning?\n\nLots of math\n\nFalse\n\nLots of data\n\nFalse\n\nLots of expensive computers\n\nFalse\n\nA PhD\n\nFalse\n\n\n\n\n\nName five areas where deep learning is now the best in the world.\n\nMedical research\nRobotics\nAssurance\nLinguistics/ Natural Lunguage Processing\nBiology\n\n\n\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nPercepton\n\n\n\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nMinsky in his book Perceptons shows the limitation of the device “perceptons” and how it cannot solve any comlex problem, the ai community did agreed with Minsky, but the did not pay attention to the solution he suggested, which is a 2 layer model.\nIn 80’s the 2 layer models were usual in ai labs. In theory a 2 layer model can solve any problem, but in practice the layers were too big and consum a lot of computation power, the solution to this is to add more layers, but this insight was not acknowledged, what leat to the 2 winter of NN.\n\n\n\nWhat is a GPU?\nIt’s a Graphic Prossessing Unit, which is used to do many computation tasks in parallel, which help to accelerate to compution of big tasks by cutting them into small tasks and compute them in parallel\n\n\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nto write a programm that recognize images in a photo we need to write a long set of rules that resume any possible photo and image, tell the computer exactly how to deal with any of them, which is way more complicated that what we can do. That’s way we use machine learning to solve those kinfd of problems, just by showing the model data and help it to learn from it.\n\n\n\nWhat did Samuel mean by “Weight Assignment”?\nWhat term do we normally use in deep learning for what Samuel called “Weights”?\nDraw a picture that summarizes Arthur Samuel’s view of a machine learning model\nweight Assignment is refer to the parameters of the model, these are what we call today weights and bias. they are set of value we assign to each data point, what make the optimization of the loss possible.\n\n\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nThis is a highly-researched topic known as interpretability of deep learning models. the natur of deep learning “deep” make it hard to really understand the way the model solve each problem, specially if the model has many layers, what makes it even hard to know exactly which layer is responsable of the procces of learning which part.\n\n\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nUniversal approximation theorem\n\n\n\nWhat do you need in order to train a model?\nIn order to train a model, we need architecture for the given problem, we first need data(+labels), then we need set of values (paramaters), then we need some kind of metric to know if our model did good or bad (loss function), and we need a way to updates these parameters in order to optimize the loss function.\n\n\n\nHow could a feedback loop impact the rollout of a predictive policing model?\nIn a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop\n\n\n\nDo we always have to use 224x224 pixel images with the cat recognition model?\nNo.\n\n\n\nWhat is the difference between classification and regression?\nClassification problem is when we need to decide between 2(or more) classes, the prediction in this problem isn’t quantity, where regression problem is focused on predecting numeric quantity.\n\n\n\nWhat is a validation set? What is a test set? Why do we need them?\nValidation set is small portion of the data set that we preserve from the training fase in order to prevent the model from memorizing the data instead of learning from it. The validation set allow us to measure the performance of the model on data that the model didn’t see before. Same we can say about test set, which is another preserved protion of data that we use in the final fase of the training in order to have a real idea of model performance.\n\n\n\nWhat will fastai do if you don’t provide a validation set?\nit will automatically create a validation set of 20% of our dataset.\nvalid_pct=0.2\n\n\n\nWhat is overfitting?\nis when the model is memorizing answears instead of learning from data.\n\n\n\nWhat is a metric? How does it differ to “loss”?\nMetric is what tells us how the model perform, in other way the loss is what we should minimize in order to optimize the model perfromance.as we will see later, sometimes, we could use the metric as loss.\n\n\n\nHow can pretrained models help?\npertrained model can be used again, we just need to fin it in our probem.\na pretrained model is a model that learned many lesson from the prior problem, and already has good set of paramters, these parameters(weights+biases) are what we seek in the proccess of using a pretrained model. this procces is called fine-tunning, which mean less money and time consuming.\n\n\n\nWhat is the “head” of a model?\nWhen using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model.\n\n\n\nWhat is an “architecture”?\nThe architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.\n\n\n\nWhat are “hyperparameters”?\nTraining models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters."
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "Further Research",
    "text": "Further Research\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:",
    "text": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:\n\nGPU stands for Graphic Processing Unit, is a specialized processor with dedicated memory that conventionally perform floating point operations required for rendering graphics.\nIn other words, it’s a single-chip processor used for extensive graphical and mathematical computations which help the CPU to achieve other tasks.\nWhile CPU is designed to handel the complex logic in code once at a time, GPU can do many small operations at the same time, which make it very convenient to deep learning where we need to do many milions of calculations in order to train a model\n\n\nWhy do Deep Learning needs GPU?\n\nGPUs are optimized for training neural networks models as they can process multiple computations simultaneously.\nFor example the model we’ve fine-tuned in chapter one resnet18 which the smaller version of resnet models with only 18 hidden layers,though it has more than 11 millions parameters, in order to do one epoch and calculate all these parameter (wights + biases) and multiply them by the input variables (images) then do the Back-probagation and update them after calculating the loss … all this multiplications will take a large amount of time if we did it on CPU."
  },
  {
    "objectID": "posts/Data PreProcessing For NLP Problem/Multi_choice_with_SWAG.html",
    "href": "posts/Data PreProcessing For NLP Problem/Multi_choice_with_SWAG.html",
    "title": "Multi Choice with Swag Dataset",
    "section": "",
    "text": "Introduction:\n\nIn this notebook I will try to investigate the SWAG datasets.\nThe idea is to understand how to deal with multiple choice datasets and how to prepare them for the next step.\nMultiple choice is frequent problem in the filed of LLMs and NLP in general\nSo the preprocessing of data will have a huge effect on the success of any proposed solution\n\n\n# load the dataset\nfrom datasets import load_dataset\ndataset = load_dataset('swag', 'regular')\n\n\n# let's grab a sample\ndataset['train'][0]\n\n{'video-id': 'anetv_jkn6uvmqwh4',\n 'fold-ind': '3416',\n 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n 'sent2': 'A drum line',\n 'gold-source': 'gold',\n 'ending0': 'passes by walking down the street playing their instruments.',\n 'ending1': 'has heard approaching them.',\n 'ending2': \"arrives and they're outside dancing and asleep.\",\n 'ending3': 'turns the lead singer watches the performance.',\n 'label': 0}\n\n\n\nThese fields represent the idea begind this dataset\n\na situation where we have to predict the right ending\nsent1 and sent2 represent the given situation and they added up to startphrase\nendings 0 to 3 represent the the endings for that situation, only one is the right\nlabel index the right answer\n\nNow let’s initialized BERT and load its tokenizer.\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n\nThe idea here is to tokenize a start sentence with each one of the 4 choices,\n\n\nending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n\n\ndef preprocess_function(examples):\n    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n    question_headers = examples[\"sent2\"]\n    second_sentences = [\n        [f\"{header} {examples[end][i]}\" for end in ending_names]\n        for i, header in enumerate(question_headers)\n    ]\n\n    first_sentences = sum(first_sentences, [])\n    second_sentences = sum(second_sentences, [])\n\n    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n\n\nTo understand each operation of that function we will do it step-by-step:\n\nFirst let’s create a sub-set of the training set\nCreate a endings list that we will use later\n\n\n\nendings = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\ntrain_ds = dataset['train']\nsmp = train_ds[:20]\n\n\nMultiply each sent1 by 4 and stack them all in a list:\n\n\nsent_1 = [[sent] * 4 for sent in smp['sent1']]\n\n\nLet’s retrieve the length of that list and see what’s inside one element of it.\n\n\nsent_1[2], len(sent_1)\n\n(['A group of members in green uniforms walks waving flags.',\n  'A group of members in green uniforms walks waving flags.',\n  'A group of members in green uniforms walks waving flags.',\n  'A group of members in green uniforms walks waving flags.'],\n 20)\n\n\n\nSo basically we have 4 copies of each first-sentence of the dataset.\nNow we will create a list of the second-sentence or the header.\n\n\nheaders = smp['sent2']\n\n\nAt this point we have:\n\nsent_1 which each element is multiplied by 4\nheaders that complete sent_1\n\nThe idea here is to create pairs of each header +sent_2 for each sent_1.\n\n\nsent_2 = [[f'{head}{smp[end][i]}' for end in endings] for i, head in enumerate(headers)]\n\n\n\n\nPre-processing\n\n\n\nNow we need to flatten the pair of sentences, so we could tokenize them:\n\n\nfrst_sent = sum(sent_1, [])\nscnd_sent = sum(sent_2, [])\ntok_smp = tokenizer(frst_sent, scnd_sent, truncation=True)\n\n\nWe tokenize the pair of list sentences which will return a dictionary with 3 keys:\n\n\ntok_smp.keys()\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nBut since we already flattend the pairs before the tokenization step, we need to get them unflatten again so we can pass it through the map() function in order to be computed by the model.\n\n\noutputs = {k: [v[i: i + 4] for i in range(0, len(v), 4)] for k, v in tok_smp.items()}\n\n\nLets check if we get the unflatten step right, we just need to make sure that the input_ids of the first sentence has the same values in both: tok_smp and outputs:\n\n\nflatten_smp = tok_smp['input_ids']\nunflatten_smp = outputs['input_ids']\n\n\nflatten_smp[0:4] == unflatten_smp[0]\n\nTrue"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "As we saw in the previous Chapter, we can create powerful model with only 6 lines of code\n\nAlthought we should understand the constraints of the process and not overestimate the capabilities of deep learnig, this may lead to frustaingtly poor result\n\nAlso we need to not overestimate the constraints, and underestimate what could do with deep learning"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "Deep Learning In Production",
    "text": "Deep Learning In Production\nWhen we create a model that met our objectives, we can then pass to the Production phase, where we transform the model into an appliction/service etc..\nBut first we need to export the model into a file:\n\nlearn.export()\n\nLet’s check that the file exists, by using the ls method that fastai adds to Python’s Path class:\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\n\nNow with the export.pkl file we can create normal program that takes inputs(images) and produce results (predictions) just like any traditional app\nAt this point we won’t call it a model anymore, we call it inference.\n\n &gt;Using the trained model as program\nThe export() function allow us to save the model in oreder to use it later, and as we know model is Architecture + Parameters, Fastai by default saves also the method of which we’ve created the DataLoaders, because otherwise we have to define it again in order to work with the new data we will feed to the model.\n\nFrom Model to Inference\nThis file export.pkl is allways needed wherever we will create an app from it, for now we will use it whithin this notebook in order to create a small app that can predict bears type from image we will provide.\nWhen we use a model for getting predictions, instead of training, we call it inference.\nTo create inference learner from export.pkl file we use load_learner:\n\nlearn_inf = load_learner(path/'export.pkl')\n\n\n# predicting one image\nlearn_inf.predict('images/bear.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([8.0130e-07, 1.0000e+00, 2.0153e-07]))\n\n\n\nThis has returned three things:\n\nthe predicted category(label) in the same format we originally provided (in this case that’s a string),\nthe index of the predicted category, and the probabilities of each category.\nthe last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories.\n\nAt inference time, you can access the DataLoaders as an attribute of the Learner:\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\nWe can see here that if we index into the vocab with the integer returned by predict then we get back “grizzly,” as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly.\n\n\nGradio + HugginFace Spaces\n\nIf we want to share our model with a broader audience, and showcase our skills we need to create a real app that can be used outside of the datascience/machine learning word where nobody know or have the ability to use jupyter notebook or python.. and that’s why we will show using a combination of python package Gradio that will allow us to build our app, then host it on HuggingFace\n\n\n# firstly install gradio\n!pip install gradio\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting gradio\n  Downloading gradio-3.6-py3-none-any.whl (5.3 MB)\n     |████████████████████████████████| 5.3 MB 5.2 MB/s \nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\nCollecting pydub\n  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\nCollecting paramiko\n  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n     |████████████████████████████████| 212 kB 67.2 MB/s \nCollecting httpx\n  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\nCollecting orjson\n  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n     |████████████████████████████████| 270 kB 62.9 MB/s \nCollecting ffmpy\n  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\nCollecting fastapi\n  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n     |████████████████████████████████| 55 kB 3.8 MB/s \nRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\nCollecting h11&lt;0.13,&gt;=0.11\n  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n     |████████████████████████████████| 54 kB 3.7 MB/s \nRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\nCollecting pycryptodome\n  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n     |████████████████████████████████| 2.3 MB 47.6 MB/s \nRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.8.2)\nCollecting markdown-it-py[linkify,plugins]\n  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\nCollecting python-multipart\n  Downloading python-multipart-0.0.5.tar.gz (32 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\nCollecting uvicorn\n  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n     |████████████████████████████████| 56 kB 5.3 MB/s \nCollecting websockets\n  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n     |████████████████████████████████| 112 kB 70.9 MB/s \nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (1.3.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (4.1.1)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (4.0.2)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (22.1.0)\nRequirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (2.1.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (1.2.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (6.0.2)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (1.8.1)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;gradio) (0.13.0)\nRequirement already satisfied: idna&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl&lt;2.0,&gt;=1.0-&gt;aiohttp-&gt;gradio) (2.10)\nCollecting starlette==0.20.4\n  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n     |████████████████████████████████| 63 kB 2.5 MB/s \nCollecting anyio&lt;5,&gt;=3.4.0\n  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n     |████████████████████████████████| 80 kB 10.6 MB/s \nCollecting sniffio&gt;=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx-&gt;gradio) (2022.9.24)\nCollecting httpcore&lt;0.16.0,&gt;=0.15.0\n  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n     |████████████████████████████████| 68 kB 6.7 MB/s \nCollecting rfc3986[idna2008]&lt;2,&gt;=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;gradio) (2.0.1)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting mdit-py-plugins\n  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n     |████████████████████████████████| 46 kB 4.5 MB/s \nCollecting linkify-it-py~=1.0\n  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\nCollecting uc-micro-py\n  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;gradio) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;gradio) (0.11.0)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;gradio) (2.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;gradio) (1.4.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;gradio) (1.15.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;gradio) (2022.4)\nCollecting pynacl&gt;=1.0.1\n  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n     |████████████████████████████████| 856 kB 63.6 MB/s \nCollecting cryptography&gt;=2.5\n  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n     |████████████████████████████████| 4.0 MB 41.4 MB/s \nCollecting bcrypt&gt;=3.1.3\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n     |████████████████████████████████| 593 kB 68.7 MB/s \nRequirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography&gt;=2.5-&gt;paramiko-&gt;gradio) (1.15.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=2.5-&gt;paramiko-&gt;gradio) (2.21)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;gradio) (1.25.11)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;gradio) (3.0.4)\nRequirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn-&gt;gradio) (7.1.2)\nBuilding wheels for collected packages: ffmpy, python-multipart\n  Building wheel for ffmpy (setup.py) ... done\n  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=bf42b4bf9ba2633a885814063d3340770c60207e05f131d9e165fafd4a5e1aa5\n  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n  Building wheel for python-multipart (setup.py) ... done\n  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=f8ff9e663a939cd5bba16c9c7ce235f69c0fab5ae8199bbfce9b5fdb3361d05e\n  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\nSuccessfully built ffmpy python-multipart\nInstalling collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\nSuccessfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.1 fastapi-0.85.1 ffmpy-0.3.0 gradio-3.6 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.19.0 websockets-10.3\n\n\n\n# create labels from dalaloaders vocab\nlabels = learn.dls.vocab\n# predicting function that take an image as input and use learn.predict to ouput:\n# prediction, prediction index, and probability.\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\n\nHere we will use gradio in order to create an app withing this notebook.\n\n\n\nimport gradio as gr\ngr.Interface(fn=predict, inputs=gr.inputs.Image(shape=(512, 512)), outputs=gr.outputs.Label(num_top_classes=3)).launch(share=True)\n\n/usr/local/lib/python3.7/dist-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n  \"Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n  warnings.warn(value)\n/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n  warnings.warn(value)\n\n\nColab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\nRunning on public URL: https://bbc80cb39aa58cc0.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n(&lt;gradio.routes.App at 0x7f45d2b49710&gt;,\n 'http://127.0.0.1:7860/',\n 'https://bbc80cb39aa58cc0.gradio.app')\n\n\n\nStill the best way of creating app for inference is to use Hugginface platform with help of gradio.\nHere is the app that classify bears types based the .pkl file we created from our model we trained: Here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this Blog I share my learning journey through machine learning and datascience."
  }
]