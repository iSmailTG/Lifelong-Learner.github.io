[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this Blog I share my learning journey through machine learning and datascience."
  },
  {
    "objectID": "posts/Fastai_ch4/Questionnaire.html",
    "href": "posts/Fastai_ch4/Questionnaire.html",
    "title": "Chapter 4: Questionnaire",
    "section": "",
    "text": "Q1:\nHow is a grayscale image represented on a computer? How about a color image? ___ * Grayscale image is way of turning an array/tensor to grayscale value on each pixel of that image, the values went from 0 to 256, the darker the pixel the closer to 256.\nQ2:\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n___\n* MNIST_SAMPLE contains two folders Train and Valid.\n* This method of structuring dataset help the community to compare the results between models by setting the same framework.\nQ3:\nExplain how the “pixel similarity” approach to classifying digits works.\n___\n* First we turn images in tensor, then we stack them together. * we take the mean value of each pixel for all images, this will give us an image that each pixel of it represent the mean of all datset. * Then we classify images by comparing the mean absolute error between that image and the ideal3/ideal7 and see which return low distance.\nQ4:\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n___\n\n# list with odd and even numbers\nlisst =  [1, 2, 3, 4, 5, 31, 17, 70]\n# using list-comprehension to select only odd numbers\ndouble_odd = [2*i for i in lisst if i%2==1]\ndouble_odd\n\n[2, 6, 10, 62, 34]\n\n\nQ5:\nWhat is a “rank-3 tensor”?\n___ * It’s a tensor with 3 dimensions, each can be represented as an array (array of array of array), it’s basically a cube.\nQ6:\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\n____\n* Rank represent the dimesion of the tensorm while shape tells how many elemen there’s in each dimension.\nQ7:\nWhat are RMSE and L1 norm?\n___\n* RMSE also called L2 stands dor Root Mean Square Error, while L1 is Least Absolute Error. * This functions are basically the same, we used them to measure distance.\nQ9:\nCreate a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.\n___\n\nimport torch\n\n\nr3_tens = torch.Tensor(list(range(1,10))).view(3,3)\nr3_tens\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\ndouble_tens= 2*r3_tens\ndouble_tens\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\ndouble_tens[1:, 1:]\n\ntensor([[10., 12.],\n        [16., 18.]])\n\n\nQ10:\nWhat is broadcasting?\n___\n* It refers to mathematical operation between different dimensions arrays and tensors.\nQ11:\nAre metrics generally calculated using the training set, or the validation set? Why?\n_____ * Metrics are calculated on validation set, so we have good measure of the model performance.\nQ12&13:\nWhat is SGD?\nWhy does SGD use mini-batches?\n____ * SGD or Stochastic Gradient Descent is an optimization function that help us to update weights and minimize the loss. * SGD updates gradients after each mini batch, otherwise it will take a lot of time if we decide to update the gradients after going through all the dataset, or the model won’t learn much if we decide to updates the gradients after each data point.\nQ14:\nWhat are the seven steps in SGD for machine learning?\n___\n* Initialize the parameters * Calculate the predictions * Calculate the loss * Calculate the gradients * Step the weights * Redo the whole process from step 2 * Stop\nQ15:\nHow do we initialize the weights in a model?\n___\n* Usually we initialize weights by picking random values\nQ16:\nWhat is “loss”?\n___\n* The loss is function that uses the model in order to optimizes it’s predictions\nQ17:\nWhat’s the gradients?\n___\n* The gradients are values that dictate how much should we change the weights in order to minimize the loss\nQ18:\nWhy can’t we always use a high learning rate?\n___\n* Picking a large learning rate will get the loss worse.\nQ19:\nDo you need to know how to calculate gradients yourself? ___\n* It’s important to understand the math behind each concept in Deep Learning, but we don’t need to do everything by ourself, we could use frameworks like pytorch and fastai.\nQ20:\nWhy can’t we use accuracy as a loss function?\n___\n* Loss function changes as the weights changes, but the accuracy only changes when the predictions change.\nQ21:\nDraw the sigmoid function. What is special about its shape?\n___\n* Sigmoid takes an input and return a number always between 0 and 1.\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\nQ22:\nWhat is the difference between a loss function and a metric?\n___\n* Loss is what model uses to optimize the predidictions, while metrics is what we (the ML practitioner) use to understand the performance of the model.\nQ23:\nWhat is the function to calculate new weights using a learning rate?\n___\n* The optimizer function\nQ24:\nWhat does the DataLoader class do?\n___ * Can be used to iterate through data, create batches, transform data..\nQ25:\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\n____\n\npredictions = linear_model(x)\nloss = mnist_loss(predictions, y)\nloss.backward()\nfor parameter in parameters:\n    parameter.data -= parameter.grad.data * learning_rate\n    parameter.grad = None\nQ26:\nCreate a function which, if passed two arguments [1,2,3,4] and ‘abcd’ , returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . What is special about that output data structure?\n___\n\nThis kind of datascructure is convinient for machine learning where we need to iterate through dataset.\n\n\ninputs = [1, 2, 3, 4]\nlabels = ['a', 'b', 'c', 'd']\ndef data_func(xb, yb):\n    return list(zip(xb, yb))\ndata_func(inputs, labels)\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nQ27:\nWhat does view in Pytorch do? ____\n* It changes the shape of the tensor without changing it content.\nQ28:\nWhat are the “bias” parameters in a neural network? Why do we need them?\n___\n* Bias allow us to all kind of multiplications without thinking if the inputs are zero in some cases.\nQ29:\nWhat does the @ operator do in Python?\n___\n* In python @ is used to do matrix multiplication.\nQ30:\nWhat does the backward method do?\n___ * Backward tells pytorch to calculate the change in the gradients at that point\nQ31:\nWhy do we have to zero the gradients?\n___ * zero gradients tell pytorch to not track the changes in gradients while we updates the weights.\nQ32:\nWhat information do we have to pass to Learner?\n___ * things we pass to Learner : - DataLoaders - architecture - loss_func - metrics\nQ33:\nShow Python or pseudocode for the basic steps of a training loop.\n____\n\n\n    def train_epoch(model, lr, params):\n        for xb,yb in dl:\n            calc_grad(xb, yb, model)\n            for p in params:\n                p.data -= p.grad*lr\n                p.grad.zero_()\n    for i in range(5):\n        train_epoch(model, lr, params)\n\n\nQ34:\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\n___ * ReLU stands from Rectified Linear Unit. This non-linear finction return any negative activations into zero. \nQ35:\nWhat is an “activation function”?\n___ * An activation function is a non-linear function that takes the outputs activations fron one layer of the neural network as inputs and output it after some kind of computation to another layer o NN."
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html",
    "href": "posts/Fastai_ch4/Ch4.html",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 27.5 MB/s \n     |████████████████████████████████| 5.3 MB 55.1 MB/s \n     |████████████████████████████████| 441 kB 70.9 MB/s \n     |████████████████████████████████| 1.3 MB 56.3 MB/s \n     |████████████████████████████████| 1.6 MB 57.3 MB/s \n     |████████████████████████████████| 115 kB 72.1 MB/s \n     |████████████████████████████████| 163 kB 71.5 MB/s \n     |████████████████████████████████| 212 kB 55.8 MB/s \n     |████████████████████████████████| 127 kB 75.4 MB/s \n     |████████████████████████████████| 115 kB 75.4 MB/s \n     |████████████████████████████████| 7.6 MB 56.7 MB/s \nMounted at /content/gdrive"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/Fastai_ch4/Ch4.html#pixels-the-foundations-of-computer-vision",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\nComputer are good with numbers, thats why in order to make them do computer vision tasks we need to turn images to series of numbers\nWe will use a small version of the famous dataset MNIST which contains only 2 digits 3 and 7.\nOur task here is create a Neural Network from scratch that can calssify 3 from 7.\n\n\n#download the dataset;\n#MNIST_simple is a small mnist that contains only 7s and 3s\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:01<00:00]\n    \n    \n\n\n\n#here is where the dataset is stored\npath\n\nPath('/root/.fastai/data/mnist_sample')\n\n\n\n# we can use ls() to investigate the dataset\n# we have 3 directories: train, valid, labels.csv\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\n\n#inside of train/valid folders, there's is 2 folders: 7, 3\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\n\n#let's have a look at what in those folders while we sorted them an put them into variables ('threes and sevens')\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\n\n\nthrees[-1], threes[22]\n\n(Path('/root/.fastai/data/mnist_sample/train/3/9991.png'),\n Path('/root/.fastai/data/mnist_sample/train/3/10210.png'))\n\n\n\n#let's open it \nimg_path = threes[1]\nimg = Image.open(img_path)\nimg\n\n\n\n\n\nIn a computer, everything is represented as a numbers.\nTo view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor.\n\n\n# here we use numpy array to represent that image \"img\" as array (matrix) \n# the img here is represented as pixels\n# the darker pixel are 0 or have values colser to 0, and the lighter pixels have higher values\narray(img)[4:10, 4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n#here we use Pytorch tensor which is the numpy array function of pytorch\n#they have the same code, and the behave basically the same (in most cases), \n#the main difference is tensors can be computed on GPu\na =tensor(img)[4:10, 4:10]\na\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\nWe ciuld convert a image into array/tensor, then represent it as pandas DataFrame by coloring each pixel using background_gradient(), the darker the pixel is the closer it is to the highest value 252, this the image is the closer to 0.\n\n\n# here we demonstrate the image and pixels values\n# values of each pixel varies between 0 and 255\nimg_t= tensor(img)\ndf = pd.DataFrame(img_t)\ndf.style.set_properties(**{'font-size':'4pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      5\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      29\n      150\n      195\n      254\n      255\n      254\n      176\n      193\n      150\n      96\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      48\n      166\n      224\n      253\n      253\n      234\n      196\n      253\n      253\n      253\n      253\n      233\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      0\n      0\n      0\n      93\n      244\n      249\n      253\n      187\n      46\n      10\n      8\n      4\n      10\n      194\n      253\n      253\n      233\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      107\n      253\n      253\n      230\n      48\n      0\n      0\n      0\n      0\n      0\n      192\n      253\n      253\n      156\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      3\n      20\n      20\n      15\n      0\n      0\n      0\n      0\n      0\n      43\n      224\n      253\n      245\n      74\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      249\n      253\n      245\n      126\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      11\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14\n      101\n      223\n      253\n      248\n      124\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      12\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      11\n      166\n      239\n      253\n      253\n      253\n      187\n      30\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      13\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      16\n      248\n      250\n      253\n      253\n      253\n      253\n      232\n      213\n      111\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      14\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      43\n      98\n      98\n      208\n      253\n      253\n      253\n      253\n      187\n      22\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      9\n      51\n      119\n      253\n      253\n      253\n      76\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      16\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      183\n      253\n      253\n      139\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      17\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      182\n      253\n      253\n      104\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      18\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      85\n      249\n      253\n      253\n      36\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      19\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      60\n      214\n      253\n      253\n      173\n      11\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      20\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      98\n      247\n      253\n      253\n      226\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      21\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      42\n      150\n      252\n      253\n      253\n      233\n      53\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      22\n      0\n      0\n      0\n      0\n      0\n      0\n      42\n      115\n      42\n      60\n      115\n      159\n      240\n      253\n      253\n      250\n      175\n      25\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      23\n      0\n      0\n      0\n      0\n      0\n      0\n      187\n      253\n      253\n      253\n      253\n      253\n      253\n      253\n      197\n      86\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      24\n      0\n      0\n      0\n      0\n      0\n      0\n      103\n      253\n      253\n      253\n      253\n      253\n      232\n      67\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      25\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      26\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      27\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#base-line-model",
    "href": "posts/Fastai_ch4/Ch4.html#base-line-model",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Base-Line Model",
    "text": "Base-Line Model\n\nIs always good to start with a base-line model then try to build something more comlex on top of it\nBase-line model help us build intuition and understand the prespectives of the problem\nConsum less time to build\n\n\nPixel Similarity\n\nCalculate the average values for each pixel location across all images for each digit\n\nThis will generate a blurry image of the target digit\n\nCompare the values for each pixel location in a new image to the average\n\n\n# store all images of 7s and 3s as a list of tensors\nsevens_tensors = [tensor(Image.open(o)) for o in sevens]\nthrees_tensors = [tensor(Image.open(o)) for o in threes]\nlen(sevens_tensors), len(threes_tensors)\n\n(6265, 6131)\n\n\n\nso now we have bunch of tensors, since they’re not image object anymore, we will use show_image of fast ai instead of PILimage\nRemember we can always use show_image?? to read the documentation/ the code\n\n\n# show a image from the tensor list\nshow_image(threes_tensors[330]);\n\n\n\n\n\nNow we need to compute the value of each pixel in respect to all images (for that digit)\nPut all images in a list of 3 dimension tensors, then stack them into a single tensor.\n\nstack images via pytorch function torch.stack\nand scale pixel values from the range [0,255] to [0,1]\n\n\n\nstacked_threes= torch.stack(threes_tensors).float()/255\nstacked_sevens= torch.stack(sevens_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\n.shape attribute tells us about the lenth of each axis\n\nin this case we can see we have 6131 images, each of size 28*28 pixel\n\nCalculate the mean values for each pixel location across all images\n\n\nmean3 = stacked_threes.mean(0)\nmean7 = stacked_sevens.mean(0)\n\n\nSow our ideal 3 and 7\n\n\nshow_image(mean3)\nshow_image(mean7)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7eff78f56f90>\n\n\n\n\n\n\n\n\n\n#pick a single 3, 7 to compare it with the ideal one\na_3 = stacked_threes[55]\na_7 = stacked_sevens[29]\n\n\nshow_image(a_3)\nshow_image(a_7)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7eff769b23d0>\n\n\n\n\n\n\n\n\n\nso now let’s say that we want to recognize if a_3 is a 3 or 7?\nto do that we can measure the distance between either of the two ideal 3 and 7**\n\nbecause just compunting the difference canot give us the right answear always since in some cases the difference will be negative!\n\n\nto avoide that we can take 2 approaches:\n* Take the mean of the absolute value of differences, this method called L1 norm or mean absolute difference .\n* Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring), this method called L2 norm or root mean squared error (RMSE)\n\n# Let's try both\ndis_3_abs = (a_3-mean3).abs().mean()\ndis_3_sqr = ((a_3-mean3)**2).mean().sqrt()\ndis_3_abs, dis_3_sqr\n\n(tensor(0.1280), tensor(0.2356))\n\n\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1832), tensor(0.3356))\n\n\n\nbased on those numbers the a_3 seems to be close to the ideal three than a_7to the ideal seven which seems to be right\nIn pytorch there’s a function that represent all of that for us:\n\ntorch.nn.functional, wich is recommended to be called as F(in fastai this recommendation is standarized!).\nl1_loss stand for l1 norm, and mse_loss stand for l2 norm\n\nmse_loss still need sqrt() to fully executed\n\n\n\n\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()\n\n(tensor(0.1832), tensor(0.3356))\n\n\n\n\nComputing metrics using Broadcasting\n\nIf we want to test the accuracy of a model we better measure it on validation set\nso let’s create a tensor of 3’s and 7’s of validation set directory, then calculate the accuracy of our “model” based on every tensor(image) in validation set\n\n\n#create tensors from image in validation set, then stack all image together\nvalid_ten_3 = torch.stack([tensor(Image.open(o)) \n                           for o in (path/'valid'/'3').ls()])\n#turn them into float and devide them by 255\nvalid_ten_3 = valid_ten_3.float()/255\nvalid_ten_7 = torch.stack([tensor(Image.open(o))\n                           for o in (path/'valid'/'7').ls()])\nvalid_ten_7 = valid_ten_7.float()/255\nvalid_ten_3.shape,valid_ten_7.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\n#create a function that will calculate the mean absolute error\ndef min_abser(a, b): return (a-b).abs().mean((-1,-2))\nmin_abser(valid_ten_3[345], mean3)\n\ntensor(0.1123)\n\n\n\nBut this is only the absolut error with one image. Now we need to calculate this distance between the ideal 3/7 with all image in validation set in order to evaluate our model.\nThe easy method that will pop up in our mind in order to do that is to loop over all images in validation set and use the function min_abser() to calulate the difference over all of those images\nThe better way, is to use a method called Broadcasting which is a way of extending a tensor to match to others in order to do calulations\n\n\n#example of broadcasting:\ntensor([2, 3, 4])+tensor(-2)\n\ntensor([0, 1, 2])\n\n\n\npytorch execute a calculation between 2 tensors with different ranks(dimension), we will take adventage of that method in order to do same with our case\n\n\n# shape of mean3/7 and the shape of validation set tensor\nmean3.shape, valid_ten_3.shape\n\n(torch.Size([28, 28]), torch.Size([1010, 28, 28]))\n\n\n\nThere’s 1010 image and we want to compare that ideal image mean3 against\nis_3() decide whether its 3 or 7 by copmuting which output of min_abser() is smaller\n\n\ndef is_3(x): return min_abser(x,mean3) < min_abser(x,mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\nNote that when we convert the Boolean response to a float, we get 1.0 for True and 0.0 for False. Thanks to broadcasting, we can also test it on the full validation set of 3s:\n\n\nis_3(valid_ten_3)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\n\nNow we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:\n\n\naccuracy_3s =      is_3(valid_ten_3).float() .mean()\naccuracy_7s = (1 - is_3(valid_ten_7).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\n\nWe’re getting +90% accuracy just by doing base-line model!!\nThe base-Line model is good for understanding the problem and building the intuition, but we didn’t build a deep learning model yet.\n\nwe did not add the Learning fase to our model, which is a crucial part according to Arthur Samuel definition\n\nIn other words, this model cannot be updated and improved by learning\n\nwe can’t improve pixel similarity approach by modifying set of parameters, because we don’t have one! and that’s why we need SGD"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#stochastic-gradient-descent",
    "href": "posts/Fastai_ch4/Ch4.html#stochastic-gradient-descent",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nInstead of trying to find similarity between an image and the “ideal image” we could add set of weights to each pixel, such as the heighest are associated with darker pixel and lowest are more likely to be white.\n\nfor example the pixel bottom right are white in 7s, so we will give them low weights\n\nThis can be represented as a function and set of weight values for each possible digit:\ndef pr_eight(x,w): return (x*w).sum()\n\nx is the image we’re predicting which will be represented as vector\nw is the weights of the image, also vector\n\nThe idea to find a method by which we update the weights, such as it can help us to make to predict better by a bit, then repeat those steps many times til we get the best prediction we can get.\n\nfind the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not.\n\nHere are the steps that we are going to require, to turn this function into a machine learning classifier:\n\n\nInitialize the weights.\n\n\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\n\n\nBased on these predictions, calculate how good the model is (its loss).\n\n\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\n\n\nStep (that is, change) all the weights based on that calculation.\n\n\nGo back to the step 2, and repeat the process.\n\n\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer). \n\n\n\n\nApllying SGD on a simple case\n\nBefore applying these steps to our image classification problem, let’s illustrate what they look like in a simpler case.\nFirst we will define a very simple function f, the quadratic—let’s pretend that this is our loss function, and x is a weight parameter of the function:\n\n\ndef f(x): return x**2\n\n\nplot_function(f, 'x', 'x**2')\n\n\n\n\nLet’s pick a rondom value for the paramter, and calculating the loss\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\n\n\n\nnow we will increase/decrease the parameter value by just a bit(0.5)and see what will happen:\n\n\nplot_function(f,'x','x**2')\nplt.scatter(-1.5, f(-1.5), color='red' );\nplt.scatter(-1,f(-1), color='blue');\n\n\n\n\n\nIt seem that our loss get better(remember, the whole objective of this process is to minimize the loss to 0), we better keep increase the paramter but this time by (0.3), then(0.3):\n\n\nplot_function(f,'x','x**2')\nplt.scatter(-1.5, f(-1.5), color='red' );\nplt.scatter(-1,f(-1), color='blue');\nplt.scatter(-0.7,f(-0.7), color='green');\nplt.scatter(-0.5, f(-0.5), color='orange');\n\n\n\n\n\nThe main idea here is to adjust the paramters in a way that causes a minimal loss \nWe can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:\n\n\n\n\nScreenshot 2022-09-13 at 14-25-28 04_mnist_basics - Jupyter Notebook.png\n\n\n\n\nCalculating Gradients\n\nIt’s obvious now that in order to optimize the loss function we need to update the weights.\n\nto do that we need the help of calculus, it will allow us to update the weights in the direction that optimize the loss\n\n\n\n\nCalculating derivative on Pytorch\n\n#let's assume this variable\ndef f(x): return x**2\nxt = tensor(3.).requires_grad_()\n\n\nhere we create a tensor at value 3 the we call the method requires_grad_ which will calculate the gradients in respect to that variable at that value.\n\n\n# add the function f(x)=x**2 as a Y\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=<PowBackward0>)\n\n\n\n# now we calculate the gradients using the backward() \nyt.backward()\n\n\n#now we can views the gradients by checking the `grad` attribute of the tensor:\nxt.grad\n\ntensor(6.)\n\n\n\n#now we will repeat the same steps but now with a vector:\nxt = tensor([3.,4.,10.]).requires_grad_()\n\n\ndef f(x): return (x**2).sum()\n\n\n#let's check our function\nf(xt)\n\ntensor(125., grad_fn=<SumBackward0>)\n\n\n\nyt = f(xt)\n\n\n#calculate the gradients\nyt.backward()\n\n\n#as expected 2xt\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\n\n#let's create another example\nx= tensor(3.).requires_grad_()\nw= tensor(4.).requires_grad_()\nb= tensor(5.).requires_grad_()\ny = x * w + b\ny\n\ntensor(17., grad_fn=<AddBackward0>)\n\n\n\ny.backward()\n\n\nprint('d(y)/dx= ', x.grad)\nprint('d(y)/dw= ', w.grad)\nprint('d(y)/bx= ', b.grad)\n\nd(y)/dx=  tensor(4.)\nd(y)/dw=  tensor(3.)\nd(y)/bx=  tensor(1.)\n\n\n\nNow we have all the ingredients to apply what we have learned on a real problem.\n\nthe goal here is to apply these 7 steps we saw in order to optimize the weights which will effect the loss, which also effect the accuracy of the model\n\none last thing before we do that, we will talk about Learnin rate\nGradient descent allow us to correct our weights by taking steps toward the optimal value of these weights, but it didn’t tell us how big or small these steps are, thats why we have to initilize a learning rate.\nLearning rate is a value, by which our gradient calculate new weights in order to get better loss.\nIn general the learning rate is some randome value we chase between 0.1 and 0.001\nOnce we have picked a learning rate we then adjust the parameters using this simple function:\n\nw-=gradient(w)*lr\n\n\nAn End-to-End SGD Example\n\nSuppose we want to predict the speed of a Roller Coaster over a hump.\nWe want to build a model that predict how the speed change over time\n\nwe measure the speed of the 20 seconds manualy/ every second:\n\n\n\ntime = torch.arange(20).float();\ntime\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n\n\n\nIt might look something like this:\n\n\n# we add some noise to the data since this is the way we usualy find data in real world\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\n\n\n\nThe goal here is to find a function that matches our observastions using SGD\nWe chose here a quadratic function!.\n\nwe need to distinguish clearly between the input of the function t (the that’s the product of our observation) and it’s parameters params\n\n\n\n#let's just assume this  quadratic function!!!\n\ndef f(t, params):\n    a, b, c= params\n    return a*(t**2)+(b*t)+c\n\n\nThe we need to set a loss function that will tell us how good or bad our prediction of the parameters a, b, c\n\n\ndef mse(preds, targets): return((preds - targets)**2).mean().sqrt()\n\n\n\nStep 1: Initialize the parameters\n\nfirst we need to initialize the parametrs by telling pytorch that we want to track its gradients\n\n\nparams= torch.randn(3).requires_grad_()\nparams\n\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n\n#??\norig_params = params.clone()\n\n\n\nStep 2: Calculate the predictions\n\nNext we calculate the prediction\n\n\npreds = f(time, params)\npreds\n\ntensor([ 1.3525e+00, -1.6391e-01, -3.2121e+00, -7.7919e+00, -1.3903e+01, -2.1547e+01, -3.0721e+01, -4.1428e+01, -5.3666e+01, -6.7436e+01, -8.2738e+01, -9.9571e+01, -1.1794e+02, -1.3783e+02,\n        -1.5926e+02, -1.8222e+02, -2.0671e+02, -2.3274e+02, -2.6029e+02, -2.8938e+02], grad_fn=<AddBackward0>)\n\n\nThe pred here are the function we create earlier. * f(t, params): am bm c= params return a*(t**2)+(b*t)+c - where the t==time, and params are the initialized values we took in the step before, so the f will be calculated by taking those values as a part of it\n\n# plot the actual calues and our prediction \ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n\nshow_preds(preds)\n\n\n\n\n\nThe blue dot represent the actal value\nThe red dots represent what our model\n\npredictions are really bad, but we need to remember that these prediction are based on Random Values\n\nSo our job here is to update these values to get better predictions\n\n\n\nStep 3: Calculate the loss\n\nWe calculate now the loss of our prediction\nRemember:\n\ndef mse(preds, targets): return((preds - targets)**2).mean().sqrt()\n\nloss= mse(preds, speed)\nloss\n\ntensor(160.6979, grad_fn=<SqrtBackward0>)\n\n\n\nOur goal is to improve this loss function by minimizing it, to be as close as possible to the target(speed)\n\n\n\nStep 4: Calculate the gradients\n\nIn order to minimize the loss function we use the gradients to aproximate how the parameters can be changed to achieve our goal\n\n\nloss.backward()\nparams.grad\n\ntensor([-165.5151,  -10.6402,   -0.7900])\n\n\n\nparams.grad\n\ntensor([-165.5151,  -10.6402,   -0.7900])\n\n\n\nparams\n\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n\n\nStep 5: Step the weights\n\nWe need to updated the weights(paramaters) we just calculated\n\nfirst we need to fix the learning rate\nthen we need to call .data on the parameters and parameters.grad, it’s like telling pytorch to not track the change in the gradients at this point.\n\n\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n\nLet’s see if the loss has improved:\n\n\npreds = f(time,params)\nmse(preds, speed)\n\ntensor(160.4228, grad_fn=<SqrtBackward0>)\n\n\n\nwhat’s about the plot\n\n\nshow_preds(preds)\n\n\n\n\n\nNow let’s execute all steps in one function\n\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n\nLet’s execute this steps for 100s of times\n\n\nfor i in range (1200): apply_step(params)\n\n160.42279052734375\n160.14772033691406\n159.87269592285156\n159.59768676757812\n159.3227081298828\n159.04774475097656\n158.7728271484375\n158.4979248046875\n158.22305297851562\n157.9481964111328\n157.67337036132812\n157.39857482910156\n157.12380981445312\n156.84906005859375\n156.5743408203125\n156.29966735839844\n156.02499389648438\n155.75035095214844\n155.4757537841797\n155.20118713378906\n154.92662048339844\n154.65211486816406\n154.37762451171875\n154.1031494140625\n153.82872009277344\n153.55430603027344\n153.27992248535156\n153.0055694580078\n152.73126220703125\n152.4569549560547\n152.18270874023438\n151.90847778320312\n151.63426208496094\n151.36009216308594\n151.08595275878906\n150.81185913085938\n150.5377655029297\n150.26370239257812\n149.9897003173828\n149.71571350097656\n149.44175720214844\n149.16783142089844\n148.89393615722656\n148.6200714111328\n148.3462371826172\n148.0724334716797\n147.79867553710938\n147.52493286132812\n147.25123596191406\n146.97756958007812\n146.7039337158203\n146.43032836914062\n146.15676879882812\n145.88323974609375\n145.60971069335938\n145.3362579345703\n145.06283569335938\n144.78941345214844\n144.51605224609375\n144.24273681640625\n143.96942138671875\n143.6961669921875\n143.4229278564453\n143.14974975585938\n142.87661743164062\n142.6034698486328\n142.3303985595703\n142.05734252929688\n141.78433227539062\n141.5113525390625\n141.23841857910156\n140.96554565429688\n140.69265747070312\n140.41983032226562\n140.14703369140625\n139.87429809570312\n139.60157775878906\n139.3289031982422\n139.05625915527344\n138.78367614746094\n138.51112365722656\n138.2386016845703\n137.96612548828125\n137.69369506835938\n137.42127990722656\n137.14894104003906\n136.87660217285156\n136.60433959960938\n136.33209228515625\n136.0598907470703\n135.7877197265625\n135.515625\n135.2435302734375\n134.9715118408203\n134.69952392578125\n134.42759704589844\n134.1556854248047\n133.88381958007812\n133.61199951171875\n133.3402099609375\n133.06849670410156\n132.79681396484375\n132.52517700195312\n132.25357055664062\n131.98204040527344\n131.71051025390625\n131.43905639648438\n131.16763305664062\n130.89627075195312\n130.62493896484375\n130.35366821289062\n130.08242797851562\n129.81126403808594\n129.54013061523438\n129.26902770996094\n128.99798583984375\n128.72698974609375\n128.4560546875\n128.18515014648438\n127.91431427001953\n127.64350891113281\n127.37277221679688\n127.1020736694336\n126.83142852783203\n126.56082916259766\n126.29029083251953\n126.01979064941406\n125.74934387207031\n125.47895812988281\n125.2086410522461\n124.93834686279297\n124.6681137084961\n124.39794921875\n124.12782287597656\n123.85773468017578\n123.58772277832031\n123.31776428222656\n123.04785919189453\n122.77799987792969\n122.5082015991211\n122.23847198486328\n121.96878814697266\n121.69915771484375\n121.42959594726562\n121.16007995605469\n120.89063262939453\n120.6212387084961\n120.35188293457031\n120.08260345458984\n119.81338500976562\n119.54422760009766\n119.27513122558594\n119.00609588623047\n118.73711395263672\n118.46820831298828\n118.19935607910156\n117.93054962158203\n117.66181182861328\n117.39315795898438\n117.12455749511719\n116.85600280761719\n116.5875473022461\n116.31912994384766\n116.05079650878906\n115.78250122070312\n115.51428985595703\n115.24613952636719\n114.97806549072266\n114.71004486083984\n114.44210052490234\n114.17422485351562\n113.90641784667969\n113.63868713378906\n113.37100982666016\n113.10340118408203\n112.83587646484375\n112.56842041015625\n112.30104064941406\n112.03372192382812\n111.7664794921875\n111.49931335449219\n111.23222351074219\n110.96520233154297\n110.6982421875\n110.43138122558594\n110.16458129882812\n109.89786529541016\n109.63121795654297\n109.36465454101562\n109.09815979003906\n108.83175659179688\n108.56542205810547\n108.2991714477539\n108.03300476074219\n107.76690673828125\n107.50091552734375\n107.23497772216797\n106.96913146972656\n106.70336151123047\n106.43769073486328\n106.17208862304688\n105.90658569335938\n105.64115142822266\n105.37582397460938\n105.1105728149414\n104.84541320800781\n104.58033752441406\n104.31534576416016\n104.05045318603516\n103.78563690185547\n103.52091979980469\n103.25628662109375\n102.99176025390625\n102.72731018066406\n102.46295928955078\n102.1987075805664\n101.9345474243164\n101.67047119140625\n101.40650177001953\n101.14262390136719\n100.87885284423828\n100.61516571044922\n100.35159301757812\n100.0881118774414\n99.8247299194336\n99.56144714355469\n99.29827117919922\n99.03519439697266\n98.772216796875\n98.50934600830078\n98.24658966064453\n97.98394012451172\n97.72138977050781\n97.45895385742188\n97.19661712646484\n96.93439483642578\n96.67227172851562\n96.41026306152344\n96.14837646484375\n95.88658905029297\n95.62493133544922\n95.3633804321289\n95.1019287109375\n94.84060668945312\n94.57940673828125\n94.31830596923828\n94.0573501586914\n93.79650115966797\n93.5357666015625\n93.27516174316406\n93.0146713256836\n92.75431823730469\n92.49407958984375\n92.23397827148438\n91.97398376464844\n91.7141342163086\n91.45439910888672\n91.1948013305664\n90.93533325195312\n90.67599487304688\n90.41679382324219\n90.15772247314453\n89.89878845214844\n89.63999938964844\n89.3813247680664\n89.122802734375\n88.86441802978516\n88.60618591308594\n88.34807586669922\n88.09011840820312\n87.8322982788086\n87.57463073730469\n87.31710815429688\n87.05973052978516\n86.80250549316406\n86.54542541503906\n86.28850555419922\n86.03172302246094\n85.77511596679688\n85.51864624023438\n85.26234436035156\n85.00618743896484\n84.75019836425781\n84.49436950683594\n84.23870086669922\n83.98320007324219\n83.72785949707031\n83.4726791381836\n83.21766662597656\n82.96282958984375\n82.70816802978516\n82.45367431640625\n82.19934844970703\n81.94519805908203\n81.69123077392578\n81.43744659423828\n81.18382263183594\n80.93038940429688\n80.67713928222656\n80.42406463623047\n80.17118072509766\n79.91848754882812\n79.66597747802734\n79.41366577148438\n79.16154479980469\n78.90961456298828\n78.65788269042969\n78.4063491821289\n78.15501403808594\n77.90387725830078\n77.65293884277344\n77.4022216796875\n77.15169525146484\n76.90137481689453\n76.65127563476562\n76.40138244628906\n76.15170288085938\n75.9022445678711\n75.65299224853516\n75.40396881103516\n75.15515899658203\n74.90658569335938\n74.6582260131836\n74.41009521484375\n74.16220092773438\n73.9145278930664\n73.6670913696289\n73.4198989868164\n73.17293548583984\n72.92621612548828\n72.67974853515625\n72.43351745605469\n72.18753051757812\n71.94180297851562\n71.69631958007812\n71.45108795166016\n71.20611572265625\n70.9614028930664\n70.71694946289062\n70.47276306152344\n70.22883605957031\n69.98519134521484\n69.74180603027344\n69.49870300292969\n69.25586700439453\n69.01331329345703\n68.77104187011719\n68.52906036376953\n68.28734588623047\n68.04594421386719\n67.80481719970703\n67.56399536132812\n67.3234634399414\n67.08323669433594\n66.84332275390625\n66.60370635986328\n66.36438751220703\n66.12539672851562\n65.88671875\n65.64836120605469\n65.41032409667969\n65.17259979248047\n64.93521881103516\n64.69815063476562\n64.46143341064453\n64.22505187988281\n63.98899841308594\n63.7532958984375\n63.5179328918457\n63.282928466796875\n63.04827117919922\n62.81398010253906\n62.580047607421875\n62.34648132324219\n62.11327362060547\n61.88043975830078\n61.64798355102539\n61.41590118408203\n61.184200286865234\n60.95288848876953\n60.72196960449219\n60.49143981933594\n60.26130294799805\n60.031578063964844\n59.80224609375\n59.573326110839844\n59.34482192993164\n59.116729736328125\n58.889068603515625\n58.66182327270508\n58.43500518798828\n58.2086296081543\n57.98268127441406\n57.757179260253906\n57.532127380371094\n57.307525634765625\n57.0833740234375\n56.85968780517578\n56.6364631652832\n56.413700103759766\n56.19141387939453\n55.969608306884766\n55.74828338623047\n55.52744674682617\n55.30709457397461\n55.087249755859375\n54.867897033691406\n54.649051666259766\n54.430721282958984\n54.21290969848633\n53.995609283447266\n53.77884292602539\n53.56260299682617\n53.34690475463867\n53.131744384765625\n52.91713333129883\n52.703067779541016\n52.489566802978516\n52.2766227722168\n52.06424331665039\n51.852447509765625\n51.641231536865234\n51.43058776855469\n51.220542907714844\n51.01108932495117\n50.802242279052734\n50.593997955322266\n50.38636779785156\n50.179351806640625\n49.972965240478516\n49.7672119140625\n49.56209182739258\n49.35761260986328\n49.153778076171875\n48.95060348510742\n48.74808120727539\n48.54623031616211\n48.345054626464844\n48.14455032348633\n47.944732666015625\n47.745609283447266\n47.547176361083984\n47.34945297241211\n47.152435302734375\n46.95613479614258\n46.76055908203125\n46.565704345703125\n46.3715934753418\n46.17822265625\n45.985599517822266\n45.79372787475586\n45.60261535644531\n45.41227340698242\n45.22270965576172\n45.03392028808594\n44.84592056274414\n44.65871810913086\n44.47231674194336\n44.286712646484375\n44.1019287109375\n43.91796875\n43.734832763671875\n43.552528381347656\n43.37106704711914\n43.190452575683594\n43.01069259643555\n42.83179473876953\n42.65375900268555\n42.476600646972656\n42.30031967163086\n42.12492752075195\n41.9504280090332\n41.77682876586914\n41.6041374206543\n41.43235778808594\n41.26150131225586\n41.09156799316406\n40.92256164550781\n40.75450134277344\n40.5873908996582\n40.42121887207031\n40.256011962890625\n40.091773986816406\n39.92850112915039\n39.766204833984375\n39.60489273071289\n39.4445686340332\n39.285240173339844\n39.12691116333008\n38.96958923339844\n38.81327438354492\n38.65798568725586\n38.50371551513672\n38.35047149658203\n38.19826889038086\n38.04710006713867\n37.896976470947266\n37.74790954589844\n37.59988784790039\n37.45293426513672\n37.30704116821289\n37.1622200012207\n37.01847457885742\n36.87580490112305\n36.73421859741211\n36.593719482421875\n36.454307556152344\n36.31599807739258\n36.17878341674805\n36.04267501831055\n35.90766906738281\n35.77377700805664\n35.640995025634766\n35.509334564208984\n35.378787994384766\n35.24936294555664\n35.121063232421875\n34.993892669677734\n34.86784744262695\n34.74293899536133\n34.619163513183594\n34.49652099609375\n34.37501907348633\n34.2546501159668\n34.13542556762695\n34.01734161376953\n33.90039825439453\n33.78459548950195\n33.66993713378906\n33.55642318725586\n33.44404983520508\n33.332820892333984\n33.22273635864258\n33.113792419433594\n33.00598907470703\n32.89932632446289\n32.79380416870117\n32.68942642211914\n32.58617401123047\n32.484066009521484\n32.38309097290039\n32.28324508666992\n32.18452835083008\n32.086936950683594\n31.99047088623047\n31.89512825012207\n31.800899505615234\n31.70779037475586\n31.61579132080078\n31.5248966217041\n31.435110092163086\n31.346420288085938\n31.25882911682129\n31.17232894897461\n31.086910247802734\n31.002580642700195\n30.919322967529297\n30.83713722229004\n30.756017684936523\n30.675962448120117\n30.596956253051758\n30.51900291442871\n30.442092895507812\n30.36622428894043\n30.291379928588867\n30.217559814453125\n30.14476203918457\n30.07297134399414\n30.00218963623047\n29.93239974975586\n29.863605499267578\n29.79578971862793\n29.72895050048828\n29.6630802154541\n29.598169326782227\n29.534212112426758\n29.4711971282959\n29.409116744995117\n29.347970962524414\n29.287738800048828\n29.228425979614258\n29.170013427734375\n29.11249351501465\n29.055862426757812\n29.000110626220703\n28.945226669311523\n28.891206741333008\n28.83803367614746\n28.78570556640625\n28.734214782714844\n28.683551788330078\n28.63370132446289\n28.584657669067383\n28.53641700744629\n28.488964080810547\n28.442289352416992\n28.396390914916992\n28.35125160217285\n28.306867599487305\n28.263225555419922\n28.220321655273438\n28.178142547607422\n28.136682510375977\n28.095928192138672\n28.055875778198242\n28.016508102416992\n27.977825164794922\n27.9398136138916\n27.902463912963867\n27.865768432617188\n27.829721450805664\n27.794307708740234\n27.759521484375\n27.725353240966797\n27.691795349121094\n27.65884017944336\n27.626476287841797\n27.594696044921875\n27.56348991394043\n27.532852172851562\n27.50277328491211\n27.473243713378906\n27.444255828857422\n27.415802001953125\n27.38787269592285\n27.360462188720703\n27.333559036254883\n27.30715560913086\n27.281248092651367\n27.25582504272461\n27.230878829956055\n27.206403732299805\n27.182390213012695\n27.158830642700195\n27.135719299316406\n27.113048553466797\n27.090810775756836\n27.06899642944336\n27.047603607177734\n27.02661895751953\n27.006038665771484\n26.985857009887695\n26.966068267822266\n26.946659088134766\n26.927631378173828\n26.908971786499023\n26.890676498413086\n26.872737884521484\n26.855154037475586\n26.837913513183594\n26.821012496948242\n26.804445266723633\n26.788204193115234\n26.77228546142578\n26.756681442260742\n26.74138832092285\n26.726398468017578\n26.711708068847656\n26.697311401367188\n26.683198928833008\n26.66937255859375\n26.65582275390625\n26.642541885375977\n26.629533767700195\n26.616785049438477\n26.604291915893555\n26.592052459716797\n26.580062866210938\n26.568313598632812\n26.55680274963379\n26.54552459716797\n26.53447723388672\n26.52365493774414\n26.513051986694336\n26.502666473388672\n26.492490768432617\n26.482528686523438\n26.472766876220703\n26.46320343017578\n26.453842163085938\n26.44466781616211\n26.43568229675293\n26.4268856048584\n26.41826820373535\n26.409826278686523\n26.401559829711914\n26.39346694946289\n26.38553810119629\n26.377775192260742\n26.37017250061035\n26.362730026245117\n26.355438232421875\n26.348299026489258\n26.341310501098633\n26.33446502685547\n26.327762603759766\n26.32120132446289\n26.314775466918945\n26.30848503112793\n26.302322387695312\n26.29629135131836\n26.29038429260254\n26.28460121154785\n26.27894401550293\n26.27340316772461\n26.267974853515625\n26.262662887573242\n26.257463455200195\n26.252370834350586\n26.247386932373047\n26.242507934570312\n26.23773193359375\n26.233055114746094\n26.228477478027344\n26.223997116088867\n26.219608306884766\n26.215314865112305\n26.211111068725586\n26.20699691772461\n26.20296859741211\n26.19902801513672\n26.195167541503906\n26.191389083862305\n26.18769073486328\n26.184070587158203\n26.18052864074707\n26.177061080932617\n26.173667907714844\n26.17034339904785\n26.16709327697754\n26.163909912109375\n26.16079330444336\n26.157745361328125\n26.154760360717773\n26.151840209960938\n26.14898109436035\n26.146183013916016\n26.14344596862793\n26.140766143798828\n26.13814353942871\n26.135576248168945\n26.133060455322266\n26.130603790283203\n26.128196716308594\n26.125839233398438\n26.123537063598633\n26.121280670166016\n26.11907196044922\n26.11690902709961\n26.114797592163086\n26.11272430419922\n26.110698699951172\n26.108718872070312\n26.10677719116211\n26.10487937927246\n26.103023529052734\n26.10120391845703\n26.09942626953125\n26.09768295288086\n26.095979690551758\n26.094310760498047\n26.092681884765625\n26.091081619262695\n26.089519500732422\n26.08799171447754\n26.08649253845215\n26.085031509399414\n26.083599090576172\n26.082195281982422\n26.080820083618164\n26.079477310180664\n26.07816505432129\n26.07687759399414\n26.075618743896484\n26.074386596679688\n26.073179244995117\n26.072004318237305\n26.070846557617188\n26.069719314575195\n26.068614959716797\n26.06753158569336\n26.06647300720215\n26.06543731689453\n26.064424514770508\n26.063432693481445\n26.062463760375977\n26.061511993408203\n26.060583114624023\n26.059673309326172\n26.05878257751465\n26.057912826538086\n26.05706214904785\n26.056228637695312\n26.055410385131836\n26.05461311340332\n26.0538330078125\n26.053068161010742\n26.05232048034668\n26.05158805847168\n26.050870895385742\n26.0501708984375\n26.049484252929688\n26.04881477355957\n26.04815673828125\n26.04751205444336\n26.046886444091797\n26.046268463134766\n26.04566764831543\n26.04507827758789\n26.04450035095215\n26.0439395904541\n26.043384552001953\n26.0428466796875\n26.04231834411621\n26.04180145263672\n26.04129409790039\n26.040800094604492\n26.040315628051758\n26.03984260559082\n26.039377212524414\n26.038921356201172\n26.038476943969727\n26.038042068481445\n26.037616729736328\n26.037200927734375\n26.036794662475586\n26.036394119262695\n26.0360050201416\n26.03562355041504\n26.03525161743164\n26.03488540649414\n26.034526824951172\n26.034177780151367\n26.03383445739746\n26.033498764038086\n26.033170700073242\n26.032852172851562\n26.032535552978516\n26.032228469848633\n26.03192901611328\n26.031635284423828\n26.03134536743164\n26.031064987182617\n26.03078842163086\n26.030517578125\n26.030254364013672\n26.02999496459961\n26.02974510192871\n26.029499053955078\n26.029254913330078\n26.029020309448242\n26.028785705566406\n26.0285587310791\n26.028337478637695\n26.028120040893555\n26.027908325195312\n26.027700424194336\n26.027498245239258\n26.02729606628418\n26.027101516723633\n26.02690887451172\n26.026721954345703\n26.026540756225586\n26.0263614654541\n26.026187896728516\n26.026016235351562\n26.025848388671875\n26.025684356689453\n26.025524139404297\n26.025365829467773\n26.02521324157715\n26.025062561035156\n26.02491569519043\n26.024770736694336\n26.024627685546875\n26.024494171142578\n26.024356842041016\n26.02422523498535\n26.02409553527832\n26.023969650268555\n26.023847579956055\n26.023725509643555\n26.023605346679688\n26.023488998413086\n26.023374557495117\n26.023263931274414\n26.023155212402344\n26.023048400878906\n26.0229434967041\n26.022842407226562\n26.022741317749023\n26.02264404296875\n26.022546768188477\n26.02245330810547\n26.022363662719727\n26.02227210998535\n26.022186279296875\n26.022096633911133\n26.022016525268555\n26.02193260192871\n26.021852493286133\n26.021772384643555\n26.02169418334961\n26.021617889404297\n26.02154541015625\n26.021472930908203\n26.02140235900879\n26.021331787109375\n26.021265029907227\n26.021198272705078\n26.021133422851562\n26.02107048034668\n26.021007537841797\n26.020946502685547\n26.02088737487793\n26.020828247070312\n26.020771026611328\n26.020713806152344\n26.020658493041992\n26.020605087280273\n26.020551681518555\n26.0205020904541\n26.020450592041016\n26.020402908325195\n26.020353317260742\n26.020305633544922\n26.0202579498291\n26.020214080810547\n26.02016830444336\n26.020126342773438\n26.020084381103516\n26.020042419433594\n26.020000457763672\n26.019960403442383\n26.019920349121094\n26.019882202148438\n26.019845962524414\n26.019807815551758\n26.019771575927734\n26.019737243652344\n26.01970100402832\n26.01966667175293\n26.019634246826172\n26.019601821899414\n26.019569396972656\n26.01953887939453\n26.019508361816406\n26.01947784423828\n26.019451141357422\n26.019418716430664\n26.019393920898438\n26.019365310668945\n26.019336700439453\n26.019309997558594\n26.019285202026367\n26.019258499145508\n26.019235610961914\n26.019208908081055\n26.019187927246094\n26.019163131713867\n26.019142150878906\n26.01911735534668\n26.01909637451172\n26.019075393676758\n26.019054412841797\n26.019033432006836\n26.019012451171875\n26.018991470336914\n26.018972396850586\n26.018953323364258\n26.01893424987793\n26.0189151763916\n26.018898010253906\n26.01888084411621\n26.018861770629883\n26.01884651184082\n26.018829345703125\n26.018814086914062\n26.018798828125\n26.018781661987305\n26.01876449584961\n26.018753051757812\n26.018735885620117\n26.018722534179688\n26.018707275390625\n26.018693923950195\n26.018678665161133\n26.018667221069336\n26.018651962280273\n26.018638610839844\n26.018625259399414\n26.018613815307617\n26.01860237121582\n26.018590927124023\n26.018579483032227\n26.018566131591797\n26.0185546875\n26.018545150756836\n26.01853370666504\n26.018524169921875\n26.018510818481445\n26.01850128173828\n26.018491744995117\n26.018484115600586\n26.018470764160156\n26.018461227416992\n26.018451690673828\n26.018442153930664\n26.018434524536133\n26.018423080444336\n26.018417358398438\n26.018409729003906\n26.018400192260742\n26.018390655517578\n26.018383026123047\n26.018375396728516\n26.018367767333984\n26.018360137939453\n26.018352508544922\n26.01834487915039\n26.01833724975586\n26.018329620361328\n26.018321990966797\n26.018314361572266\n26.018308639526367\n26.01830291748047\n26.01829719543457\n26.018287658691406\n26.01828384399414\n26.01827621459961\n26.01827049255371\n26.018264770507812\n26.018259048461914\n26.018251419067383\n26.018245697021484\n26.018239974975586\n26.018234252929688\n26.01822853088379\n26.01822280883789\n26.018217086791992\n26.01820945739746\n26.018207550048828\n26.018199920654297\n26.01819610595703\n26.0181884765625\n26.018184661865234\n26.0181827545166\n26.018177032470703\n26.018171310424805\n26.01816749572754\n26.01816177368164\n26.018157958984375\n26.01815414428711\n26.01814842224121\n26.018144607543945\n26.018138885498047\n26.01813507080078\n26.018131256103516\n26.01812744140625\n26.018123626708984\n26.01811981201172\n26.018115997314453\n26.018110275268555\n26.018108367919922\n26.018102645874023\n26.01810073852539\n26.018096923828125\n26.018091201782227\n26.01808738708496\n26.018085479736328\n26.018081665039062\n26.018077850341797\n26.018075942993164\n26.018070220947266\n26.01806640625\n26.018062591552734\n26.018062591552734\n26.018056869506836\n26.018054962158203\n26.018049240112305\n26.018049240112305\n26.01804542541504\n26.018041610717773\n26.018037796020508\n26.018035888671875\n26.018033981323242\n26.018030166625977\n26.01802635192871\n26.018022537231445\n26.018020629882812\n26.01801872253418\n26.018014907836914\n26.01801300048828\n26.01801109313965\n26.01800537109375\n26.018003463745117\n26.018001556396484\n26.01799774169922\n26.01799774169922\n26.017993927001953\n26.017990112304688\n26.017988204956055\n26.01798439025879\n26.017982482910156\n26.017980575561523\n26.01797866821289\n26.017974853515625\n26.017972946166992\n26.017969131469727\n26.017967224121094\n26.01796531677246\n26.017963409423828\n26.017959594726562\n26.017959594726562\n26.017955780029297\n26.017953872680664\n26.0179500579834\n26.017948150634766\n26.0179443359375\n26.0179443359375\n26.017942428588867\n26.017940521240234\n26.01793670654297\n26.01793670654297\n26.017932891845703\n26.017929077148438\n26.017929077148438\n26.017927169799805\n26.01792335510254\n26.01792335510254\n26.017919540405273\n26.01791763305664\n26.017913818359375\n26.017911911010742\n26.017911911010742\n26.017908096313477\n26.017906188964844\n26.017902374267578\n26.017902374267578\n26.017898559570312\n26.017898559570312\n26.017894744873047\n26.017892837524414\n26.017892837524414\n26.01789093017578\n26.01788902282715\n26.01788330078125\n26.01788330078125\n26.017881393432617\n26.017879486083984\n26.01787757873535\n26.01787567138672\n26.01787567138672\n26.01786994934082\n26.01786994934082\n26.017868041992188\n26.017868041992188\n26.01786231994629\n26.01786231994629\n26.017860412597656\n26.017860412597656\n26.01785659790039\n26.01785659790039\n26.017852783203125\n26.017850875854492\n26.017847061157227\n26.017847061157227\n26.017845153808594\n26.017845153808594\n26.017841339111328\n26.017837524414062\n26.017837524414062\n26.01783561706543\n26.017833709716797\n26.017831802368164\n26.017831802368164\n26.01782989501953\n26.01782989501953\n26.017826080322266\n26.017822265625\n26.017822265625\n26.017820358276367\n26.017818450927734\n26.0178165435791\n26.01781463623047\n26.017812728881836\n26.017810821533203\n26.01780891418457\n26.017807006835938\n26.017805099487305\n26.017805099487305\n26.01780128479004\n26.017799377441406\n26.017797470092773\n26.017799377441406\n26.017793655395508\n26.01779556274414\n26.01778793334961\n26.017789840698242\n26.01778793334961\n\n\n\nAfter 1200 iterations we manage to reduce the loss from 160. to 26.\nNow let’s plot the process\n\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\nAfter each iteration we will have a brand new quadratic function, which will be much more close to the actual one that represent the real data\n\n\n\nStep 7: stop\n\nAfter n# of iterations we decide to stop\n\n\n\nSummarizing Gradient Descent\n\nAt the beginnig we start with the Weights\n\npick the randomly if we build the model from scartch\nget them pretrained from another model : TransferLearning\n\nBuild a Loss Function\n\nallow us to see how good or bad the outputs the model gives us\nthen we try to change/update the weights in way that makes the loss function better==lower\n\nTo find a way to do that (update the weights to optimize the loss) we use calculus to caltulate the Gradients\n\ngradients descent tells us either we decrease or increase the weights in order to minimize the loss (that simple!!)\n\nWe then iterate till we reached the lowest point\nStop"
  },
  {
    "objectID": "posts/Fastai_ch4/Ch4.html#the-mnist-loss-function",
    "href": "posts/Fastai_ch4/Ch4.html#the-mnist-loss-function",
    "title": "Chapter 4: Deep learning for coders with fastai and pytorch",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\nWe saw previously the function pr_eights() where we represent the input images x as vector, just like the w weight vector.\n\nhere we will do the same with our MNIST_sample\n\nWe already have our dataset for 3s and 7s as tensors == the x of the function:\n\ntraining-set: stacked_threes, stacked_sevens\nvalidation-set : valid_ten_3, valid_ten_7\n\nWe’ll concatenate them all into a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor)\n\nby using .view\nwhich is a Pytorh method that changes the shape of a tensor without changing its content\n-1 is a special parameter to .view that mean: make this axis as big as necessary to fit the data\n\n\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\n\ntrain_x.shape\n\ntorch.Size([12396, 784])\n\n\n\nWe need a label for each image. We’ll use 1 for 3s and 0 for 7s:\n\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens))\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396]))\n\n\n\nThe problem here is that train_x and train_y don’t match in terms of shape.\nHere we will use unsqueeze() method to train_y which will return rank 2 tensor: \n\n\ntrain_y= train_y.unsqueeze(1)\ntrain_y.shape\n\ntorch.Size([12396, 1])\n\n\n\nHere we create dset dataset for training by ziping both dependent and independent variables.\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\n\nGo through the same steps for validation set\n\n\nvalid_x = torch.cat([valid_ten_3, valid_ten_7]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_ten_3) + [0]*len(valid_ten_7)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\nNow we begin the 7 steps we saw earlier but now for the mnist model\n### 1-INIT the parameters\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nInitialize weights\n\n\nweights = init_params((28*28, 1))\nweights.shape\n\ntorch.Size([784, 1])\n\n\n\nInitialize the bias\n\n\nbias = init_params(1)\nbias\n\ntensor([0.6863], requires_grad=True)\n\n\n\n2-Prediction calculation\n\nNow we can calculate the prediction based on those randome weights and biases for one image[0]\nIn order to multiply 2 matrix you need to make sure they have same number of inner product:\n\nnumber of columns first matrix==number of rows second matrix\n\n\n\n(train_x[0]*weights.T).sum()+bias\n\ntensor([20.2336], grad_fn=<AddBackward0>)\n\n\n\nIn python matrix multiplication is represented by @\nHere we create a function that return our training set as matrix multiplied @ by weights then added to the bias radome value\n\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[20.2336],\n        [17.0644],\n        [15.2384],\n        ...,\n        [18.3804],\n        [23.8567],\n        [28.6816]], grad_fn=<AddBackward0>)\n\n\n\nCheck the accuracy of our modl based on random weights\n\n\n# Let's check our accuracy\ncorrects = (preds>0.0).float()==train_y\ncorrects\n\ntensor([[ True],\n        [ True],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\n# check the mean\ncorrects.float().mean().item()\n\n0.4912068545818329\n\n\n\nNow let’s see what the change in accuracy is for a small change in one of the weights (note that we have to ask PyTorch not to calculate gradients as we do this, which is what with torch.no_grad() is doing here):\n\n\nwith torch.no_grad(): weights[0] *= 1.0001\n\n\npreds = linear1(train_x)\n((preds>0.0).float() == train_y).float().mean().item()\n\n0.4912068545818329\n\n\n\nAs we notice even with the change we commited in one of the weights we didn’t observe any change at all in the model accuracy! which rise a problem for our method. we need a formula that can reflexes the changes we commited to the parameters, so we update the parameters in a way that make our predictions better.\nThe problem with the thresh preds>0.0 is that the gradients are allways equal to zero, because as we know the gradients are calculate as rise over run, and in this case the small change in parameters is rarely could change the prediction from 3 to 7 or vice-versa, so the gradients will allways indicate to 0.\n\nInstead we need a loss function, which when our weights results a slightly better prediction, gives us a slightly better loss function\n\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\n\nHere we assume that the trgts is the image we want to predict, 1 for 3’s and 0 for 7’s,\nSo in this case the model is confident in the first image it’s a 3 with 0.9, and not sure about the second image with 0.4, and way too incorrect in the 3d image with 0.2\nThe objective is to build a loss function that will calculate the accuracy of the model and returns some metrics that will be updated if we update our parameters\n\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\ntorch.where is a way of puting specific condition, in other way we can say this:\n\nif targets==1: loss = 1-predictions else: targets==0\n\ntorch.where(trgts==1, 1-prds, prds).mean()\n\ntensor(0.4333)\n\n\n\nSee if the loss will be updated if we change the weights.\n\nwe will pass this tensor [0.9, 0.4, 0.8] and see how to change the prediction from 0.2 to 0.8 will efects the loss function\n\n\n\nmnist_loss(tensor([0.9, 0.4, 0.8]), trgts)\n\ntensor(0.2333)\n\n\n\nAs we see here the loss function gets better when we minimize the distance between the prediction and the target in the third image\n\nThe problem that we still face in this method is that we assume that the predictions will allways be between 0 and 1\nTo solve this problem we need a function that return any prediction how matter bigger that 1 or smaller than 0 to the interval between those 2 numbers\nIn fact there’s a function that do the same exact thing, we call it SIGMOID FUNCTION\n\n\n\nSigmoid Function\nThe sigmoid function always outputs a number between 0 and 1. It’s defined as follows:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\nWhatever input we give it to Sigmoid it will always return a number between 0 and 1\nNow let’s update the mnist_loss by adding sigmoid to its inputs\n\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nSGD and Mini-Batches\n\nNow we have our loss function for the SGD, we need to know which method we will take in order to update the gradients, we can update them after taking all the data points or after calculating each data points.\nThe first method will take a lot of time, and the second will not use much information, so we will take another track which is to use Mini-Batches and update the gradients after one mini-batch\nHow many datapoints in each batch is called batch-size. a larger batch size will produce more accurate result but it will take much time, and smaller batch-size will need many epochs to learn but it will be faster\nWe will see later how to decide the suitable batch-size for each situation\nWe will use DataLoader in order to shuffle data items before we create the mini-batches, so we vary the data items in each of the mini batches\n\n\n#for example\nc = range(15)\ndl = DataLoader(c, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([ 3, 12,  8, 10,  2]),\n tensor([ 9,  4,  7, 14,  5]),\n tensor([ 1, 13,  0,  6, 11])]\n\n\n\nBut in training model scenarios we won’t need any pyhton collection, we want a collection containing dependent and independent variables.\n\nA tuple that contains dependents and independent variables called in Pytorch a Dataset\n\nHere is a simple dataset:\n\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\nWhen we pass a Dataset to a DataLoader we will get back mini-batches which are themselves tuples of tensors representing batches of independent and dependent variables:\n\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n (tensor([2, 4]), ('c', 'e'))]\n\n\n\n\nPutting It All Together\nNow we have :\n* dset, valid_dset * linear1 * mnist_loss * sigmoid_function\nwe can create the model from scratch\nFirst we re-intialize our parameters:\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n\nThen create the DataLoader from Dataset\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nWe’ll do the same for the validation set:\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nLet’s create a mini-batch of size 4 for testing:\n\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\nCall linear1 model we created earlier on the batch:\n\n\npreds = linear1(batch)\npreds\n\ntensor([[-2.1876],\n        [-8.3973],\n        [ 2.5000],\n        [-4.9473]], grad_fn=<AddBackward0>)\n\n\n\nCreate the loss function:\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.7419, grad_fn=<MeanBackward0>)\n\n\n\nNow we can calculate the gradients:\n\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0061), tensor([-0.0420]))\n\n\n\nLet’s put that all these steps in a single function:\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\nAnd test it..\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0121), tensor([-0.0840]))\n\n\n\nBut look what happens if we call it twice:\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0182), tensor([-0.1260]))\n\n\n\nThe gradients have changed!\n\nin order to not calculate the gradients and add to the last one every time we call this function calc_gradwe need to use grad.zero_ which set the current gradients to zero\n\n\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\nOur only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\nLet’s build a function that calculate the batch accuracy\n\n\n# accuracy of the batch\n(preds>0.0).float() == train_y[:4]\n\ntensor([[False],\n        [False],\n        [ True],\n        [False]])\n\n\n\nFunction to calculate our validation accuracy:\n\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\n\n# check if it works\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.2500)\n\n\n\n# all together\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.5262\n\n\n\nNow lets train for one epoch and see if the accuracy improve\n\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6663\n\n\n\nThats promising\n\nfrom 0.4642 to 0.6096 after one epoch\n\nThen do a few more:\n\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8265 0.89 0.9183 0.9276 0.9398 0.9467 0.9506 0.9525 0.9559 0.9579 0.9599 0.9608 0.9613 0.9618 0.9633 0.9638 0.9647 0.9657 0.9672 0.9677 \n\n\n\n\nCreating an Optimizer\n\nAs we know the model we created is only for learning .\n\nin real world scenarios we do not need to implement everything from scratch, framworks like Pytorch and Fastai provide us with everything.\n\nThe linear1 model we created can be remplaced with nn.Linear which can do the same work and more\n\nnn.Linear combine the role of linear1 and weights+bias\n\n\n\nlinear_model = nn.Linear(28*28, 1)\n\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\n\nw, b = linear_model.parameters()\nw.shape, b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nWe can create optimizer class\n\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\n\n# passing the model params to the optimizer\nopt = BasicOptim(linear_model.parameters(), lr)\n\n\n# simplifying the epoch training function\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\n# validation doesn't change\nvalidate_epoch(linear_model)\n\n0.4606\n\n\n\n# simplifying the training loop:\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\n# same results as before\ntrain_model(linear_model, 20)\n\n0.4932 0.7686 0.8555 0.9136 0.9346 0.9482 0.957 0.9634 0.9658 0.9678 0.9697 0.9717 0.9736 0.9746 0.9761 0.9771 0.9775 0.9775 0.978 0.9785 \n\n\n\nThe good thing is all of this is provided by Fastai:\n\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.8179 0.8496 0.9141 0.9346 0.9482 0.957 0.9619 0.9658 0.9673 0.9692 0.9712 0.9741 0.9751 0.9761 0.9775 0.9775 0.978 0.9785 0.979 \n\n\n\nFastai also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders:\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nTo create a Learner we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nNow we can call fit:\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.636709\n      0.503144\n      0.495584\n      00:00\n    \n    \n      1\n      0.429828\n      0.248517\n      0.777233\n      00:00\n    \n    \n      2\n      0.161680\n      0.155361\n      0.861629\n      00:00\n    \n    \n      3\n      0.072948\n      0.097721\n      0.917566\n      00:00\n    \n    \n      4\n      0.040128\n      0.073205\n      0.936212\n      00:00\n    \n    \n      5\n      0.027210\n      0.059466\n      0.950442\n      00:00\n    \n    \n      6\n      0.021837\n      0.050799\n      0.957802\n      00:00\n    \n    \n      7\n      0.019398\n      0.044980\n      0.964181\n      00:00\n    \n    \n      8\n      0.018122\n      0.040853\n      0.966143\n      00:00\n    \n    \n      9\n      0.017330\n      0.037788\n      0.968106\n      00:00\n    \n  \n\n\n\n\n\nAdding a Nonlinearity\n\nSo far we managed to create a linear function that can predict hand written digit with high performance.\nBut still, it’s just a simple linear classifier, with very constraint abilities.\nTo make it more complex and capable handle complex tasks, we need to add something non-linear between two linear classifiers.\n\nThis is gives us a Neural network\n\nHere a basic architecture of a neural network:\n\n\ndef simple_net(xb): \n    res = xb@w1 + b1 # first linear function\n    res = res.max(tensor(0.0)) # non linear function\n    res = res@w2 + b2  # 2nd linear function\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n\nw1.shape, b1.shape\n\n(torch.Size([784, 30]), torch.Size([30]))\n\n\n\nthe lines: res = xb@w1 + b1 and res@w2 + b2 are two classifier, basic linear function similar to the nn.Linear we use previously\nWhile the line res = res.max(tensor(0.0)) is a non linear function\n\nthis function called rectified linear unit ReLu which return every negative number with 0\n\nSo if we think about this architecture we have here:\n\nfirst we have a linear function that does the matrix multiplication between dataset tensors and initialzed weights + bias and output 30 (we can chose any number) features, which represent for each a different mix of pixels\nthese outputs are taken by the ReLU and converted to 0 if they are negativem and x==y if it not, then output also 30 features to the next linear layer, which will do the sam computaion as the first and output the results\n\nIn pythorch there’s a modul that fit our neural nets here: nn.Sequential().\n\ntake results from a layer to another\n\nwe also replace the linear function we built by nn.Linear and max((0.0)) with nn.ReLU()\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\n#lets try the model again but now with 2 layers\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.333021\n      0.396112\n      0.512267\n      00:00\n    \n    \n      1\n      0.152461\n      0.235238\n      0.797350\n      00:00\n    \n    \n      2\n      0.083573\n      0.117471\n      0.911678\n      00:00\n    \n    \n      3\n      0.054309\n      0.078720\n      0.940628\n      00:00\n    \n    \n      4\n      0.040829\n      0.061228\n      0.956330\n      00:00\n    \n    \n      5\n      0.034006\n      0.051490\n      0.963690\n      00:00\n    \n    \n      6\n      0.030123\n      0.045381\n      0.966634\n      00:00\n    \n    \n      7\n      0.027619\n      0.041218\n      0.968106\n      00:00\n    \n    \n      8\n      0.025825\n      0.038200\n      0.969087\n      00:00\n    \n    \n      9\n      0.024441\n      0.035901\n      0.969578\n      00:00\n    \n    \n      10\n      0.023321\n      0.034082\n      0.971541\n      00:00\n    \n    \n      11\n      0.022387\n      0.032598\n      0.972031\n      00:00\n    \n    \n      12\n      0.021592\n      0.031353\n      0.974485\n      00:00\n    \n    \n      13\n      0.020904\n      0.030284\n      0.975466\n      00:00\n    \n    \n      14\n      0.020300\n      0.029352\n      0.975466\n      00:00\n    \n    \n      15\n      0.019766\n      0.028526\n      0.975466\n      00:00\n    \n    \n      16\n      0.019288\n      0.027788\n      0.976448\n      00:00\n    \n    \n      17\n      0.018857\n      0.027124\n      0.977429\n      00:00\n    \n    \n      18\n      0.018465\n      0.026523\n      0.978410\n      00:00\n    \n    \n      19\n      0.018107\n      0.025977\n      0.978901\n      00:00\n    \n    \n      20\n      0.017777\n      0.025479\n      0.978901\n      00:00\n    \n    \n      21\n      0.017473\n      0.025022\n      0.979392\n      00:00\n    \n    \n      22\n      0.017191\n      0.024601\n      0.980373\n      00:00\n    \n    \n      23\n      0.016927\n      0.024213\n      0.980373\n      00:00\n    \n    \n      24\n      0.016680\n      0.023855\n      0.981354\n      00:00\n    \n    \n      25\n      0.016449\n      0.023521\n      0.981354\n      00:00\n    \n    \n      26\n      0.016230\n      0.023211\n      0.981354\n      00:00\n    \n    \n      27\n      0.016023\n      0.022922\n      0.981354\n      00:00\n    \n    \n      28\n      0.015827\n      0.022653\n      0.981845\n      00:00\n    \n    \n      29\n      0.015641\n      0.022401\n      0.981845\n      00:00\n    \n    \n      30\n      0.015463\n      0.022165\n      0.981845\n      00:00\n    \n    \n      31\n      0.015294\n      0.021944\n      0.983317\n      00:00\n    \n    \n      32\n      0.015132\n      0.021736\n      0.982826\n      00:00\n    \n    \n      33\n      0.014977\n      0.021541\n      0.982826\n      00:00\n    \n    \n      34\n      0.014828\n      0.021357\n      0.982336\n      00:00\n    \n    \n      35\n      0.014686\n      0.021184\n      0.982336\n      00:00\n    \n    \n      36\n      0.014549\n      0.021019\n      0.982336\n      00:00\n    \n    \n      37\n      0.014417\n      0.020864\n      0.982336\n      00:00\n    \n    \n      38\n      0.014290\n      0.020716\n      0.982336\n      00:00\n    \n    \n      39\n      0.014168\n      0.020576\n      0.982336\n      00:00\n    \n  \n\n\n\n\nAt this point we did everything we can to improve the model performane, and we got an accuracy of 982826, which is very solid number.\nfrom here on, all we can do is looking inside our model and understand the mechanic of it in each steps\n\n\n# we can see the architecture of the model\nm =learn.model\n\n\nAs expected the model architecture contains 2 layers and a non-linear function in-between.\n\n\nm\n\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\n\n\nWe could also see what did the model learn in each layer\n\n\nw, b= m[0].parameters()\n\n\nshow_image(w[22].view(28,28))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7eff5bce6210>"
  },
  {
    "objectID": "posts/Fastai_ch2/Questionnaire.html",
    "href": "posts/Fastai_ch2/Questionnaire.html",
    "title": "Chapter 2: Questionnaire",
    "section": "",
    "text": "Q1:\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n___ * There is many cases where the bear_classifaction model could produce low results because of low quality of training data: - imbalance dataset: where we have much datapoint of one class way more than other classes, what causes the model to be biased toward one class. - the image used in training are low quality, low resolution.\nQ2: Where do text models currently have a major deficiency? ___ * The current transformers models shows oustanding results in generating texts and essais, understanding (in a way!) human language and can participate in a full conversation on different topics and give understandable and admirable responses. * However the way models learn from text is way different than the human do. Models needs a huge amount of text data: \n\nIn this paper,Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data Emily Bender and Alexander Koller consider whether LMs such as GPT-3 or BERT can ever learn to “understand” language? the researchers insists on deffirentiate between form (which LMs are good at understanding) and meaning( which obviously LMs can’t understand).\n\nQ3:\nWhat are the possible negative societal implications of text generation models?\n___ * If someone uses these LMs to generate highly-compeling responds on social media in order to spred misinformation or encourage conflits.\nQ4:\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n___\n* In this case we need to set a system where there’s a human intervention.\nQ5:\nWhat are the steps of the Drivetrain approach?\n___ - Define your Objective\n- Levers\n- Data\n- Models\nQ6:\nWhat is DataLoaders?\n___ * DataLoaders is a Fastai thin class that coutains dataloader for training and validation.\nQ7:\nWhat four things do we need to tell fastai to create DataLoaders?\n___ * Data we have * How to get items * How to label them * How to create train/validation\nQ8:\nWhat does the splitter parameter to DataBlock do? ___ * Splitter provide the way we want our data set to be splited.\nQ9:\nHow do we ensure a random split always gives the same validation set?\n___ * By fixing the seed value.\nQ10:\nWhat letters are often used to signify the independent and dependent variables?\n____ * x for independent variables, y for dependent.\nQ11:\nWhat’s the difference between crop, pad, and squish Resize() approaches? When might you choose one over the other?\n___\n\nCrop is the default Resize() method, which crop the image and take desired dimension, this may cause losing important information.\nPad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images), this may results in a lower effective resolution for the part of the image we actually use.\nSquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nThe better method is something depends on the problem we have, type of data we will use..\nWe will see later different methods, like RandomResizeCrop and many more.\n\nQ12:\nWhat is data augmentation? Why is it needed?\n___\n* Data augmentation refer to the process of generating more datapoints from the actaual data we have, and representing it within the dataset. * For example we could take an image and do some type of transformation to it, like flipping it or ratating it the resize the crop it, which will give us many images with different views and sizes. * This method other than making our dataset larger, it make it rich and diverse which will without doubt influence the generalization of the model.\nQ13:\nWhat is the difference between item_tfms and batch_tfms?\n___ * item_tfms is done on cpu, batch_tfms on gpu.\nQ14:\nWhat is a confusion matrix?\n___ * confusion_matrix return where the model get wrong prediction and what was the actual label.\nQ15:\nWhat does export save do?\n____ * It saves 3 things: - the architecture - the updated parameters(weihts+biases) - the way we built dataloaders\nQ15:\nWhat is it called when we use a model for getting predictions, instead of training?\n____ * Inference"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "As we saw in the previous Chapter, we can create powerful model with only 6 lines of code\n\nAlthought we should understand the constraints of the process and not overestimate the capabilities of deep learnig, this may lead to frustaingtly poor result\n\nAlso we need to not overestimate the constraints, and underestimate what could do with deep learning"
  },
  {
    "objectID": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "href": "posts/Fastai_ch2/2022_09_09_Fastai_Lesson2.html#deep-learning-in-production",
    "title": "Chapter 2: Deep learning for coders with fastai and pytorch",
    "section": "Deep Learning In Production",
    "text": "Deep Learning In Production\nWhen we create a model that met our objectives, we can then pass to the Production phase, where we transform the model into an appliction/service etc..\nBut first we need to export the model into a file:\n\nlearn.export()\n\nLet’s check that the file exists, by using the ls method that fastai adds to Python’s Path class:\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\n\nNow with the export.pkl file we can create normal program that takes inputs(images) and produce results (predictions) just like any traditional app\nAt this point we won’t call it a model anymore, we call it inference.\n\n >Using the trained model as program\nThe export() function allow us to save the model in oreder to use it later, and as we know model is Architecture + Parameters, Fastai by default saves also the method of which we’ve created the DataLoaders, because otherwise we have to define it again in order to work with the new data we will feed to the model.\n\nFrom Model to Inference\nThis file export.pkl is allways needed wherever we will create an app from it, for now we will use it whithin this notebook in order to create a small app that can predict bears type from image we will provide.\nWhen we use a model for getting predictions, instead of training, we call it inference.\nTo create inference learner from export.pkl file we use load_learner:\n\nlearn_inf = load_learner(path/'export.pkl')\n\n\n# predicting one image\nlearn_inf.predict('images/bear.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([8.0130e-07, 1.0000e+00, 2.0153e-07]))\n\n\n\nThis has returned three things:\n\nthe predicted category(label) in the same format we originally provided (in this case that’s a string),\nthe index of the predicted category, and the probabilities of each category.\nthe last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories.\n\nAt inference time, you can access the DataLoaders as an attribute of the Learner:\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\nWe can see here that if we index into the vocab with the integer returned by predict then we get back “grizzly,” as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly.\n\n\nGradio + HugginFace Spaces\n\nIf we want to share our model with a broader audience, and showcase our skills we need to create a real app that can be used outside of the datascience/machine learning word where nobody know or have the ability to use jupyter notebook or python.. and that’s why we will show using a combination of python package Gradio that will allow us to build our app, then host it on HuggingFace\n\n\n# firstly install gradio\n!pip install gradio\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting gradio\n  Downloading gradio-3.6-py3-none-any.whl (5.3 MB)\n     |████████████████████████████████| 5.3 MB 5.2 MB/s \nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\nCollecting pydub\n  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\nCollecting paramiko\n  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n     |████████████████████████████████| 212 kB 67.2 MB/s \nCollecting httpx\n  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\nCollecting orjson\n  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n     |████████████████████████████████| 270 kB 62.9 MB/s \nCollecting ffmpy\n  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\nCollecting fastapi\n  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n     |████████████████████████████████| 55 kB 3.8 MB/s \nRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\nCollecting h11<0.13,>=0.11\n  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n     |████████████████████████████████| 54 kB 3.7 MB/s \nRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\nCollecting pycryptodome\n  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n     |████████████████████████████████| 2.3 MB 47.6 MB/s \nRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.8.2)\nCollecting markdown-it-py[linkify,plugins]\n  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n     |████████████████████████████████| 84 kB 3.9 MB/s \nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\nCollecting python-multipart\n  Downloading python-multipart-0.0.5.tar.gz (32 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\nCollecting uvicorn\n  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n     |████████████████████████████████| 56 kB 5.3 MB/s \nCollecting websockets\n  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n     |████████████████████████████████| 112 kB 70.9 MB/s \nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\nCollecting starlette==0.20.4\n  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n     |████████████████████████████████| 63 kB 2.5 MB/s \nCollecting anyio<5,>=3.4.0\n  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n     |████████████████████████████████| 80 kB 10.6 MB/s \nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (2022.9.24)\nCollecting httpcore<0.16.0,>=0.15.0\n  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n     |████████████████████████████████| 68 kB 6.7 MB/s \nCollecting rfc3986[idna2008]<2,>=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting mdit-py-plugins\n  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n     |████████████████████████████████| 46 kB 4.5 MB/s \nCollecting linkify-it-py~=1.0\n  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\nCollecting uc-micro-py\n  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.4)\nCollecting pynacl>=1.0.1\n  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n     |████████████████████████████████| 856 kB 63.6 MB/s \nCollecting cryptography>=2.5\n  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n     |████████████████████████████████| 4.0 MB 41.4 MB/s \nCollecting bcrypt>=3.1.3\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n     |████████████████████████████████| 593 kB 68.7 MB/s \nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.25.11)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\nBuilding wheels for collected packages: ffmpy, python-multipart\n  Building wheel for ffmpy (setup.py) ... done\n  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=bf42b4bf9ba2633a885814063d3340770c60207e05f131d9e165fafd4a5e1aa5\n  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n  Building wheel for python-multipart (setup.py) ... done\n  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=f8ff9e663a939cd5bba16c9c7ce235f69c0fab5ae8199bbfce9b5fdb3361d05e\n  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\nSuccessfully built ffmpy python-multipart\nInstalling collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\nSuccessfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.1 fastapi-0.85.1 ffmpy-0.3.0 gradio-3.6 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.19.0 websockets-10.3\n\n\n\n# create labels from dalaloaders vocab\nlabels = learn.dls.vocab\n# predicting function that take an image as input and use learn.predict to ouput:\n# prediction, prediction index, and probability.\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\n\nHere we will use gradio in order to create an app withing this notebook.\n\n\n\nimport gradio as gr\ngr.Interface(fn=predict, inputs=gr.inputs.Image(shape=(512, 512)), outputs=gr.outputs.Label(num_top_classes=3)).launch(share=True)\n\n/usr/local/lib/python3.7/dist-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n  \"Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n  warnings.warn(value)\n/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n  warnings.warn(value)\n\n\nColab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\nRunning on public URL: https://bbc80cb39aa58cc0.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n(<gradio.routes.App at 0x7f45d2b49710>,\n 'http://127.0.0.1:7860/',\n 'https://bbc80cb39aa58cc0.gradio.app')\n\n\n\nStill the best way of creating app for inference is to use Hugginface platform with help of gradio.\nHere is the app that classify bears types based the .pkl file we created from our model we trained: Here"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html",
    "href": "posts/Fastai_ch1/Chapter1.html",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 34.6 MB/s \n     |████████████████████████████████| 1.3 MB 53.8 MB/s \n     |████████████████████████████████| 5.3 MB 53.3 MB/s \n     |████████████████████████████████| 441 kB 65.8 MB/s \n     |████████████████████████████████| 1.6 MB 46.3 MB/s \n     |████████████████████████████████| 212 kB 71.6 MB/s \n     |████████████████████████████████| 115 kB 71.6 MB/s \n     |████████████████████████████████| 163 kB 73.8 MB/s \n     |████████████████████████████████| 127 kB 72.7 MB/s \n     |████████████████████████████████| 115 kB 73.2 MB/s \n     |████████████████████████████████| 7.6 MB 48.1 MB/s \nMounted at /content/gdrive"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "href": "posts/Fastai_ch1/Chapter1.html#brief-history-of-neural-network",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Brief History of Neural Network",
    "text": "Brief History of Neural Network\n\nIn 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, both devoloped a concept called Artificial Neuron which is seen today as the first theorotical demonstration of a machine that’s “..capable of perceiving, recognizing and identifying its surrondings”\n\nIn the 50’s Frank Rossenblatt devoloped a device based on the principiles of Artificial Neuron\n\nRosenblatt invented The Perceptron which was capable of recognizing simple shapes and patterns\n\n\nIn 1969 Marvin minsky write a book called Perceptrons where he showed that Perceptron limits of solving critical math problem\n\n\nParallel Distributed Processing (PDP)\n\nAfter a long winter of Deep Learning, a group of researchers at MIT released a papper in 1986 released the most influencial papper in history of Neural Network\n\nAuthors claimed that PDP approach was closer to how human brain works\n\nPDP require some enviroments elements:\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\n1980’s and 90’s\n\nDuring this epoch researcher started to build models with 2 layers of neurons\n\nWe saw real world application of Deep Learning"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "href": "posts/Fastai_ch1/Chapter1.html#top-to-bottom-learning-approach",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "Top to bottom learning approach",
    "text": "Top to bottom learning approach\n\nStart with the end point\n\nBuild a foundation of intuition through application then build on it with theory\nShow students how individuals pieces of theory are combined in a real-world application\n\nTeach through examples\n\nProvide a context and a purpose for abstract concepts\n\n\n\nWhat is Machine Learning?\nTraditional Programming:\n- It’s hard to explicitly code hard tasks specialy when you don’t know the exact steps\n\n\n\na-traditional-program.png\n\n\nMachine Learning\n- A program that need to be showed examples of the desired tasks so it can Learn from them\n- Demand sufficient amount of examples(Data)\n- In 1949 Arthur Samuel manage to build a machine that can play checker - This work introduced multiple concepts to the world of machine learning: - The idea of a weight assignment\n- The fact that every weight assignment has some actual performance\n- The requirement that there be an automatic means of testing that performance\n- The need for a mechanism (i.e., another automatic process) for improving the performance by changing the weight assignments\n- Update the weight values based on the performance with the current values\n\n\n\nprogram-using-weight-assignments.png\n\n\n\nA system that used Weight Assignment\n\n >Training Machine Learning model with help of weights updating\n >Using the trained model as program\nNow Let’s Build Our First Model!"
  },
  {
    "objectID": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "href": "posts/Fastai_ch1/Chapter1.html#first-deep-learning-model-its-a-bird",
    "title": "Chapter 1: Deep learning for coders with fastai and pytorch",
    "section": "First Deep Learning Model: It’s a Bird!",
    "text": "First Deep Learning Model: It’s a Bird!\n\nImport Dependencies\n\n#Import fastai library\n#Import fastai computer vision library \nfrom fastbook import *\nfrom fastai.vision.all import *\n\n\n\nStart working with Data\n\n# Here we use search_images_ddg in order to download an url from ddg\n# search engine accornding to the keyword we choose: birds photo\nurls = search_images_ddg('birds photo', max_images=1)\nlen(urls), urls[0]\n\n(1,\n 'https://www.wallpapergeeks.com/wp-content/uploads/2014/02/Colorful-Bird-Perched-Wallpaper.jpg')\n\n\n\n# download the photo from the url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nPath('bird.jpg')\n\n\n\n# then show it\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow we will do the same etapes we did with bird.jpg image but with forest picture\n\n# Download the url, download the image then open it\ndownload_url(search_images_ddg('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\n\n\n\nThe idea here is to build a model that can classify pictures of birds and forests accuratly, but first we need to build the Dataset\n\n# create 2 directories, one for birds and other for forest, then do the same etapes we did earlier,\n# download the urls then the images\nsearches = 'forest','bird'\npath = Path('bird_or_not')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images_ddg(f'{o} photo'))\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSince we are dowlnloading images from the web, there is a chance that some of them are corrupted, so we need to clean the dataset from them\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nNow we have two directories :\n\nbirds\nforests\n\nAs we saw before, machine learning model needs to be feed many examples in order to learn from them.\nThese examples are the images of birds and forests in our case\nWe need to tell the model bunch of informations about our data before the training process, this will be done with help of DataBlock\n\n\n\nCreating DataBlock and DataLoaders\nNow we need to build DataBlock in order to feed the dataset to the model for the training.it’s basically set of rules of how to organize the dataset for the model.\nI will write a BlogPost about this concept later\nThere’s also the concept of DataLoaders which is an iterator class that load data to the model according to the set of rules that we set earlier while creating DataBlock\n\n# create the dataBlock\n# tell the model what kind of data we'r dealing with\n# how to get the data\n# how to create validation and training set\n# how to get the labels\n# how to set tranformers(items in this case)\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\nTraining the Model\nNow we are ready to train our model and see if we could classify photo of birds and forests\nThe model we will use here is resenet18 which is a famous model that’s used widely among computer vision classification problem, 18 stands for how many layers does the model have.\nfastai comes with fine_tune() which uses the best practices of fine tuning process.\n\nlearn =  vision_learner(dls, resnet18, metrics= error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.550929\n      2.057105\n      0.447368\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.084728\n      0.225560\n      0.092105\n      00:02\n    \n    \n      1\n      0.045721\n      0.024148\n      0.000000\n      00:02\n    \n    \n      2\n      0.031065\n      0.012299\n      0.000000\n      00:02\n    \n  \n\n\n\n\nWith help of fastai library, and less than 20 lines of code we managed to build a model that can predict images of birds and forests with accuracy of 100%!\n\n\n\nFine Tuning\nis a process where we start with a model that has already be trained and we use it on our problem and on our dataset, this operation needs to adapt the pre-trained model by a bit so it fit our system\nThis approach can save us money and time, all we need to do is adapt the pre-trained model and take advantage of set of weights and use them in our problem\n\n\n\nLearner\nIn Fastai we have the Learner which takes 2 things(For now!): firsly the Dataset, and secondly the Model.\nBecause Deep Learning is more mature now,there’s a small group of Architecture that can be used in nearly any problem, that’s why the actual work of deep learning practionaire is to work on data preparation, and model deploying, more than anything else.\nThis is why Fastai integrated a famous library Timm which collect all SOTA (State Of The Art) computer vision models.\n\n\nVisualizing layers of a trained neural network\n\nIt is possible to inspect deep learning models and get insights from them\n\ncan still be challenging to fully understand\n\nVisualizing and Understanding Convolutional Networks paper\n\npublished by PhD student Matt Zeiler and his supervisor Rob Fergus in 2013\nshowed how to visualize the neural network weights learned in each layer of a model\ndiscovered the early layers in a convolutional neural network recognize edges and simple patterns which are combined in later layers to detect more complex shapes\n\nWe will see this paper in details later, but for now we could show the model architecture.\n\n\n# show the model architecture\nm = learn.model\nm\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)\n\n\n\n\nOther Deep Learning Applications\n\nImage Segmentation\n\ntraining a model to recognize the content of every single pixel in an image\n\n\n\n\n\noutput_47_2.png\n\n\n\nNatural Language Processing (NLP)\n\ngenerate text\ntranslate from one language to another\nanalyze comments\nlabel words in sentences\n\nTabular Data\n\ndata that in in the form of a table\n\nspreedsheets\ndatabases\nComma-separated Values (CSV) files\n\nmodel tries to predict the value of one column based on information in other columns\n\nRecommendation Systems\n\nmodel tries to predict the rating a user would give for something"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "",
    "text": "Do you need these for deep learning?\n\nLots of math\n\nFalse\n\nLots of data\n\nFalse\n\nLots of expensive computers\n\nFalse\n\nA PhD\n\nFalse\n\n\n\n\n\nName five areas where deep learning is now the best in the world.\n\nMedical research\nRobotics\nAssurance\nLinguistics/ Natural Lunguage Processing\nBiology\n\n\n\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nPercepton\n\n\n\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nSet of processing units\n\nA state of activation\n\nAn output function for each unit\n\nPattern of connectivity among units\n\nPropagation rule for propagating patterns of activities through the network of connectivities\n\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n\nLearning rule whereby patterns of connectivity are modified by experience\n\nAn environment within which the system must operate\n\n\n\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nMinsky in his book Perceptons shows the limitation of the device “perceptons” and how it cannot solve any comlex problem, the ai community did agreed with Minsky, but the did not pay attention to the solution he suggested, which is a 2 layer model.\nIn 80’s the 2 layer models were usual in ai labs. In theory a 2 layer model can solve any problem, but in practice the layers were too big and consum a lot of computation power, the solution to this is to add more layers, but this insight was not acknowledged, what leat to the 2 winter of NN.\n\n\n\nWhat is a GPU?\nIt’s a Graphic Prossessing Unit, which is used to do many computation tasks in parallel, which help to accelerate to compution of big tasks by cutting them into small tasks and compute them in parallel\n\n\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nto write a programm that recognize images in a photo we need to write a long set of rules that resume any possible photo and image, tell the computer exactly how to deal with any of them, which is way more complicated that what we can do. That’s way we use machine learning to solve those kinfd of problems, just by showing the model data and help it to learn from it.\n\n\n\nWhat did Samuel mean by “Weight Assignment”?\nWhat term do we normally use in deep learning for what Samuel called “Weights”?\nDraw a picture that summarizes Arthur Samuel’s view of a machine learning model\nweight Assignment is refer to the parameters of the model, these are what we call today weights and bias. they are set of value we assign to each data point, what make the optimization of the loss possible.\n\n\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nThis is a highly-researched topic known as interpretability of deep learning models. the natur of deep learning “deep” make it hard to really understand the way the model solve each problem, specially if the model has many layers, what makes it even hard to know exactly which layer is responsable of the procces of learning which part.\n\n\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nUniversal approximation theorem\n\n\n\nWhat do you need in order to train a model?\nIn order to train a model, we need architecture for the given problem, we first need data(+labels), then we need set of values (paramaters), then we need some kind of metric to know if our model did good or bad (loss function), and we need a way to updates these parameters in order to optimize the loss function.\n\n\n\nHow could a feedback loop impact the rollout of a predictive policing model?\nIn a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop\n\n\n\nDo we always have to use 224x224 pixel images with the cat recognition model?\nNo.\n\n\n\nWhat is the difference between classification and regression?\nClassification problem is when we need to decide between 2(or more) classes, the prediction in this problem isn’t quantity, where regression problem is focused on predecting numeric quantity.\n\n\n\nWhat is a validation set? What is a test set? Why do we need them?\nValidation set is small portion of the data set that we preserve from the training fase in order to prevent the model from memorizing the data instead of learning from it. The validation set allow us to measure the performance of the model on data that the model didn’t see before. Same we can say about test set, which is another preserved protion of data that we use in the final fase of the training in order to have a real idea of model performance.\n\n\n\nWhat will fastai do if you don’t provide a validation set?\nit will automatically create a validation set of 20% of our dataset.\nvalid_pct=0.2\n\n\n\nWhat is overfitting?\nis when the model is memorizing answears instead of learning from data.\n\n\n\nWhat is a metric? How does it differ to “loss”?\nMetric is what tells us how the model perform, in other way the loss is what we should minimize in order to optimize the model perfromance.as we will see later, sometimes, we could use the metric as loss.\n\n\n\nHow can pretrained models help?\npertrained model can be used again, we just need to fin it in our probem.\na pretrained model is a model that learned many lesson from the prior problem, and already has good set of paramters, these parameters(weights+biases) are what we seek in the proccess of using a pretrained model. this procces is called fine-tunning, which mean less money and time consuming.\n\n\n\nWhat is the “head” of a model?\nWhen using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model.\n\n\n\nWhat is an “architecture”?\nThe architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.\n\n\n\nWhat are “hyperparameters”?\nTraining models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters."
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#further-research",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "Further Research",
    "text": "Further Research\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?"
  },
  {
    "objectID": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "href": "posts/Fastai_ch1/Ch1_Questionnaire.html#what-is-a-gpu-what-make-it-different-from-cpu-and-why-do-we-need-it-to-do-deep-learning-tasks",
    "title": "Chapter 1: Questionnaire and Further Research",
    "section": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:",
    "text": "What is a GPU? What make it different from CPU? and Why do we need it to do Deep Learning tasks:\n\nGPU stands for Graphic Processing Unit, is a specialized processor with dedicated memory that conventionally perform floating point operations required for rendering graphics.\nIn other words, it’s a single-chip processor used for extensive graphical and mathematical computations which help the CPU to achieve other tasks.\nWhile CPU is designed to handel the complex logic in code once at a time, GPU can do many small operations at the same time, which make it very convenient to deep learning where we need to do many milions of calculations in order to train a model\n\n\nWhy do Deep Learning needs GPU?\n\nGPUs are optimized for training neural networks models as they can process multiple computations simultaneously.\nFor example the model we’ve fine-tuned in chapter one resnet18 which the smaller version of resnet models with only 18 hidden layers,though it has more than 11 millions parameters, in order to do one epoch and calculate all these parameter (wights + biases) and multiply them by the input variables (images) then do the Back-probagation and update them after calculating the loss … all this multiplications will take a large amount of time if we did it on CPU."
  },
  {
    "objectID": "posts/Fastai_ch5/Fastai-Ch5.html",
    "href": "posts/Fastai_ch5/Fastai-Ch5.html",
    "title": "Chapter 5: Deep learning for coders with fastai and pytorch",
    "section": "",
    "text": "Since we now familiar the whole process of creating deep learning model, using pre-built model, building them from scratch, handling data, and putting these model into web apps, we will now to go deeper and keep focus on details that make model accurate and reliable.\nIt takes many tweaks and parameters changing in order to “polish” a model.\nIn order to achieve this goal we need to be familiar with many concepts and technics, different types of layers, regularization methods, optimizers, how to put layers together into architectures, labeling techniques, and much more.\n\n\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n     |████████████████████████████████| 719 kB 30.4 MB/s \n     |████████████████████████████████| 1.3 MB 60.3 MB/s \n     |████████████████████████████████| 5.3 MB 53.0 MB/s \n     |████████████████████████████████| 441 kB 66.2 MB/s \n     |████████████████████████████████| 1.6 MB 54.6 MB/s \n     |████████████████████████████████| 115 kB 75.5 MB/s \n     |████████████████████████████████| 163 kB 67.3 MB/s \n     |████████████████████████████████| 212 kB 71.5 MB/s \n     |████████████████████████████████| 127 kB 76.1 MB/s \n     |████████████████████████████████| 115 kB 62.1 MB/s \n     |████████████████████████████████| 7.6 MB 58.8 MB/s \nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n\n\nIn real world scenarios, the first thing we do is we get in contact with data, usualy at this phase we know nothing about the dataset. We then start to look how to extract the data we want from it, and what the data looks like, and how it is structured.\nUsually data is provided in one of two ways:\n\nIndividual files representing items of data, possibly organized into folder or with filenames representing information about those items\n\ntext documents\n\nimages\n\n\nA table of data in which each row is an item and may include filenames providing connections between the data in the table and data in other formats\n\nCSV files\n\n\n\nExceptions:\n\nDomains like Genomics\n\nbinary database formats\n\nnetwork streams\n\n\n\n# download the dataset\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:02<00:00]\n    \n    \n\n\n\n#get the path as variable, and see what inside\nPath.BASE_PATH = path\npath.ls()\n\n(#2) [Path('images'),Path('annotations')]\n\n\n\nAs we notice here, the data is provided with 2 directories:\n\nimages\nannotations\n\n\n\n#take a look at what inside the images directory\n(path/'images').ls()\n\n(#7393) [Path('images/german_shorthaired_87.jpg'),Path('images/Russian_Blue_200.jpg'),Path('images/Siamese_25.jpg'),Path('images/japanese_chin_48.jpg'),Path('images/miniature_pinscher_161.jpg'),Path('images/pug_150.jpg'),Path('images/pug_120.jpg'),Path('images/pug_63.jpg'),Path('images/samoyed_143.jpg'),Path('images/yorkshire_terrier_190.jpg')...]\n\n\n\nWhen we took a look at these names, we see some paterns: we already know from chapter1 that cats name are uppercase, here we see that after the breed’s name there is a (_) then a number, and finally the extension.\nThis may help us to write some code that extract the breed from a single Path.\n\n\n#pick one \nfname = (path/\"images\").ls()[0]\nfname\n\nPath('images/german_shorthaired_87.jpg')\n\n\n\nThe best way to work with strings and extract patterns from them is to use Regex, which stands for Regular Expression.\n\n\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n\n['german_shorthaired']\n\n\n\nNow we need to label the whole dataset using this code.\nFastai comes with many classes for labeling, in this case when we need to label with help of regex we could use RegexLabeller class within DataBlaock API.\n\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n\n\n\n\n\nFastai has this method of presizing the images in a way that conserve its quality after the data augmentation, so it helps the model to learn more lessons from data, and also it helps our dataset to be more varies\nThe idea behind the presizing is we crop the image and resize it to 460 by 460 first, which is a big size by deep learning norms, this operation is done on CPU, then we do the data augmentation in batches, by cropping a rotated random part of that 460^2 image, and taking the cropped image then resize again to a 224 by 224 image, all this operation are done on batch level, which mean on GPU.**\n\n\nitem_tfms=Resize(460),  \nbatch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n![REsize_method](1.png)\n\n* Usually all the augmentation operations we do on a image reduce the quality of the image, but with this approach we could say we can preserve big part of informations of that image so the model can learn better \n\n### Checking and Debugging a DataBlock\n\n* `DataBlock` is just a blueprint for orginizing data before we feed it to the model, you have no guarantee that your template is going to work on your data source as you intend.\n* So, before training a model you should always check your data. You can do this using the show_batch method:\n\n::: {.cell outputId='0f1711e2-f2c6-4703-ed64-1ea456f04a2d' execution_count=10}\n``` {.python .cell-code}\ndls.show_batch(nrows=1, ncols=4 ,unique= True)\n\n\n\n:::\n\nIn case we made a mistake in the process of creating datablock, we could use .summary to track the problem.Here we didn’t resize the images in one formm, so couldn’t use the batch transform\nAs we see here the .summary gives us precise diagnostic of the problem:**\n\nat least two tensors in the batch are not the same size.\n\n\n\n\n\nOnce we feel like the datablock is well created, we better begin train the model, and use it as a tool of cleaning the data. If there’s a problem with data or the model, we better know that before we lost lot of time and energy on data cleaning even before testing the model.\n\n\n# train the model\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.526362\n      0.348364\n      0.111637\n      01:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.482150\n      0.309586\n      0.093369\n      01:16\n    \n    \n      1\n      0.327810\n      0.229392\n      0.064953\n      01:17\n    \n  \n\n\n\n\n\n\n\nAs we saw before in Chapter 2 the best tool to clean the data is basically the model itself\nAfter creating the datablock and dataloader we better train the model and get some feedback so we know if something is wrong very early, and if not we start to use the model as tool to investigate the data\nUsually before we train the model we have to decide the function that will update the parameters, a Loss Function. But here we didn’t create any loss function?\nIf we didn’t decide the way by which we update the paramters, Fastai by default will chose a loss function for us.\n\nthe chosen loss function will suite the kind of model we build, and the type of dataset we have.\n\n\n\n# check the loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\nThe loss function used to train this model is Cross Entropy Loss\n\n\n\n\n\n\nThe Cross-Entropy Loss is a function similar to what we saw in the previous chapter, when we created the mnist_loss function:\n\ndef mnist_loss(predictions, targets):\n    prediction = prediction.sigmoid()\n    return torch.where(targets==1, 1-prediction, predictions).mean()\n\nThe problem with this function is, it only takes 2 categories(3, 7) but here we have 37 types of breeds.but here we have multiple classes.\n\nit can takes more than 2 categories\n\n\n\n\n\nIn order to understand the cross-entropy loss, let’s grab a batch of data\n\n\nx,y = dls.one_batch()\n\n\nIt return the activations of dependent and independent variable of one mini-batch\n\n\n# independent variable\ny, len(y)\n\n(TensorCategory([ 8, 34,  6, 33, 12, 15, 32, 14,  5, 27, 20, 32, 18, 24,  8, 29,  5,  4,  0, 28, 10, 12, 16, 25, 29,  3, 34, 27, 30, 15,  6, 15, 27, 34, 14, 21,  5, 17, 31, 26, 13, 35, 17, 35, 23, 14,\n                 35, 35,  8,  7, 21,  0, 22, 17, 19, 26, 16,  5, 15, 27, 11, 22, 34, 18], device='cuda:0'),\n 64)\n\n\n\nIt return 64 number, each represent on of the 37 breeds index\n\n\n# all indepent variables\ndls.vocab, len(dls.vocab)\n\n(['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier'],\n 37)\n\n\n\nHere we use get_preds to get the predictions for each image in the dataset or mini-batch (like in this example).\n\n\npreds,_ = learn.get_preds(dl=[(x,y)])\n\n\n\n\n\n\n\n\n\n# prediction for image [0] in the mini-batch\npreds[0]\n\nTensorBase([1.0919e-05, 6.0375e-08, 8.5680e-05, 7.1281e-07, 1.8754e-06, 4.7956e-07, 1.1708e-04, 1.4080e-06, 9.9972e-01, 9.4731e-07, 6.8318e-08, 1.7055e-07, 1.9338e-06, 7.7372e-07, 1.7931e-07,\n            1.5261e-07, 1.1363e-08, 2.3353e-07, 1.3689e-07, 4.5361e-07, 7.1548e-08, 4.9425e-06, 1.0026e-05, 2.6908e-07, 3.5724e-07, 3.1201e-07, 1.0177e-08, 2.7219e-08, 1.6157e-06, 3.5337e-08,\n            1.8094e-06, 3.3425e-05, 2.3514e-08, 3.0318e-06, 1.4910e-07, 1.9582e-07, 9.5147e-08])\n\n\n\nThe 37 predictions refer to the probability of each breed to match the image[0].\n\nif we sum() them up they add up to 1:\n\n\n\npreds[0].sum()\n\nTensorBase(1.0000)\n\n\n\nIn order to transform the activations of our model into prediction, we use Soft-Max\n\n\n\n\n\nAs we said before, Softmax is similar to sigmoid function we use before, but it only can handel more than 2 classes.\n\n\n# reminder of sigmoid\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\nThis function allow us to predict whether a activation number is pointing to each category of the two, by calculating which activation is big and by much. But in our case today we have 37 category, which means by this logic we need a activation for each one.\nFirst let’s create a similar situation where we have only 2 categories, but we won’t solve it as it’s a binary problem (it's 3) but as 2 categories problem, each has it’s activation, and their probabilty sum up to 1.\n\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\n\nWe can’t just take the sigmoid of this directly, since we don’t get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):\n\n\nsigmoid(acts)\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\n\nEevn though we try different approach to solve the same problem, we still have some similarities.\nWe will use sigmoid on each activation.\nAnd we still need to substract an activation from another beacuse that represent how much the model sure about an image is assigned to each category, thats for first column.\nIn the second colun we just use 1 - prediction (activation of the second column)\n\n\ndiffs = acts[:, 0] - acts[:, 1]\n\n\n# create the sigmoid function of both categories\nsigm_ver = torch.stack([diffs.sigmoid(), 1-diffs.sigmoid()], dim=1)\n\n\nWe can express the softmax function as:\n\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\n\nLet’s check that softmax returns the same values as sigmoid for the first column, and those values subtracted from 1 for the second column:\n\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts, sigm_ver\n\n(tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]), tensor([[0.6025, 0.3975],\n         [0.5021, 0.4979],\n         [0.1332, 0.8668],\n         [0.9966, 0.0034],\n         [0.5959, 0.4041],\n         [0.3661, 0.6339]]))\n\n\n\nSoftmax calculate the \\(exp^{x}\\) and divide it by sum \\(exp^{x}\\) of all activations of other categories.\n\nthe exp make sure the biggest activation is way bigger than others\ndividing by the sum is what make softmax values add up to 1\n\n\n\n\n\n\nIn the previous chapter when we created mnis_loss, we used torch.where to select between the input and 1-input.\nWith softmax, we will use indexing.\n\n\n# the pretended targets\ntargs = tensor([0, 1, 0, 1, 1, 0])\n\n\n# create an index\nidx = range(6)\n\n\n# the softmax activations\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nHere we make the targs decide which activation we pick in each row\n\n\nsm_acts[idx, targs]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\n\nlet’s display what we just did :\n\n\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targs'] = targs\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targs]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('</style>')[1]\nhtml = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\ndisplay(HTML(html))\n\n\n\n\n  \n    \n      3\n      7\n      targs\n      idx\n      result\n    \n  \n  \n    \n      0.602469\n      0.397531\n      0\n      0\n      0.602469\n    \n    \n      0.502065\n      0.497935\n      1\n      1\n      0.497935\n    \n    \n      0.133188\n      0.866811\n      0\n      2\n      0.133188\n    \n    \n      0.996640\n      0.003360\n      1\n      3\n      0.003360\n    \n    \n      0.595949\n      0.404051\n      1\n      4\n      0.404051\n    \n    \n      0.366118\n      0.633882\n      0\n      5\n      0.366118\n    \n  \n\n\n\n\nBut idea here is not to use it in a simple binary problem, because torch.where could did the same job here, but is to use it in order to solve a multi-categorie problem\n\nPyTorch provides a function that does exactly the same thing as sm_acts[range(n), targ] (except it takes the negative, because when applying the log afterward, we will have negative numbers), called nll_loss (NLL stands for negative log likelihood):\n\n-sm_acts[idx, targs]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targs, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\n\n\n\n\n\ncross-entropy-loss-function.png\n\n\n\nThe using of logarithms allow us to do all kind of multiplications without carring about the size of the output.\n\nthe nature of log functions make them increase lineary when the underlying signal increase exponentialy.\nlog(a*b) = log(a)+log(b)\nthe log of a number approaches negative infinity when the number approaches zero\n\nIn our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is “good” (closer to 1) and a large value when the prediction is “bad” (closer to 0).\nNotice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class. One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training.\n\nCalculating the loss pay attention only to the high softmax value.\n\n\n\n\n\nAfter taking the log of the softmax, we can then call the negative log likelihood.\n\nfirst : log_softmax\nthen : nll_loss\nor : nn.CrossEntropyLoss()\n\n\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targs)\n\ntensor(1.8045)\n\n\n\nnn.CrossEntropyLoss()(acts, targs)\n\ntensor(1.8045)\n\n\n\nThe nn.CrossEntrpyLoss() make do all the steps for us, but if we want to go through all those steps one by one softmas+log then negative log we could do it also:\n\n\nF.nll_loss(nn.Softmax()(acts).log(), targs,)\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\ntensor(1.8045)\n\n\n\nAdding the reduction='none' to this functions will return the loss of each row, if we didn’t add this aparameter the fuction will return the mean loss of all rows.\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\n\n\n\n\nAs we saw in chapter 3 it’s hard for us to interpret the loss function, since it’s some the computers use in order to updates the parameters and optimize the performance.\nBut we can use some kind of demonstration that shows where the model did good, and where did bad.\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIts was easy to understand what happened when they were only 3 classes in bears model, but here we have 37 breeds.\n\nthats why we will useinterp.most_confused(min_val=5) to output to most bad decisions the model taked\n\n\n\ninterp.most_confused(min_val=5)\n\n\n\n\n\n\n\n\n[('Ragdoll', 'Birman', 6)]\n\n\n\nThe best way to understand what happend is to google the names of each breed and see why the model confused it with the other breed, so we know that the model is in the right track\n\n\n\n\n\nAt this point all we can do is improve the model by correcting some detaills that may optimize the final prefromance\n\n\n\n\nOne way of improving our model is by picking the right learning rate.\n\nit will help to get faster result per epoch\nminimize the loss and updating parameters with less steps\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.615709\n      5.479418\n      0.529093\n      01:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      3.068122\n      1.406392\n      0.443843\n      01:15\n    \n  \n\n\n\n\nHere we pick a learning rate 0.1 which is 5 times bigger than the last one 0.002 and we get bad results error rate at: 0.5\n\nbig learning rate may reduce the computation needed for the training process but the model performance will be bad\n\nAlso if we pick a small learning rate it will take forever to achieve something.\nThe answear for this dilemma is The Learning Rate Finder\n\nFastai library adopte this method created by the resaercher Leslie Smith in a paper in 2015.\n\nthe idea of Smith is to start with a small learning rate (very small), and use it for one mini-batch, see how much the loss changed, and then start increasing the learning rate by some percentage (doubling it since its very small anyway)\nrepeate this process again(track the loss, double the learning rate ..) until the loss get worse.\nat this point we just pick a learning rate smaller than the one that causes the loss to get worse.\n\nFastai course advice is either:\n\none order of magnitude less than where the minimun loss was achieved(divide by 10)\nthe last point where the loss was clearly decreasing\n\nBoth point are giving the same value usually.\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n\nMinimum/10: 1.00e-02, steepest point: 4.37e-03\n\n\n\nThe plot shows that the loss between 10e-6 and 10e-3 almost didn’t change, but after it start to decrease until it reachs the minimum at 10e-1.\nWe don’t want a learning rate bigger than 10e-1 because there where the loss get worse, and we don’t need learning rate at 10e-1 because at this value we’ve left the stage where the loss was decreasing.\n\nwe need to pick the learning rate where the just start to decrease all the way to the minimum: 1e-3\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.327572\n      0.370063\n      0.120433\n      01:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.570779\n      0.429716\n      0.131935\n      01:15\n    \n    \n      1\n      0.322137\n      0.246614\n      0.073072\n      01:17\n    \n  \n\n\n\n\nThe error rate get better 10 times just by using the learning rate methode. Loss also get better by this percentage.\n\n\n\n\n\nWe are familiar with the idea of Transfer Learning, where we use a pretrainned model on our dataset, by fine tuning it in a way that keep all the learned weights and use them in our task.\nWe know tha Convolutional Neural Network consist of many linear layers, and between each two of them there’s a nonlinear activation function (ReLU for example), followed by the final layer with an activation function such as Softmax. The final layer uses a matrix with enough columns such that the ouput size is has the number of classes our model trained to predict(assuming we have a classfication task) This final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset.\nSo we first delete it when we start the transfer learning process, and replace it with a new linear layer with the correct number of outputs that matches our desired task(in this case 37 breeds, so 37 activations)\nThis new linear layer have total randome set of weights, but that doesn’t mean we should set all weights randomly even for the pretrained part.\n\nAll of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the Zeiler and Fergus paper, the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\n\nWe want to build a model such as preserve all the learned weights, and apply them on our dataset, so only adjust them as required for the specifics of our particular task.\nSo, the idea is to keep the pretrained part’s weights intact, and only update the weights of the added part. This process is called Freezing\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nOf Course this is just the default approach, fine_tune has many parameters that allow us to apply different tweaks for each specific situation.\nFor now, let’s do this process manually without using fine_tune\n\n\n# check fine_tune source acode\nlearn.fine_tune??\n\n\nFirst we create our learner from the dls and arch using vision_learner\n\nby default vision_learner will freeze the pre-trained part of the model (freeze the params)\n\nThen train the added layer with randome weights for number of epochs with a learning rate we pick\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.114612\n      0.429605\n      0.134641\n      01:10\n    \n    \n      1\n      0.538649\n      0.245115\n      0.083221\n      01:10\n    \n    \n      2\n      0.313570\n      0.207912\n      0.065629\n      01:10\n    \n  \n\n\n\n\nNow we need to unfreeze the model:\n\n\nlearn.unfreeze()\n\n\nNow we run lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn’t appropriate any more:\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.999999747378752e-06)\n\n\n\n\n\n\nAs we see here the graph is different than what we saw before when we use randome weights to train the model, because that the model has been trained already.\nThe approach to pick the right lr here is to chose a point before the sharp increase.\n\n\n34se3a\n\n\nlearn.fit_one_cycle(6 , lr_max=4.786300905834651e-06)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.260985\n      0.207057\n      0.062246\n      01:16\n    \n    \n      1\n      0.244476\n      0.201592\n      0.064276\n      01:18\n    \n    \n      2\n      0.240523\n      0.193317\n      0.058187\n      01:15\n    \n    \n      3\n      0.234724\n      0.189429\n      0.054127\n      01:15\n    \n    \n      4\n      0.227204\n      0.188406\n      0.056157\n      01:21\n    \n    \n      5\n      0.209963\n      0.187695\n      0.056157\n      01:16\n    \n  \n\n\n\n\n\n\n\nAfter training the model for 6 epochs we get eror_rate at 6% which is fine, but we could do better.\nThe thing we could optimize here is to rethink the learning rate again.\n\npicking one learning rate value for the whole neural network isn’t a good idea.\nthe model is consisted of 2 parts as we know:\n\nthe pre-trained part contained good parameters that has been trained for many epochs\nthe last layer which we trained ourself for not more than 10 (3+6)\n\nso idea here is we shouldn’t trait both parts as if they are the same by picking one learning rate for the whole model\ninstead we could go with a small lr value for the first part, then aplly a slightly bigger one for the last layer.\n\nThis technic is devloped by Jason Yosinski and his team. They shows in 2014 that with transfer learning, different layer should be trained at different speed. \n\nFastai adopt this idea by using slice, which is a built-in object that let you pass 2 values:\n\nthe first define the learning rate of the earlier layer\nthe second for the last layers\n\nThe layers in between will have learning rates that are multiplicatively equidistant throughout that range\n\nLet’s see this technic in action\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(14, lr_max=slice(1e-6,1e-4))\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.129876\n      0.383506\n      0.121786\n      01:10\n    \n    \n      1\n      0.516040\n      0.284697\n      0.092693\n      01:13\n    \n    \n      2\n      0.328486\n      0.217860\n      0.071042\n      01:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.269753\n      0.210582\n      0.071719\n      01:15\n    \n    \n      1\n      0.256654\n      0.203205\n      0.067659\n      01:17\n    \n    \n      2\n      0.245040\n      0.196284\n      0.066982\n      01:14\n    \n    \n      3\n      0.222485\n      0.197652\n      0.066306\n      01:14\n    \n    \n      4\n      0.186354\n      0.193144\n      0.062923\n      01:17\n    \n    \n      5\n      0.185777\n      0.189425\n      0.060217\n      01:15\n    \n    \n      6\n      0.150886\n      0.190105\n      0.060893\n      01:15\n    \n    \n      7\n      0.146768\n      0.186121\n      0.057510\n      01:18\n    \n    \n      8\n      0.134524\n      0.177772\n      0.054804\n      01:15\n    \n    \n      9\n      0.135853\n      0.180999\n      0.058187\n      01:15\n    \n    \n      10\n      0.127154\n      0.178239\n      0.056834\n      01:18\n    \n    \n      11\n      0.110540\n      0.179652\n      0.056157\n      01:15\n    \n    \n      12\n      0.122252\n      0.180609\n      0.056834\n      01:14\n    \n    \n      13\n      0.105743\n      0.180926\n      0.054804\n      01:17\n    \n  \n\n\n\n\nWe can plot the training and the validation loss\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nChoosing the right amount of epoch you will train the model on is also something we should address properly.\nWe need to keep eye on the train/val loss as shown above, but also on error rate (or any metric we pick).\nIf the loss and the netric are getting better significantly at the end of training, that’s mean we didn’t train for too long\nThe loss is just something we use to allow the optimizer to have something it can different and optimize, it’s not something we really should care about in practice.\n\nif the loss of the validation get worse at during the training because the model is getting over confident, only later it get worse because of overfitting, in practice we care only about the later issue\nIn case of overfitting, the easy solution is to retrain from scratch again, and this time select a total number of epochs based on where your previous best results were found\n\nIt’s not all about epochs, we could add more parameters to the model to get better result\n\n\n\n\n\nIn general, more parameters handle the date more accuratly.\nUsing a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error.\n\nThe way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=\n\nThe other downside of deeper architectures is that they take quite a bit longer to train.\n\nOne technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training.\nTo enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).\n\nYou can’t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let’s try a ResNet-50 now with mixed precision:\n\n\nfrom fastai.callback.fp16 import *\nlearn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.314968\n      0.331779\n      0.112314\n      01:07\n    \n    \n      1\n      0.600175\n      0.297889\n      0.089310\n      01:09\n    \n    \n      2\n      0.424932\n      0.264503\n      0.078484\n      01:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.294915\n      0.276637\n      0.076455\n      01:08\n    \n    \n      1\n      0.303050\n      0.266962\n      0.077131\n      01:10\n    \n    \n      2\n      0.241821\n      0.301895\n      0.086604\n      01:08\n    \n    \n      3\n      0.144625\n      0.222015\n      0.060217\n      01:08\n    \n    \n      4\n      0.082382\n      0.166509\n      0.056834\n      01:10\n    \n    \n      5\n      0.060593\n      0.161509\n      0.060893\n      01:08\n    \n  \n\n\n\n\nWe get better results, at less epochs, and less time per epochs only by usung deeper architecture.\n\nbut it’s allways better to start with small model, before scaling-up"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lifelong-Learner.github.io",
    "section": "",
    "text": "Fastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nNumpy\n\n\nPandas\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nNumpy\n\n\nPandas\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nHugginFace\n\n\nGradio\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFastai\n\n\nPytorch\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsmail TG\n\n\n\n\n\n\nNo matching items"
  }
]