{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "058e10b7-f4c4-4dd4-bdfb-13fb945e1bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-veredict.txt', <http.client.HTTPMessage at 0x7a22fc9ac800>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url =  (\"https://raw.githubusercontent.com/rasbt/\"\n",
    " \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    " \"the-verdict.txt\")\n",
    "file_path = \"the-veredict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2d991dd-3cc2-450a-958f-25170f7c7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset:\n",
    "import re\n",
    "with open(\"the-veredict.txt\", \"r\") as f:\n",
    "    txt_ds = f.read()\n",
    "\n",
    "# seperate ponctuation and others from words\n",
    "txt_ds = re.split(r'([,.;:\"!?\\'()_]|--|\\s)', txt_ds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d9c5afc-de7c-42e2-9dec-10c43cb88942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9235"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check lengthl\n",
    "len(txt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "107e2196-36de-47c3-a2c0-4c39190a0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git rid of whitespace\n",
    "txt_ds = [item.strip() for item in txt_ds if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de484c8a-f9b2-4816-bb1d-4cd4ee537a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check lengthl\n",
    "len(txt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bcb290c-15fd-4320-bda6-cae2da761d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the vocabulary and validate unique words only\n",
    "vocs = sorted(set(txt_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c0f618e-e96c-4080-820f-8f893302c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ID for each word\n",
    "vocs = {i: v for v, i in enumerate(txt_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02722da6-f8e8-444c-957f-3d4fe71b7f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87de0ecb-9497-4a06-a3fe-c0c1eaf738cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 4653)\n",
      "('HAD', 1)\n",
      "('always', 3820)\n",
      "('thought', 4585)\n",
      "('Jack', 2420)\n",
      "('Gisburn', 2923)\n",
      "('rather', 4398)\n",
      "('a', 4642)\n",
      "('cheap', 8)\n",
      "('genius', 9)\n",
      "('--', 4677)\n",
      "('though', 164)\n",
      "('good', 314)\n",
      "('fellow', 2496)\n",
      "('enough', 4625)\n",
      "('so', 4496)\n",
      "('it', 4665)\n",
      "('was', 4482)\n",
      "('no', 4682)\n",
      "('great', 4252)\n",
      "('surprise', 2734)\n",
      "('to', 4626)\n"
     ]
    }
   ],
   "source": [
    "# Print first 50 element\n",
    "for i, v in enumerate(vocs.items()):\n",
    "    print(v)\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f757ad9-638e-47cd-8ade-e20753faaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizerv1:\n",
    "    def __init__(self, vocs):\n",
    "        self.str_to_int = vocs\n",
    "        self.int_to_str = {k:v for v, k in vocs.items()}\n",
    "\n",
    "    def encode(self, txt):\n",
    "        pre = re.split(r'([,.;!?()_\"\\']|--|\\s)', txt)\n",
    "        pre = [item.strip() for item in pre if imtem.strip()]\n",
    "        ids = [self.str_to_int[s] for s in pre]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        txt = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        txt = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', txt)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54356b45-574a-4d87-863a-e55f91f0981c",
   "metadata": {},
   "source": [
    "* In **Version2** of the tokenizer we will add 2 more features:\n",
    "  - Replace unkown words that doesn't appear in the vocabulary with `<|unk|>` marker.\n",
    "  - Add special token `<|EOD|>` at the end of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7cbdf99-8c4c-4d81-a739-a83261008e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tok2:\n",
    "    def __init__(self, vocs):\n",
    "        self.str_to_int= vocs\n",
    "        self.int_to_str= {k:v for v, k in vocs.items()}\n",
    "    def encode(self, txt):\n",
    "        pre = re.split(r'([,.;:!?()_\"\\']\\s)', txt)\n",
    "        pre = [item.strip() for item in pre if item.strip()]\n",
    "        pre = [item if item in self.str_to_int else '|<UK>|' for item in pre]\n",
    "        ids = [self.str_to_int[s] for s in pre]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        txt = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        txt = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', txt)\n",
    "        return txt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a423e67e-cd5b-40ad-9fdb-4ec134f20750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi my name is Smail |<EOT>| here starts the new doc'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1 = 'Hi my name is Smail'\n",
    "txt2 = 'here starts the new doc'\n",
    "text = ' |<EOT>| '.join((txt1, txt2))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec1e0f6b-22af-4307-8a73-906e8b6cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "348e34de-a4fe-48f2-881d-d938f0454335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "\n",
    "                \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2705ea15-2f3e-4906-a832-c3c36a25484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaderv1(txt, batch_size=4, max_length=256, stride=128, shuffle= True,\n",
    "                drop_last= True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last= drop_last,\n",
    "                            num_workers= num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21a9c869-0059-4dec-bdb3-2696fd5ea36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-veredict.txt', 'r', encoding= 'utf-8') as f:\n",
    "    raw_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bd45802-5ee0-4426-bfd6-4867ee3940ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader= dataloaderv1(raw_txt, batch_size=1, max_length=4, stride= 1, shuffle= False)\n",
    "data_iter = iter(dloader)\n",
    "a_batch= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6fb9769-82d0-4cc8-8503-edd287889fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f915f41-4086-4a4d-8458-fb964f97e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dataloaderv1(raw_txt, batch_size=8, max_length=4, stride=4, shuffle= False)\n",
    "data_iter = iter(dataloaders)\n",
    "inps, targs= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8194611c-f895-47e9-9709-aa8583c58600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   40,   367,  2885,  1464],\n",
       "         [ 1807,  3619,   402,   271],\n",
       "         [10899,  2138,   257,  7026],\n",
       "         [15632,   438,  2016,   257],\n",
       "         [  922,  5891,  1576,   438],\n",
       "         [  568,   340,   373,   645],\n",
       "         [ 1049,  5975,   284,   502],\n",
       "         [  284,  3285,   326,    11]]),\n",
       " tensor([[  367,  2885,  1464,  1807],\n",
       "         [ 3619,   402,   271, 10899],\n",
       "         [ 2138,   257,  7026, 15632],\n",
       "         [  438,  2016,   257,   922],\n",
       "         [ 5891,  1576,   438,   568],\n",
       "         [  340,   373,   645,  1049],\n",
       "         [ 5975,   284,   502,   284],\n",
       "         [ 3285,   326,    11,   287]]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps, targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d235fa-d7d1-4476-863a-1a87c13430f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
