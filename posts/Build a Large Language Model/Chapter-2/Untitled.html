<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ismail TG">
<meta name="dcterms.date" content="2024-11-16">

<title>Chapter 2: Build a Large Language Model – Ismail's Personal Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Ismail’s Personal Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ismaai008l"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ismailTG3"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 2: Build a Large Language Model</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">Deep Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Ismail TG </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 16, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#about-this-chapter" id="toc-about-this-chapter" class="nav-link active" data-scroll-target="#about-this-chapter">About this Chapter:</a>
  <ul class="collapse">
  <li><a href="#understanding-word-embeddings" id="toc-understanding-word-embeddings" class="nav-link" data-scroll-target="#understanding-word-embeddings">Understanding word embeddings:</a></li>
  <li><a href="#tokenizing-text" id="toc-tokenizing-text" class="nav-link" data-scroll-target="#tokenizing-text">Tokenizing text:</a></li>
  <li><a href="#converting-tokens-into-token-ids" id="toc-converting-tokens-into-token-ids" class="nav-link" data-scroll-target="#converting-tokens-into-token-ids">Converting tokens into token IDs:</a></li>
  <li><a href="#adding-special-context-tokens" id="toc-adding-special-context-tokens" class="nav-link" data-scroll-target="#adding-special-context-tokens">Adding special context tokens:</a></li>
  <li><a href="#byte-pair-encoding" id="toc-byte-pair-encoding" class="nav-link" data-scroll-target="#byte-pair-encoding">Byte pair Encoding:</a></li>
  <li><a href="#data-sampling-with-a-sliding-window" id="toc-data-sampling-with-a-sliding-window" class="nav-link" data-scroll-target="#data-sampling-with-a-sliding-window">Data sampling with a sliding window:</a></li>
  <li><a href="#creating-token-embeddings" id="toc-creating-token-embeddings" class="nav-link" data-scroll-target="#creating-token-embeddings">Creating token embeddings:</a></li>
  <li><a href="#encoding-word-positions" id="toc-encoding-word-positions" class="nav-link" data-scroll-target="#encoding-word-positions">Encoding word positions:</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#about-this-chapter" id="toc-about-this-chapter">About this Chapter:</a>
  <ul>
  <li><a href="#understanding-word-embeddings" id="toc-understanding-word-embeddings">Understanding word embeddings:</a></li>
  <li><a href="#tokenizing-text" id="toc-tokenizing-text">Tokenizing text:</a></li>
  <li><a href="#converting-tokens-into-token-ids" id="toc-converting-tokens-into-token-ids">Converting tokens into token IDs:</a></li>
  <li><a href="#adding-special-context-tokens" id="toc-adding-special-context-tokens">Adding special context tokens:</a></li>
  <li><a href="#byte-pair-encoding" id="toc-byte-pair-encoding">Byte pair Encoding:</a></li>
  <li><a href="#data-sampling-with-a-sliding-window" id="toc-data-sampling-with-a-sliding-window">Data sampling with a sliding window:</a></li>
  <li><a href="#creating-token-embeddings" id="toc-creating-token-embeddings">Creating token embeddings:</a></li>
  <li><a href="#encoding-word-positions" id="toc-encoding-word-positions">Encoding word positions:</a></li>
  </ul></li>
  </ul>
</nav>
<section id="about-this-chapter" class="level2">
<h2 class="anchored" data-anchor-id="about-this-chapter">About this Chapter:</h2>
<ul>
<li>In order to build an LLM we need to provide a very large chunk of text.</li>
<li>In this chapter we will discuss how to prepare the text-dataset and feed it to LLM, and various technics and methods used in data preparation context.</li>
</ul>
<section id="understanding-word-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="understanding-word-embeddings">Understanding word embeddings:</h3>
<ul>
<li>LLM cannot perform any kind of computation on raw text, it can only work with numbers.</li>
<li>Therefor we need to apply some kind of numerical transformation to all input text.</li>
<li>This transformation called <strong>Embedding</strong> and the numerical representation of each word is <strong>Vector</strong>.</li>
<li>The embedding process work also on other data format like <em>Audio</em> <em>Video</em> .. but each type has its own embedding model. <img src="embeddings.png" class="img-fluid" alt="Embedding Layer with Different types of Data"></li>
<li>Many algorithms have been developed to produce embeddings for words.</li>
<li>The famous one was <em>World2Vec</em>.
<ul>
<li>Its approach was to train Neural Network on predicting embedding of a given word based on its context, and Vice-Versa.</li>
<li>The main idea here is that words that have appear in similar context atend to have similar meaning which also efect their embedding.</li>
<li>So if these words are projected in two dimensional embedding they will be appeared in clusters. <img src="two-dimensional.png" class="img-fluid" alt="clusters"></li>
</ul></li>
<li>In current LLM embeddings are way more larger and have higher dimensionality.
<ul>
<li>For example <strong>GPT-2</strong> (117 Millions Parameters) used embeddings of <code>768</code> dimensions, where the largest <strong>GPT-3</strong> (117 Billions Parameters) uses <code>12.288</code>.</li>
<li>Each word will be projected to 12.288 dimensions.</li>
</ul></li>
</ul>
</section>
<section id="tokenizing-text" class="level3">
<h3 class="anchored" data-anchor-id="tokenizing-text">Tokenizing text:</h3>
<ul>
<li><p><strong>Breaking Text into Units</strong>: Tokenization splits text into smaller pieces called <strong>tokens</strong> (words, subwords, or characters), which are the building blocks the model uses to process and understand language.</p></li>
<li><p><strong>Mapping Tokens to Numbers</strong>: Each token is assigned a unique numerical identifier, allowing the LLM to work with numbers instead of raw text during computations.</p></li>
<li><p><strong>Efficient Representation</strong>: The way text is tokenized affects how efficiently the LLM processes input and generates output, balancing between <strong>accuracy</strong> (preserving meaning) and <strong>memory usage</strong> (fewer tokens).</p></li>
<li><p>Here we will work with raw text called <strong>The Veredict</strong> and apply some kind of tokenization on it:</p></li>
</ul>
<div id="ab1c02e1-3236-46dd-9ad1-3ad8a9309e2b" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span>  (<span class="st">"https://raw.githubusercontent.com/rasbt/"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> <span class="st">"LLMs-from-scratch/main/ch02/01_main-chapter-code/"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a> <span class="st">"the-verdict.txt"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> <span class="st">"the-veredict.txt"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(url, file_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>('the-veredict.txt', &lt;http.client.HTTPMessage at 0x705fec0466c0&gt;)</code></pre>
</div>
</div>
<ul>
<li>Open the data and check its length (how many chars):
<ul>
<li>the text contains <code>20479</code> charaters:</li>
</ul></li>
</ul>
<div id="31747301-4352-4ac1-a30d-0719f7c4b865" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span> (<span class="st">"the-veredict.txt"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span> <span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    raw_text <span class="op">=</span> f.read()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"total number of characters: </span><span class="sc">{</span><span class="bu">len</span>(raw_text)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total number of characters: 20479</code></pre>
</div>
</div>
<ul>
<li>Take a look on first <code>100 character</code>:</li>
</ul>
<div id="4d456a1a-5ea0-4144-88fb-54cbb2c68a11" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>raw_text[:<span class="dv">99</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '</code></pre>
</div>
</div>
<ul>
<li>The goal is to tokenize the <code>20479</code> charaters in order to turn theem into embeddings.</li>
<li>We will <code>Regular Expression</code> as a tool to tokenize the text, just to understand the idea of tokenization since it is not the right tool for this for our context is great.</li>
<li>lets take this simple text:</li>
</ul>
<div id="f4ebbbf5-d60c-4082-a0d1-91a8c2b88ee4" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>simple_text <span class="op">=</span> <span class="st">'Hi there! how was your day?'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Now lets split it to small pieces (words) with spaces <code>" "</code>:</li>
</ul>
<div id="71e8d37d-10e8-4094-9bba-69de3a83d307" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> re.split(<span class="vs">r"\s"</span>, simple_text)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>tokenz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>['Hi', 'there!', 'how', 'was', 'your', 'day?']</code></pre>
</div>
</div>
<ul>
<li>We need to make sur to seperate ponctuations from words:</li>
</ul>
<div id="e1dfad5c-85df-4a03-93cf-7e2ae1cdb5c2" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> re.split(<span class="vs">r'([!?]|\s)'</span>, simple_text)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>tokenz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>['Hi',
 ' ',
 'there',
 '!',
 '',
 ' ',
 'how',
 ' ',
 'was',
 ' ',
 'your',
 ' ',
 'day',
 '?',
 '']</code></pre>
</div>
</div>
<ul>
<li>Remove whitespaces:</li>
</ul>
<div id="b895703b-f483-474d-9f5a-3e3034a4eece" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> tokenz <span class="cf">if</span> word.strip()]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>['Hi', 'there', '!', 'how', 'was', 'your', 'day', '?']</code></pre>
</div>
</div>
<ul>
<li>Now lets add more special charatcters and ponctuations such as question marks, quotation marks, and the double-dashes :</li>
</ul>
<div id="cc078d85-64cd-45d8-bec2-df5bc509f16d" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> re.split(<span class="vs">r'([,.:!?_;"()\']|--|\s)'</span>,simple_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Now lets apply this simple tokenizer on our <code>raw_text</code>:</li>
</ul>
<div id="730396b5-7308-40a0-a8b7-051bf15c99a7" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> re.split(<span class="vs">r'([.,:;()_!?"\']|--|\s)'</span>, raw_text)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> [word.strip() <span class="cf">for</span> word <span class="kw">in</span> tokenz <span class="cf">if</span> word.strip()]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(tokenz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>4690</code></pre>
</div>
</div>
</section>
<section id="converting-tokens-into-token-ids" class="level3">
<h3 class="anchored" data-anchor-id="converting-tokens-into-token-ids">Converting tokens into token IDs:</h3>
<ul>
<li>Since we now have tokenz we could transform all <code>str</code> words into <code>int</code> ID.</li>
<li>First we need to build vocabulary for all words we have in our <code>tokenz</code> before converting token ID into embeddings. <strong>Image</strong></li>
</ul>
<div id="afd32ee7-b610-4f00-adba-22a378799e57" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(tokenz))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(vocabulary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>1130</code></pre>
</div>
</div>
<ul>
<li>After sorting the vocabulary and getting rid of all repeated words we have <code>1130</code> unique token in our vovabulary.</li>
<li>Here we assign an <code>int</code> for each vocabulary element:</li>
</ul>
<div id="a912c20c-83de-4bf2-b610-7b8357b0cd71" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {tok: integer <span class="cf">for</span> integer, tok <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5808f213-a779-4b33-8f68-821ded83c058" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v, i <span class="kw">in</span> vocab.items():</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">50</span>:</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0:!
1:"
2:'
3:(
4:)
5:,
6:--
7:.
8::
9:;
10:?
11:A
12:Ah
13:Among
14:And
15:Are
16:Arrt
17:As
18:At
19:Be
20:Begin
21:Burlington
22:But
23:By
24:Carlo
25:Chicago
26:Claude
27:Come
28:Croft
29:Destroyed
30:Devonshire
31:Don
32:Dubarry
33:Emperors
34:Florence
35:For
36:Gallery
37:Gideon
38:Gisburn
39:Gisburns
40:Grafton
41:Greek
42:Grindle
43:Grindles
44:HAD
45:Had
46:Hang
47:Has
48:He
49:Her</code></pre>
</div>
</div>
<ul>
<li>Now we have create a simple tokenizer class that handles encoding and decoding of text based on a given vocabulary:
<ul>
<li>Takes a <code>vocab</code> dictionary as a class attribute</li>
<li><code>self.str_to_int</code>: Stores the vocabulary mapping from string to integer.</li>
<li><code>self.int_to_str</code>: Creates a reverse dictionary that maps integers back to strings.</li>
</ul></li>
<li><strong><code>encode</code> Method:</strong>
<ul>
<li>Converts text into a list of integer IDs based on the vocabulary.</li>
<li>Splits the text into words and punctuation while preserving punctuation as separate items.</li>
<li>Removes leading/trailing spaces from each item and filters out empty strings.</li>
<li>Converts the processed words/punctuation into a list of corresponding integer IDs using the <code>str_to_int</code> dictionary.</li>
<li>Returns the list of IDs.</li>
</ul></li>
<li><strong><code>decode</code> Method:</strong>
<ul>
<li>Converts a list of integer IDs back into text.</li>
<li>Converts the list of integer IDs into corresponding words/punctuation using the <code>int_to_str</code> dictionary and joins them into a single string with spaces between words.</li>
<li>Cleans up the text by removing spaces before punctuation marks using a regular expression.</li>
<li>Returns the decoded text with properly placed punctuation.</li>
</ul></li>
</ul>
<div id="d676729d-2ea7-45b8-8df1-d55556af0177" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTokenizerV1:</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.str_to_int <span class="op">=</span> vocab</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.int_to_str <span class="op">=</span> {i:s <span class="cf">for</span> s,i <span class="kw">in</span> vocab.items()}</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        preprocessed <span class="op">=</span> re.split(<span class="vs">r'([,.?_!"()\']|--|\s)'</span>, text)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        preprocessed <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> preprocessed <span class="cf">if</span> item.strip()]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> [<span class="va">self</span>.str_to_int[s] <span class="cf">for</span> s <span class="kw">in</span> preprocessed]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join([<span class="va">self</span>.int_to_str[i] <span class="cf">for</span> i <span class="kw">in</span> ids])</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r'\s+([,.?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Now we could instentiate a tokenizer object from the class <code>SimpleTokenizerV1</code> and tokenize a passage from our raw text:</li>
</ul>
<div id="6c9411d6-c06e-40da-a3bc-6905bf31d2ec" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> SimpleTokenizerV1(vocab)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">""""It's the last he painted, you know,"</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="st"> Mrs. Gisburn said with pardonable pride."""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="af033886-2db0-4115-ad3c-0a627d8ce443" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>[1,
 56,
 2,
 850,
 988,
 602,
 533,
 746,
 5,
 1126,
 596,
 5,
 1,
 67,
 7,
 38,
 851,
 1108,
 754,
 793,
 7]</code></pre>
</div>
</div>
<ul>
<li>we can also get pass this ids into <code>decode()</code> method and get back the original text:</li>
</ul>
<div id="8e7822c2-2f26-4ab6-a206-bd3b5c409c52" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> tokenizer.decode(ids)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>'" It\' s the last he painted, you know," Mrs. Gisburn said with pardonable pride.'</code></pre>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Untitled_files/figure-html/5d77e725-72c1-41cf-8f16-77b6ffcf9878-1-29868ee0-2f23-49d7-a41f-70bc6085cf5b.png" class="img-fluid figure-img"></p>
<figcaption>decoder_encode.png</figcaption>
</figure>
</div>
</section>
<section id="adding-special-context-tokens" class="level3">
<h3 class="anchored" data-anchor-id="adding-special-context-tokens">Adding special context tokens:</h3>
<ul>
<li>We need to modeify the tokenizer to
<ul>
<li>accept <code>Uknown words</code>.*</li>
<li>use <code>special tokens</code> to handel markers of thes unkown words abd document boundaries.</li>
</ul></li>
<li>If the tokenizer encounter words that doesn’t belong to the vocabulary will use <code>&lt;|Unk|&gt;</code> as marker for it.</li>
<li>We can also specify bondaries between documents that help the model during training to understand where the document ends and where begins.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Untitled_files/figure-html/118e39e1-d03c-4f06-8bc7-03e6625c5576-1-f877fa4a-5855-45ca-ac1d-06af912f0c30.png" class="img-fluid figure-img"></p>
<figcaption>end_of_text.png</figcaption>
</figure>
</div>
<ul>
<li>Lets modify the vocabulary to incorporate the <code>unkown</code> and <code>endoftext</code> markers:</li>
</ul>
<div id="c8fb7bd1-7ff9-426f-908d-903d245d5328" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create vocabulry with unique words</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>all_tokenz <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(tokenz))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># add EOT and UK to the vocabulary</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>all_tokenz.extend([<span class="st">"&lt;|EOT|&gt;"</span>, <span class="st">"&lt;|UK|&gt;"</span>])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># give each unique vocabulary an ID</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {tokens: integers <span class="cf">for</span> integers, tokens <span class="kw">in</span> <span class="bu">enumerate</span>(all_tokenz)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Check the last to items we just added:</li>
</ul>
<div id="ee747152-d371-4120-b5c3-bd153e4ba371" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, item <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(vocab.items())[<span class="op">-</span><span class="dv">2</span>:]):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>('&lt;|EOT|&gt;', 1130)
('&lt;|UK|&gt;', 1131)</code></pre>
</div>
</div>
<ul>
<li>Lets now re-build the tokenizer class adding this two features:</li>
</ul>
<div id="73156d1f-5029-4065-a139-371c562f9f31" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTokenizerV2:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.str_to_int <span class="op">=</span> vocab</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.int_to_str <span class="op">=</span> {i: v <span class="cf">for</span> v, i <span class="kw">in</span> vocab.items()}</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split each word from [,."'?!();]</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        preprocessed <span class="op">=</span> re.split(<span class="vs">r'([,."!()?;\']|--|\s)'</span>, text)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cut white spaces from each item in preprocessed</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        preprocessed <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> preprocessed <span class="cf">if</span> item.strip()]</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if the token isn't in the vocab mark it as uknown</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        preprocessed <span class="op">=</span> [item <span class="cf">if</span> item <span class="kw">in</span> <span class="va">self</span>.str_to_int <span class="cf">else</span> <span class="st">"&lt;|UK|&gt;"</span> <span class="cf">for</span> item <span class="kw">in</span> preprocessed]</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> [<span class="va">self</span>.str_to_int[s] <span class="cf">for</span> s <span class="kw">in</span> preprocessed]</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add space after each element in int_to_str</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join([<span class="va">self</span>.int_to_str[i] <span class="cf">for</span> i <span class="kw">in</span> ids])</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply some regulare expression magic </span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span>  text <span class="op">=</span> re.sub(<span class="vs">r'\s+([,.:;?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Lets apply the new version of the tokenizer on some text:</li>
</ul>
<div id="5c80e8fc-63a2-4bd0-8c93-ad373ee09ef9" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>txt1 <span class="op">=</span> <span class="st">"Hello, my name is Ismail!"</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>txt2 <span class="op">=</span> <span class="st">"I like to drink Tea."</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>txts <span class="op">=</span> <span class="st">" &lt;|EOT|&gt; "</span>.join((txt1, txt2))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>txts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>'Hello, my name is Ismail! &lt;|EOT|&gt; I like to drink Tea.'</code></pre>
</div>
</div>
<ul>
<li>Now we tokenize this example:</li>
</ul>
<div id="bc902968-1838-4391-85fb-aec9245128b8" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> SimpleTokenizerV2(vocab)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>tokenizer.encode(txts)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>[1131, 5, 697, 1131, 584, 1131, 0, 1130, 53, 628, 1016, 1131, 1131, 7]</code></pre>
</div>
</div>
<ul>
<li>As we see the <code>&lt;|EOT|&gt;</code> token’s ID is <code>1130</code> as axpected.</li>
<li>The list of ids also contains <code>1131</code> which represent unkown words.</li>
<li>We can detokenize the sentences and see them:</li>
</ul>
<div id="df073988-f47c-4a60-9be7-d7cca5ad4094" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>tokenized <span class="op">=</span> tokenizer.encode(txts)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>detokenized <span class="op">=</span> tokenizer.decode(tokenized)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>detokenized</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>'&lt;|UK|&gt;, my &lt;|UK|&gt; is &lt;|UK|&gt;! &lt;|EOT|&gt; I like to &lt;|UK|&gt; &lt;|UK|&gt;.'</code></pre>
</div>
</div>
<ul>
<li>there is more special tokenz we could add to our raw text dataset:
<ul>
<li><code>EOS</code> <strong>End-of-Sentence</strong> similar to <code>EOT</code> it indicate where the sentence begin which allo the llm to learn more the sturcture of the dataset.</li>
<li><code>BOS</code> <strong>Begin-Of-Sentence</strong> works along with <code>EOS</code> to <em>containerize</em> each sentence and force llm to learn boundaries between sentences.</li>
<li><code>PAD</code> <strong>Padding</strong> is added to short sentences in odrer to make sure all sentences has the same size length.</li>
</ul></li>
<li>The origional <strong>GPT</strong> model doesn’t use any of these special tokens except <code>EOT</code> including the <code>UN</code> toekn for out-of-vocabulary marker since it uses <em>bytes-pair-encoding</em>.</li>
</ul>
</section>
<section id="byte-pair-encoding" class="level3">
<h3 class="anchored" data-anchor-id="byte-pair-encoding">Byte pair Encoding:</h3>
<ul>
<li>The <strong>Byte-pair-encoding</strong> <strong><code>BPE</code></strong> is a technique used in the tokenization schema for training most GPT models.</li>
<li>We will use a library called <code>Tiktoken</code> to tokenize our input text in much effiecent way that our <code>TokenizeV2</code> class.</li>
</ul>
<div id="f39663f4-83a2-42d2-9a41-57ed1abe9c84" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> importlib.metadata <span class="im">import</span> version</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"tiktoken version:"</span>, version(<span class="st">"tiktoken"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tiktoken version: 0.8.0</code></pre>
</div>
</div>
<ul>
<li>Instantiate the tokenizer using the schema of <strong>GPT</strong>:</li>
</ul>
<div id="85297e2a-89b6-4f91-ae2c-1adf805239a8" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> tiktoken.get_encoding(<span class="st">'gpt2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="194a8bc7-2e28-40b9-81c6-19b215240d15" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"hi my name is Ismail &lt;|endoftext|&gt; I like the rainy weather someweirdsyntax!"</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>tokenz <span class="op">=</span> tokenizer.encode(text, allowed_special<span class="op">=</span>{<span class="st">"&lt;|endoftext|&gt;"</span>})</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>tokenz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>[5303,
 616,
 1438,
 318,
 1148,
 4529,
 220,
 50256,
 314,
 588,
 262,
 37259,
 6193,
 4209,
 68,
 1447,
 1837,
 41641,
 0]</code></pre>
</div>
</div>
<ul>
<li>Now decode the tokenz into words again</li>
</ul>
<div id="998ef1d3-9eb9-4473-b992-1e07a70f812e" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> tokenizer.decode(tokenz)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>'hi my name is Ismail &lt;|endoftext|&gt; I like the rainy weather someweirdsyntax!'</code></pre>
</div>
</div>
<ul>
<li>The <code>&lt;|endoftext|&gt;</code> is assigned to a large id: <code>50256</code>.
<ul>
<li><strong>GPT2</strong> vocabulary has <strong>5027</strong> so basically <code>&lt;|endoftext|&gt;</code> is assinged to the largest id.</li>
</ul></li>
<li>The tiktoken handels <code>someweirdsyntax</code> just like any other word.</li>
<li>The tiktoken library handles larger or unknown words by breaking them down into subwords or smaller tokens. This process is based on the Byte Pair Encoding (BPE) algorithm, which tiktoken uses.
<ul>
<li><p>For large words: The word is split into smaller, frequently occurring subword units (e.g., “transformation” might become “transform” and “ation”). These subword units are part of the tokenizer’s vocabulary.</p></li>
<li><p>For unknown words: If a word or sequence is not in the vocabulary, the tokenizer falls back to encoding it as a sequence of smaller tokens, often down to individual characters or byte-level tokens if necessary.</p></li>
</ul></li>
<li>This approach ensures that any input can be tokenized, even if the word is rare or entirely unseen</li>
</ul>
<div id="37b8631a-1502-4bf3-b227-f401f554e86c" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>weird_words <span class="op">=</span> [<span class="st">'wakapondiom'</span>, <span class="st">'jumanymasoodi'</span>, <span class="st">'kumarytifor'</span>]</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>int_list <span class="op">=</span> [tokenizer.encode(word) <span class="cf">for</span> word <span class="kw">in</span> words]</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>word1 <span class="op">=</span> tokenizer.encode(weird_words[<span class="dv">0</span>])</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>word2 <span class="op">=</span> tokenizer.encode(weird_words[<span class="dv">1</span>])</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>word3 <span class="op">=</span> tokenizer.encode(weird_words[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a3647166-64dc-46d8-8201-613e8e5b326c" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>word1, word2, word3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>([86, 461, 499, 623, 29005],
 [73, 388, 1092, 5356, 702, 72],
 [74, 388, 560, 49929, 273])</code></pre>
</div>
</div>
</section>
<section id="data-sampling-with-a-sliding-window" class="level3">
<h3 class="anchored" data-anchor-id="data-sampling-with-a-sliding-window">Data sampling with a sliding window:</h3>
<ul>
<li>For Language Modeling (Causal or Autoregressive Models)
<ul>
<li><strong>Input</strong>: A sequence of tokens (e.g., a sentence or text passage).</li>
<li><strong>Target</strong>: The next token(s) in the sequence.</li>
</ul></li>
<li>Example:
<ul>
<li>Input: “The cat sat on the”</li>
<li>Target: “mat”</li>
<li>The model learns to predict the next word or token based on the given input.</li>
</ul></li>
<li>Lets implement dataloader that fetches the input-target pairs from the dataset using the sliding window approch:
<ul>
<li>first tokenize the dataset using <code>BPE</code>:</li>
</ul></li>
</ul>
<div id="c20eb023-39c9-41cc-ba72-a150e24c71c8" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>encode_text <span class="op">=</span> tokenizer.encode(raw_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="24a4ea6c-5d6c-42e1-b630-5b0253143c40" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># slice out the first 50 data points</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>smpl_text <span class="op">=</span> encode_text[<span class="dv">50</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Here we create the input-target window by assigning <code>x</code> to the <strong>current/input</strong> token and <code>y</code> to the <strong>next/target</strong> token:</li>
</ul>
<div id="78b63332-10e0-4f7a-8353-b21a6badcdd2" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First creat context window:</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>context_window <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the input tokens:</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> smpl_text[:context_window]</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the next token to predict:</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> smpl_text[<span class="dv">1</span>: context_window<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"     </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[290, 4920, 2241, 287]
     [4920, 2241, 287, 257]</code></pre>
</div>
</div>
<ul>
<li>After processing the current token the llm will shift to the next token untill the last one:</li>
</ul>
<div id="65123c3b-bc4a-448f-9608-14b91b45d562" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,context_window <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> smpl_text[:i]</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    desired <span class="op">=</span> smpl_text[i]</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(current, <span class="st">'---&gt;'</span>, desired)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[290] ---&gt; 4920
[290, 4920] ---&gt; 2241
[290, 4920, 2241] ---&gt; 287
[290, 4920, 2241, 287] ---&gt; 257</code></pre>
</div>
</div>
<ul>
<li>Convert the same tokens into words again using the same method:</li>
</ul>
<div id="62b8cf56-4392-434c-9f9f-c45eec9c7473" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, context_window<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> smpl_text[:i]</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    desired <span class="op">=</span> smpl_text[i]</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(current)<span class="sc">}</span><span class="ss"> ----&gt; </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode([desired])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> and ----&gt;  established
 and established ----&gt;  himself
 and established himself ----&gt;  in
 and established himself in ----&gt;  a</code></pre>
</div>
</div>
<ul>
<li>Now we need to implement efficient data loader that iterate over dataset and returns pairs of <strong><code>Input --&gt; Target</code></strong> pairs tokens as <strong>Tensors</strong>. <img src="Untitled_files/figure-html/b5a840c9-191a-4c70-95a0-cc56ead149f2-1-a1006025-7c34-4675-982a-d429efeb1983.png" class="img-fluid" alt="dataLoader.png"></li>
<li>We will built-in <strong>Pytorch</strong> <strong>Dataset</strong> &amp; <strong>DataLoader</strong> classes:</li>
</ul>
<div id="e0b86098-22a9-4438-8f48-1c2726fa5e47" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="52d003a5-09a1-46cf-ad87-517bd9ea1a68" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTDatasetV1(Dataset):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, txt, tokenizer, max_length, stride):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_ids <span class="op">=</span> []</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_ids <span class="op">=</span> []</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        token_ids <span class="op">=</span> tokenizer.encode(txt)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(token_ids)<span class="op">-</span>max_length, stride):</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>            input_chunk <span class="op">=</span> token_ids[i:i <span class="op">+</span> max_length]</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>            target_chunk <span class="op">=</span> token_ids[i<span class="op">+</span><span class="dv">1</span>: i<span class="op">+</span>max_length<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_ids.append(torch.tensor(input_chunk))</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.target_ids.append(torch.tensor(target_chunk))</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.input_ids)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.input_ids[idx], <span class="va">self</span>.target_ids[idx]</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>                </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p><strong>Parameters</strong>:</p>
<ul>
<li><code>txt</code>: A large text sequence to process (e.g., the dataset for training).</li>
<li><code>tokenizer</code>: A tokenizer object that encodes text into token IDs (e.g., Hugging Face tokenizer).</li>
<li><code>max_length</code>: The maximum length of each input sequence for the model.</li>
<li><code>stride</code>: The step size for creating overlapping sequences.</li>
</ul></li>
<li><p><strong>Attributes</strong>:</p>
<ul>
<li><code>self.input_ids</code>: Stores the input token sequences for the model.</li>
<li><code>self.target_ids</code>: Stores the corresponding target sequences (shifted version of input for next-token prediction).</li>
</ul></li>
<li><p><strong>Tokenization</strong>:</p>
<ul>
<li><code>tokenizer.encode(txt)</code> converts the input text into a list of token IDs (<code>token_ids</code>).</li>
</ul></li>
<li><p><strong>Sliding Window with Stride</strong>:</p>
<ul>
<li><code>range(0, len(token_ids) - max_length, stride)</code> iterates through the tokenized text in steps of <code>stride</code>.</li>
<li>At each step:
<ul>
<li><strong>Input Chunk</strong>: <code>token_ids[i:i + max_length]</code> selects <code>max_length</code> tokens starting from index <code>i</code>.</li>
<li><strong>Target Chunk</strong>: <code>token_ids[i + 1: i + max_length + 1]</code> selects the next <code>max_length</code> tokens (input shifted by 1 token for next-token prediction).</li>
</ul></li>
</ul></li>
<li><p><strong>Appending Chunks</strong>:</p>
<ul>
<li>The input and target chunks are converted into PyTorch tensors and appended to <code>self.input_ids</code> and <code>self.target_ids</code>, respectively.</li>
</ul></li>
<li><p>Returns the number of input-target pairs in the dataset.</p></li>
<li><p>Takes an index <code>idx</code> and returns the corresponding <code>input_ids</code> and <code>target_ids</code> as a tuple.</p></li>
<li><p>This allows the dataset to be indexed like a list.</p></li>
<li><p>This dataset would create input-target pairs for training a GPT model, ensuring that the model can learn from overlapping sequences in the text.</p></li>
</ul>
<div id="2f5ca8d2-2516-4b25-8174-8fcf59cd569d" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> DataLoaderGPTV1(txt, batch_size<span class="op">=</span><span class="dv">4</span>, max_length<span class="op">=</span><span class="dv">256</span>, </span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span> <span class="va">True</span>, drop_last<span class="op">=</span> <span class="va">True</span>, num_workers<span class="op">=</span> <span class="dv">0</span>):</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> tiktoken.get_encoding(<span class="st">'gpt2'</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> GPTDatasetV1(txt, tokenizer, max_length, stride)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        dataset,</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> batch_size,</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span> shuffle,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        drop_last<span class="op">=</span>drop_last,</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        num_workers<span class="op">=</span>num_workers)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataloader</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Here we test our dataloader for one batch, with context window of size 4:</li>
</ul>
<div id="bd7d7636-ecab-497c-84c5-e69bfecb8b49" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"the-veredict.txt"</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    raw_txt <span class="op">=</span> f.read()</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="712822c9-04cf-4247-b01c-858a1a472469" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoaderGPTV1(raw_txt, batch_size<span class="op">=</span><span class="dv">1</span>, max_length<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">1</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>data_iter <span class="op">=</span> <span class="bu">iter</span>(dataloader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c67e0301-bd79-4752-8a89-f87b1b8837db" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(data_iter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]</code></pre>
</div>
</div>
<ul>
<li>The first batch returns 2 tensors:
<ul>
<li>the first tensor stores the input token IDs.</li>
<li>and the second tensor stores the target token IDs.</li>
</ul></li>
<li>Since we fixed the <strong><code>max_length</code></strong> in 4 we get tensors of size 4.</li>
<li>The <strong><code>stride</code></strong> decides of the slide from batch to batch. here since we pick<code>1</code> the target input will shift by one only.</li>
<li>Here we use batch of 8 and <strong><code>stride=4</code></strong></li>
</ul>
<div id="dd9858a2-1d33-4676-9e8a-67658d4649f5" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoaderGPTV1(raw_txt, batch_size<span class="op">=</span> <span class="dv">8</span>, max_length<span class="op">=</span> <span class="dv">4</span>, stride<span class="op">=</span> <span class="dv">4</span>, shuffle<span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>data_iter <span class="op">=</span> <span class="bu">iter</span>(dataloader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ba8e4ef5-24b0-4436-836e-e06d23b96cb8" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(data_iter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>[tensor([[   40,   367,  2885,  1464],
         [ 1807,  3619,   402,   271],
         [10899,  2138,   257,  7026],
         [15632,   438,  2016,   257],
         [  922,  5891,  1576,   438],
         [  568,   340,   373,   645],
         [ 1049,  5975,   284,   502],
         [  284,  3285,   326,    11]]),
 tensor([[  367,  2885,  1464,  1807],
         [ 3619,   402,   271, 10899],
         [ 2138,   257,  7026, 15632],
         [  438,  2016,   257,   922],
         [ 5891,  1576,   438,   568],
         [  340,   373,   645,  1049],
         [ 5975,   284,   502,   284],
         [ 3285,   326,    11,   287]])]</code></pre>
</div>
</div>
</section>
<section id="creating-token-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="creating-token-embeddings">Creating token embeddings:</h3>
<ul>
<li>Next step is to create <strong>Embeddings</strong> from token ID’s.</li>
<li>Embeddings are <strong>trainable weights</strong> that help the model to <strong>Learn</strong> meanings from words.</li>
<li>To fully understand how embedding works we will create simple embedding layer:</li>
</ul>
<div id="6a844487-c476-4174-ac3a-1a95c6e85b34" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>output_dim <span class="op">=</span> <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Just in order to grasp the idea of embedding we imagine that we have a vocabulary of size <strong>6</strong> and each vocab is projected to <strong>3</strong> dimensions.</li>
<li>Here we use <strong>Pytorch</strong> to create an embedding layer:</li>
</ul>
<div id="904199d7-9b1b-4003-96da-597ed884f264" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> torch.nn.Embedding(vocab_size, output_dim)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>The embedding layer dimensions as created:</li>
</ul>
<div id="f8a13470-1364-412b-954a-f83fbf9e24cc" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>embedding_layer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>Embedding(6, 3)</code></pre>
</div>
</div>
<ul>
<li>And those are the weights that are randomely set:</li>
</ul>
<div id="318dba7a-78d1-49c3-86f3-696b91fda28c" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>embedding_layer.weight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)</code></pre>
</div>
</div>
<ul>
<li>The <strong>weigth matrix</strong> contains small values that will be optimized during the training of the LLM.</li>
<li>Each of the six rows represent a token.</li>
<li>Each of the three columns represents on dimension of that token.</li>
</ul>
<div id="05146193-2bfc-4d6e-b787-ea2b7b43dbb8" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>embedding_layer(torch.tensor([<span class="dv">3</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="encoding-word-positions" class="level3">
<h3 class="anchored" data-anchor-id="encoding-word-positions">Encoding word positions:</h3>
<ul>
<li>Since transformers lack inherent sequence-awareness (they process tokens in parallel), position encoding adds this information to the input embeddings.</li>
<li>Token position encoding incorporate information about the order of tokens in a sequence.</li>
<li>There are two common methods for position encoding:
<ul>
<li><strong>Absolute Position Encoding</strong>: Assigns a unique position value (e.g., using sinusoidal functions or learned embeddings) to each token’s position in the sequence.</li>
<li><strong>Relative Position Encoding</strong>: Encodes the positional relationship between tokens, focusing on their relative distance rather than their absolute positions.</li>
</ul></li>
<li>Now lets build a larger version of the embedding layer with <strong>256</strong> dimensions instead of <strong>3</strong>, still this number is way smaller than the original <strong>gpt3</strong> with <strong>12288</strong>.</li>
<li>Assuming we have a vocab_size of <code>50257</code>:</li>
</ul>
<div id="d965768c-b737-46a6-bb21-3d644c969d07" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">50257</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>output_dim <span class="op">=</span> <span class="dv">256</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="35516296-f240-4202-bc3d-3ab9b11e8951" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>token_embedding_layer <span class="op">=</span> torch.nn.Embedding(vocab_size, output_dim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>If we use the our <code>dataloader</code> with this embedding layer, for exmaple with a batch size of 8, for tokens each, we will have tensor of size: <strong><code>8*4*256</code></strong></li>
</ul>
<div id="0a611ff8-d578-48ca-a69a-ca0aada7dbe3" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoaderGPTV1(raw_txt, batch_size<span class="op">=</span><span class="dv">8</span>, max_length<span class="op">=</span>max_length, stride<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1ae6313e-542d-40c2-b2fa-00b3680c4886" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>data_iter <span class="op">=</span> <span class="bu">iter</span>(dataloader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b133e226-da4e-4b3a-bae2-cbe8ab316e4b" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>inputs, targets<span class="op">=</span> <span class="bu">next</span>(data_iter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9fcd35e1-e9da-4c57-8261-437656c35d4f" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Token IDs:</span><span class="ch">\n</span><span class="st">'</span>, inputs)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Input shape:</span><span class="ch">\n</span><span class="st">'</span>, inputs.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token IDs:
 tensor([[   40,   367,  2885,  1464],
        [ 1807,  3619,   402,   271],
        [10899,  2138,   257,  7026],
        [15632,   438,  2016,   257],
        [  922,  5891,  1576,   438],
        [  568,   340,   373,   645],
        [ 1049,  5975,   284,   502],
        [  284,  3285,   326,    11]])

Input shape:
 torch.Size([8, 4])</code></pre>
</div>
</div>
<ul>
<li>Pass it through the embedding layer we created:</li>
</ul>
<div id="d6be443d-a5e9-4381-a200-9fd1b706745f" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>token_embedding<span class="op">=</span> token_embedding_layer(inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c57c6719-d74a-47fa-9e42-159856fdbb84" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>token_embedding.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>torch.Size([8, 4, 256])</code></pre>
</div>
</div>
<ul>
<li>To create position encoding layer the <strong>gpt3</strong> way, we just need to create another embedding layer that has the same dimension as <strong>token_embedding_layer</strong>:</li>
</ul>
<div id="dd9da2ab-8015-4b20-84b7-5ed64f0a80b7" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>pos_embedding_layer <span class="op">=</span> torch.nn.Embedding(context_length, output_dim)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>pos_embedding <span class="op">=</span> pos_embedding_layer(torch.arange(context_length))</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>pos_embedding.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>torch.Size([4, 256])</code></pre>
</div>
</div>
<ul>
<li>The input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, …, up to the maximum input length –1.</li>
</ul>
<div id="c652bcad-cb23-4842-8681-8c59c13496d8" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>input_embedding <span class="op">=</span> token_embedding <span class="op">+</span> pos_embedding</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>input_embedding.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>torch.Size([8, 4, 256])</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/iSmailTG\.github\.io\/Lifelong-Learner\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>