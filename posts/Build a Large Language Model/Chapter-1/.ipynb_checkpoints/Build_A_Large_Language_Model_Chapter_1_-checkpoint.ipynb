{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "In this series on notebooks I will share my learning/note-taking of the book [**Build a Large Lunguage Model(From Scratch)**](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) by the Author [**Sebastian Raschka**](https://x.com/rasbt)."
      ],
      "metadata": {
        "id": "G4l41m48EiG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure of the Book:\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "├── chapters  \n",
        "│   ├── chapter1_understanding_LLMs: high-level introduction to the fundamental concepts behind\n",
        "LLMs.  \n",
        "│   ├── chapter2_text_data: It covers the process of\n",
        "preparing text for LLM training, including splitting text into word and subword\n",
        "tokens.  \n",
        "│   ├── chapter3_attention_mechanisms:  It introduces a basic\n",
        "self-attention framework and progresses to an enhanced self-attention mechanism.  \n",
        "│   ├── chapter4_GPT_model: focuses on coding a GPT-like LLM that can be trained to generate\n",
        "human-like text.  \n",
        "│   ├── chapter5_pretraining: implements the pretraining process of LLMs.  \n",
        "│   ├── chapter6_text_classification:  introduces different LLM fine-tuning approaches.  \n",
        "│   ├── chapter7_instruction_following:  explores the instruction fine-tuning process of LLMs.  \n",
        "└──  \n",
        "```"
      ],
      "metadata": {
        "id": "DvudVxpYM9YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter -1-:\n",
        "* The aim of this chapter is to introduce the foundational concepts of large language models (LLMs) and the advancements in deep learning that made them possible\n",
        "  - this chapter doesn't contain any code.\n",
        "* Large language models (LLMs), like OpenAI's ChatGPT, are deep neural networks that revolutionized natural language processing (NLP) in recent years.\n",
        "* Traditional NLP methods excelled in tasks like spam classification and simple pattern recognition but struggled with complex tasks requiring advanced understanding and generation abilities.\n",
        "* Contemporary LLMs can handle sophisticated language tasks, such as writing an email from keywords, which was challenging for earlier models.\n",
        "\n",
        "* When we say language models \"understand,\" we mean they can produce text that seems coherent and contextually appropriate, not that they have human-like awareness or true comprehension.\n",
        "\n",
        "* The transformer architecture and large datasets have driven the shift in NLP, enabling more advanced language understanding and interaction.\n",
        "\n",
        "\n",
        "### What's an LLM:\n",
        "\n",
        "* LLM's are neural network designed to understand and produce huma-like text.\n",
        "* Large in LLM refer to the size of the datasets those model trained on, but also on the size of parameters ( 100's of billions)\n",
        "  - Parameters are adjusted weights during training to predict next word in sentence.\n",
        "* The architecture of an LLM is called `transformers` which apply the `attention mechanism` to different parts of the input while performing the next word prediction.\n",
        "### Applications of LLM's:\n",
        "* LLM's can be used in many contexts to perform different tasks:\n",
        "  - machine translation\n",
        "  - sentiments analysis\n",
        "  - text generation\n",
        "  ..\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "_-3aj7ZjEibI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NHMsK4o9EieN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BdlAukq0Eihg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gm4eNkBPEik4"
      }
    }
  ]
}