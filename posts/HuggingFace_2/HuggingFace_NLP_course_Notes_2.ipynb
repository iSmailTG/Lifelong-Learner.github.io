{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hugging Face Course Notes: Chapter2 \"\n",
    "author: \"Ismail TG\"\n",
    "date: \"10/19/2023\"\n",
    "categories: [Hugging-Face, NLP, LLMs, Pytorch]\n",
    "image: \"nlp-course.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlBVuXXbyRWv"
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "* Transformer models are very large with Ms to 10s of Billions of parameters, which make the process of training and fine-tuning and deploying them very hard.\n",
    "* Here comes the **Hugging Face** library which adress that problem, the goal is to provide a single `API` through which any transformer model can be loaded, trained and saved.\n",
    "* With **`Transformer`** library we can:\n",
    "      - Download, load and use models for inference or fine-tuning with just couple lines of code\n",
    "      - all models in the library are stored like any other model, at their core they are just a simple pytorch `nn.Module` class.\n",
    "      - All components of the models are stored in one file, so no abstarctions or shared modules across files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCjbXZ0I1IZT"
   },
   "source": [
    "# Behind the PipeLine:\n",
    "\n",
    "* To understand what's happenening behind the scene we must first start with what already know: **Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHCVWrsn0Get",
    "outputId": "476937af-1205-4b9b-8163-f668cfd56158"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996117949485779}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier(['My birthday is today!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMVAamS3H7n"
   },
   "source": [
    "* As we saw in the previous chapter the `pipeline` goups 3 steps in order to perform such a task:  \n",
    "![pipeline](pic1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxOsm19u3kI9"
   },
   "source": [
    "## Preprocessing with a Tokenizer:\n",
    "\n",
    "* In order to convert raw text to its numerical form before we feed it to the model, we use **Tokenizer**.\n",
    "* Here is how we tokenize any input words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLXTlCFB2zT8",
    "outputId": "9ac39f1a-423f-42d7-f716-275325e4afb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2026, 5798, 2003, 2651,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "mdl_ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\n",
    "inputs = 'My birthday is today!'\n",
    "outputs = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Vzq-0WG5QxV"
   },
   "source": [
    "* First we pick a model `distilbert-base-uncased-finetuned-sst-2-english` which is basically the same model our pipeline used to classify the sentence.\n",
    "* We use `AutoTokenizer` to get to tokenization method according to that model, because each model has its own method of tokenizing words.\n",
    "* Then we feed the text to the tokenizer, and we pick which type of tensors we want to get returned\n",
    "    - `pt` stands for pytorch\n",
    "    - other parameters will be covered later\n",
    "\n",
    "* We get a dictionary with 2 keys: `input_ids` and `attention_mask`\n",
    "* `attention_mask` will be covered later, `input_ids` contains one list of integers.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjakIlW872iZ"
   },
   "source": [
    "##  Going through the model:\n",
    "\n",
    "* We can download the pretraind model same we did with tokenizer, by usin `AutoModel` class which also has `from_pretrained` method.\n",
    "* We just need to download the same model as used in tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfFIPZC_5J0q"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(mdl_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqoTx-V79lDm"
   },
   "source": [
    "* This architecture we just downloaded conatins onlly the base transformer module: given some inputs, it outputs what we call **Hidden_state**.\n",
    "* For each model inputs we will retrieve a high-dimensional vector representing the contextual understanding of that input by the model\n",
    "* These Hidden_states can be used as it is, but usually it will be feeded as input to another part of the model called the **Head**.\n",
    "* Each Head is a task_specific head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ-wPnKW-pAq"
   },
   "source": [
    "##  A high-dimensional vector?\n",
    "\n",
    "* Usually the model outputs a large vector with 3 dimensions:\n",
    "  - **Batch-size**: the number of sequence processed (in our case we pass only one sentence)\n",
    "  - **Sequence-length**: The length of the numerical representation of the sequence (8 in our example)\n",
    "  - **Hidden size**: The vector dimension of each model input.\n",
    "\n",
    "* The high-dimentionality of this vector comes from the last dimension, the hidden-size is very large dimension: usually ~700:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgkMNy-M8gX7",
    "outputId": "52133427-783f-492d-dc91-be3f3effc848"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = model(**outputs)\n",
    "outs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ2yjfG_D_Su"
   },
   "source": [
    "##  Model heads: Making sense out of numbers:\n",
    "\n",
    "* So to wrap-up the whole process: First get inputs converted input ID then the embedding layer convert them into tokenized vectors.\n",
    "* The subsequent layers manipulate thes vectors using attention mechanism to produce a contextual understanding of that input in form of **High-dimensional-vector**.\n",
    "\n",
    "![model](pic2.png)\n",
    "\n",
    "\n",
    "* There rae many architecture available in the Transformers library, each is designed to tackle specific task.\n",
    "* For example if we want a model for a sequence classification head, we will use `AutoModelForSequenceClassification` instead of `AutoModel`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_q0It9KHhl0"
   },
   "outputs": [],
   "source": [
    "text = ['do you feel any better today?', 'I feel warm and cosy in my house']\n",
    "tokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\n",
    "inps = tokenizer(text, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KdPSYexDTcw",
    "outputId": "f93f33f0-574e-4f79-bcdf-b4a029249453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2121,  0.4987],\n",
       "        [-3.9382,  4.1996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "mdl_ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(mdl_ckpt)\n",
    "outs = model(**inps)\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FHmSn_BHUSh",
    "outputId": "7ade4d11-0861-4cb0-f0aa-578a9d200f9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3wiqTV4IdsM"
   },
   "source": [
    "* In this case we have 2 sentences and 2 labels `negative` `positive`.\n",
    "* The model will take the high dimensional vector as input and outputs a vector that match our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnL8-sDjJYJh"
   },
   "source": [
    "### Post processing:\n",
    "\n",
    "* The vector we get doesn't make any sense as it is, so we need to make it meaningful for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lP3LyVYhHcmc",
    "outputId": "08a9531b-a0dc-4bda-997e-675b202e6a99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2121,  0.4987],\n",
       "        [-3.9382,  4.1996]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1-DTXxJukR"
   },
   "source": [
    "* those are prediction for each sentence, and each prediction can be mapped to a label, so we need to know each label which, then convert those logits into some meaningful values.\n",
    "* To convert the logits into probabilies we will pass them through a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbJk8rOtJtGS",
    "outputId": "0701368e-574a-4f60-d80f-230fef87d929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2942e-01, 6.7058e-01],\n",
       "        [2.9218e-04, 9.9971e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "preds = torch.nn.functional.softmax(outs.logits, dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xup_ugSuLVDN"
   },
   "source": [
    "* Now we need to know the label of each colomn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7R1HFiDkKvp2",
    "outputId": "66c82ec7-af43-4636-996b-ca51bc3185bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCsrlxOWLvxV"
   },
   "source": [
    "* So the position [0] is negative where the position [1] positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MvkiQPM2zm"
   },
   "source": [
    "# Models\n",
    "\n",
    "* As we saw before the `AutoModel()` class is handy tool to instantiate a model from a `chekcpoint(weights)`\n",
    "* It can guess the correspondent architecture for the checkpoint.\n",
    "\n",
    "### Building the transformer:\n",
    "\n",
    "* We also could call the class of the model precisely if we know exactly the model we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye4Stf6VLuCJ"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "cnfg = BertConfig()\n",
    "mdl = BertModel(cnfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtNTjwiFOW-9"
   },
   "source": [
    "* The configurations contains many attributes related the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCgdW6hTOOgC",
    "outputId": "d3a4e181-a8a3-4222-b93b-15b092f2a89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_367YGOzOmKO"
   },
   "source": [
    "* We can understand many of these attributes like:\n",
    "   - `hidden_act`: activation function : `gelu`\n",
    "   - `hidden_size`: vector dimensions of each input word\n",
    "   - `attention_head`, `num_hidden_layer`, `model_type` ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFQyjD-eP5jY"
   },
   "source": [
    "* While it is possible to build model like this way and using it, but it will produce very low predictions beacause its weights are set randomly.\n",
    "* This forces us to train it from scratch, which is a very daunting and time, noney, energy consuming process.\n",
    "* This is way its very preferably to use to other way of loading the model by starting with a pretrained one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vj-mHMqZOiHP"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "mdl = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0bcyTRjRbEq"
   },
   "source": [
    "* we even could use `AutoModel` instead of `BertModel` since it will produce agnostic code that fits all situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIhvSWgFR4dP"
   },
   "source": [
    "* At this point the model is initialized with all the weights of the checkpoint, it can be used for inference directly on the tasks it was trained on, and also it can be fine-tuned on new tasks or more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tBmGv06Try3"
   },
   "source": [
    "## Saving the model:\n",
    "\n",
    "* To save a model we are satisfied with its prformance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QYmYctzRWG1"
   },
   "outputs": [],
   "source": [
    "mdl.save_pretrained('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mlJdC_QVP77",
    "outputId": "8a585ae9-7d44-42ec-d74a-53ded3889ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\tspecial_tokens_map.json  tokenizer_config.json\tvocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls 'path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "117xyVcMUlJb"
   },
   "source": [
    "* This saves 2 files:\n",
    "    - `config.json`: contains all attributes necessary to build the model architecture, and also it contains some metadata\n",
    "    - `pytorch_model.bin`: contains the learnable weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDIrBNrwVclL"
   },
   "source": [
    "##  Using a Transformer model for inference:\n",
    "\n",
    "* Tokenizer convert input words into input ID:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DluGWYTHUh8E",
    "outputId": "713c92f0-19f0-446a-dbec-cd7d02a34d2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 7592, 999, 102], [101, 4658, 1012, 102], [101, 3835, 999, 102]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "inps = tokenizer(sequences)\n",
    "encoded_sequences = inps.input_ids\n",
    "encoded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-7DI-tQYsQ-"
   },
   "source": [
    "* The output we get here is a list of list, the problem is that tensors accept only rectangular shapes.\n",
    "* So we nee to cenvert it into the targeted shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SggbHKaNXZem",
    "outputId": "2064b705-e417-4a0c-d2fc-48159dd921bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(encoded_sequences)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdnmxQK7ZOkQ"
   },
   "source": [
    "###  Using the tensors as inputs to the model\n",
    "\n",
    "* Making use of this returned tensor is easy as pass it through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CAtUWShX3Mw",
    "outputId": "70f5b950-e5a1-46cd-e68f-d9152c8e7d84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n",
       "           3.9394e-01, -9.4770e-02],\n",
       "         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n",
       "           2.2992e-01, -4.1172e-02],\n",
       "         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n",
       "           2.8224e-01,  7.5566e-02],\n",
       "         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n",
       "           1.0441e+00, -6.1970e-03]],\n",
       "\n",
       "        [[ 3.6436e-01,  3.2464e-02,  2.0258e-01,  ...,  6.0111e-02,\n",
       "           3.2451e-01, -2.0995e-02],\n",
       "         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n",
       "           1.4553e-01, -3.7545e-02],\n",
       "         [ 3.3223e-01, -2.3271e-01,  9.4877e-02,  ..., -2.5268e-01,\n",
       "           3.2172e-01,  8.1079e-04],\n",
       "         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n",
       "           1.0526e+00, -5.6255e-01]],\n",
       "\n",
       "        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n",
       "           3.3564e-01,  2.8262e-01],\n",
       "         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n",
       "           2.0175e-01,  3.3275e-01],\n",
       "         [ 2.0160e-01,  1.5783e-01,  9.8974e-03,  ..., -3.8850e-01,\n",
       "           4.1308e-01,  3.9732e-01],\n",
       "         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n",
       "           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n",
       "        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n",
       "        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs= mdl(input)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZdOuAmPCL5K"
   },
   "source": [
    "# Tokenization:\n",
    "\n",
    "* One of the core component of the NLP Pipeline is the idea of **Tokenezation**\n",
    "* It serve the purpose of translating raw text to its numerical form, so the model can process\n",
    "* In general there's 3 approches to tokenize raw text, we won't get in much details:\n",
    "    * **Word-based**: each word has its own token\n",
    "    * **Character-Based**:  split text into charchters and gives each one of them a unique token\n",
    "    * **Sub-Words Tokenization**: basically gives frequent words unique tokenm while split rare words into sub parts and tokenize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ntDLAoEEiy8"
   },
   "source": [
    "## Loading and Saving:\n",
    "\n",
    "* Loading and saving tokenizer is simple and very similar to how we load and save Models.\n",
    "* By using the same two methods: `from_pretrained` and `save_pretrained`.\n",
    "* Also we can load the tokenizer either by calling tokenizer class `BertTokenizer` or by just using `AutoTokenizer`, same as how we load models: `AutoModel` or `BertModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1b77ENgCLa3"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWM0KcgUYHhs"
   },
   "outputs": [],
   "source": [
    "# or the easy way\n",
    "tokenizer1 = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIdMJhNNGDuS"
   },
   "outputs": [],
   "source": [
    "text = 'try to tokenize this text so I can see the difference between them!'\n",
    "inp = tokenizer(text, return_tensors='pt')\n",
    "inp1 = tokenizer1(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlPXASBMGj8M",
    "outputId": "d471224c-0590-4906-de8f-f7b5d7d4a219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2222,  1106, 22559,  3708,  1142,  3087,  1177,   146,  1169,\n",
       "           1267,  1103,  3719,  1206,  1172,   106,   102]]),\n",
       " tensor([[  101,  2222,  1106, 22559,  3708,  1142,  3087,  1177,   146,  1169,\n",
       "           1267,  1103,  3719,  1206,  1172,   106,   102]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.input_ids, inp1.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyL1u9FGrEy"
   },
   "source": [
    "* So both methods yield same results, but as we said before we prefer using the second method since its code agnostinc and can be applied with all model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYXyvAlUHJG0"
   },
   "source": [
    "* Saving tokenizer is also similar to how we save models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCZUPWWDGpDJ",
    "outputId": "250ea12e-922c-4451-e107-d14835f413a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('path/tokenizer_config.json',\n",
       " 'path/special_tokens_map.json',\n",
       " 'path/vocab.txt',\n",
       " 'path/added_tokens.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTiGTGhmPqJt"
   },
   "source": [
    "## Encoding:\n",
    "\n",
    "* The process of translating words to numbers is called `encoding`.\n",
    "* The encoding is done through 2 steps:\n",
    "   - Tokenization\n",
    "   - conversion to input IDs\n",
    "\n",
    "![Tokenizer](tok_pipe.png)\n",
    "* The first we create token-word, ,ostly complete words, but in some cases the one word will be splited to 2 or more parts.\n",
    "    - this sub parts can be dentified by the `##` preffix.\n",
    "* Then we need to convert those tokens into input IDs in order to feed them to the model.\n",
    "* To do that the tokenizer pass this tokens through a `Vocabulary`.\n",
    "   - When we instentiate the tokenizer with `from_pretrained()` we already download that vocabulary the we can match ewach token against an  ID.\n",
    "   - we need to use the same checkpoint during the training.\n",
    "* Here we will explore these 2 steps seperatly:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wulDcp6PHTxT",
    "outputId": "46c8e19b-da85-4446-a58a-28900784bbf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = 'Using a transformer network is simple'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zXpOi5FVPuZ"
   },
   "source": [
    "* As we see the word `transformer` get splited to 2 parts, the second one is represented with `##` prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T-Z6zoKVH7j",
    "outputId": "e52e4445-946b-4a4d-a83f-ac30a3c0d076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E17g7-LGV0I3"
   },
   "source": [
    "## Decoding:\n",
    "\n",
    "* The idea of decoding is the exact opsite of encoding, its the process of converting ids to their text/word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tNes-UU2Vqip",
    "outputId": "21489804-1c4c-45f6-a901-e7edac57d1ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Using a transformer network is simple'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_ids = tokenizer.decode(ids)\n",
    "decoded_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTDn5lsOWv9C"
   },
   "source": [
    "* We get the text we begin with back by using the `decode()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2yqoledXZV6"
   },
   "source": [
    "#  Handling multiple sequences:\n",
    "\n",
    "* Performing inference on a single sequence is a simple task, but what if we want to process multiple sequence wuth different lengths?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_IK9vUlWps0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# model checkpoint\n",
    "mdl_chkpt = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\n",
    "# model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(mdl_ckpt)\n",
    "# sequence\n",
    "text = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "#tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# convert them into tensor\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUIZC_Xlc_fx"
   },
   "source": [
    "* Now we can pass the tensor into the model to perform some prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "HEATjAhBcZSH",
    "outputId": "6126b963-4ca6-4b62-89de-58e5f274be7d"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-9aae2e9c06e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   3939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3940\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3941\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3942\u001b[0m             warn_string = (\n\u001b[1;32m   3943\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "preds = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e97wZK0dhkM"
   },
   "source": [
    "* We did same steps as before but we get an error?\n",
    "* If we look again to what the tokenizer output we see that it didn't just return a tensor, but it added a dimension of top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bhh4gUIOdLfI",
    "outputId": "63f83cde-371d-4071-f580-2002dc933800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is normal way: tensor([[ 101, 2023, 2003, 2074, 1037, 3722, 2742, 2000, 2156, 1996, 4489, 2090,\n",
      "         1996, 1016,  999,  102]])\n",
      "this is the manual way: tensor([2023, 2003, 2074, 1037, 3722, 2742, 2000, 2156, 1996, 4489, 2090, 1996,\n",
      "        1016,  999])\n"
     ]
    }
   ],
   "source": [
    "seq = 'this is just a simple example to see the difference between the 2!'\n",
    "normal_input = tokenizer(seq, return_tensors='pt')\n",
    "the_tokens = tokenizer.tokenize(seq)\n",
    "ids_1 = tokenizer.convert_tokens_to_ids(the_tokens)\n",
    "inp_ids = torch.tensor(ids_1)\n",
    "print(f'this is normal way: {normal_input.input_ids}')\n",
    "print(f'this is the manual way: {inp_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9StfloIgLT6"
   },
   "source": [
    "* The normal way has one more dimension that the manual way, so we need to add this dimension while converting to tensor in order to feed it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXDdOrkUgITr",
    "outputId": "a57bf0f6-a36b-4b55-b6a3-38f9ebd33764"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids= torch.tensor([ids])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qonvvI-ug0Aw"
   },
   "source": [
    "* Now let's try again with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCdymqDggn_F",
    "outputId": "ec3e75ca-8751-4045-e7d5-9061721ccc3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(input_ids)\n",
    "preds.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dqXNddkhzbc"
   },
   "source": [
    "* The thing we need to consider here the importance to **`Batching`** when feeding input sentences to tthe model even if we have one single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXdDCq56g-Vj",
    "outputId": "fb041258-9ea7-44f3-8360-97b2076462c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "batched_inp_ids = torch.tensor(batched_ids)\n",
    "outs = model(batched_inp_ids).logits\n",
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ9Zvq8Ujptd"
   },
   "source": [
    "* batching allows the model to process multiple sentences.\n",
    "* What about if these sentences has different length? which is always the case.\n",
    "* As we know the tensors needs to be rectangular in order to conver list of IDs to tensor.\n",
    "* to solve this kind of problems and more we will go through some techninques to work around them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1Erf_S8khPk"
   },
   "source": [
    "## Padding:\n",
    "\n",
    "* Suppose we have this list of list as ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO1Fb_F7kgjW"
   },
   "outputs": [],
   "source": [
    "batched_ids = [[200, 200], [200, 200, 100]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqCx8GAta1Wd"
   },
   "source": [
    "* We need to get this list of list in rectangular shape before we convert it into tensor.\n",
    "* This is where we use `padding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYU8bPI5HgCf"
   },
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "batched_ids = [[200, 200, padding_id], [200, 200, 100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4NCie0-eL7A"
   },
   "source": [
    "* The padding token ID can be found in `tokenizer.pad_token_id`.\n",
    "* Now let's do a simple experience to see differences between 3 batches after going through a model:\n",
    "      - first list\n",
    "      - second list\n",
    "      - both lists batched and padding applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbIMS2JXeK36"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(mdl_chkpt)\n",
    "sequence1_ids = [[200, 200]]\n",
    "sequence2_ids = [[200, 200, 200]]\n",
    "batched_ids = [[200, 200,tokenizer.pad_token_id ], [200, 200, 200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUvNmJuXfsHV",
    "outputId": "ca82dadf-c757-4a78-c44c-f8937948c86c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.3374, -1.2163],\n",
      "        [ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnWxYbJrgmom"
   },
   "source": [
    "* The thing we could observe here is that we get same logits for the second sequence ID compared with batched_ids but not the first one?\n",
    "* The first sequence id is where we applied padding, and as we know transformer models are very sensetive to any context of the words (in this case context of the ids) so the element that we added in order to get a rectangular shape, is also get computed by the transformer model, which influence the final prediction.\n",
    "* We need to tell the model to ignore these padding values during the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kocnWUSbIS91"
   },
   "source": [
    "### Attention mask:\n",
    "\n",
    "* Attention mask is what tell the model during the predecting phase to ignore padding values and not including them while computing the attention mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJw_bN97f-S-",
    "outputId": "d0cf5086-a090-4570-f33c-ecf056a951cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0L4PaOFJaFX"
   },
   "source": [
    "* Now we get the same logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IonWLiNXKp8D"
   },
   "source": [
    "##  Longer sequences:\n",
    "\n",
    "* Transformer model cannot handle very long sentences, usually they have between 512 and 1024 tokens as maximum length for a sentence.\n",
    "* I we have a situation where we need to deal with very large sequence we either:\n",
    "    - use models that can handle long sentences\n",
    "    - use `truncation` method\n",
    "\n",
    "* `Truncation` is a way of making sequences of the same batch the same length, either by picking the length of the longest sequence or the short one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQKW4vm5ayr7"
   },
   "source": [
    "# Putting All Together:\n",
    "\n",
    "* What we have done till now is hard-coding each step of the tokenization process without full help from the `Tokenizer`.\n",
    "* However as we saw before the Transformer API can handle all this kind of work with a high-level functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlSXj3OAI5-E"
   },
   "outputs": [],
   "source": [
    "mdl_ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mdl_ckpt)\n",
    "seq = \"I've been waiting for a HuggingFace course my whole life.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqbIU-s0b07X"
   },
   "source": [
    "* We can call the tokenizer function directly on a sequence and get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yL6_7YOboqY",
    "outputId": "4a27a216-8725-4db4-dfa3-8e79ba0c8275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(seq)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55FmdpD0cYF7"
   },
   "source": [
    "* We get input IDs and even the `attention_mask` is applied.\n",
    "* We can also define more feature we want our tokenizer to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imlhikJlbyCm",
    "outputId": "a2f1aedd-6115-448a-c51c-955d60d04aaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 6251, 2193, 2028, 102], [101, 2023, 2003, 1996, 2117, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass multiple sentences:\n",
    "seqs = ['this is sentence number one', 'this is the second']\n",
    "inps = tokenizer(seqs)\n",
    "inps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yT9Dm9sni96"
   },
   "source": [
    "* Padd on different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoGIypLYdG1V"
   },
   "outputs": [],
   "source": [
    "# padd based on the longest sequence\n",
    "inps = tokenizer(seqs, padding='longest')\n",
    "\n",
    "# padd based on the model max length\n",
    "inps = tokenizer(seqs, padding='max_length')\n",
    "\n",
    "# padd on specified length\n",
    "\n",
    "inps = tokenizer(seqs, padding='max_length', max_length = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUY6C0cUop9A",
    "outputId": "f529a1e5-35a8-4e04-cd58-33a6731a4821"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 6251, 2193, 2028, 102], [101, 2023, 2003, 1996, 2117, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "inps = tokenizer(seqs, truncation=True)\n",
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4FY6dwDteKQ",
    "outputId": "8ba739b7-dce3-4d85-a3b1-3b0cb1873a32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 6251, 2193, 2028, 102], [101, 2023, 2003, 1996, 2117, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will truncate the sequences that are longer than the specified max length\n",
    "inps = tokenizer(seqs, max_length=7, truncation=True)\n",
    "inps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR84Ho51tuwZ"
   },
   "source": [
    "* The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model==>`pt for pytorch`\n",
    "* The padding in this case should be always set as `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxeyS_dRtkiJ",
    "outputId": "eb516512-452a-451e-ea57-eef8b6317f26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 6251, 2193, 2028,  102],\n",
       "        [ 101, 2023, 2003, 1996, 2117,  102,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.instructblip.modeling_instructblip import InstructBlipQFormerSelfOutput\n",
    "inps = tokenizer(seqs, return_tensors='pt', padding=True)\n",
    "inps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQkqP8Hqt6v1"
   },
   "source": [
    "## Special Tokens\n",
    "\n",
    "* If we look closely to the input ID's we get, we can spot a small difference from what we got earlier\n",
    "* The tokenizer added 2 ID's to the list, one in the begining and another at the end.\n",
    "   - they alwayas have the same value: `101` and `102`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yLHwiBhtyoA",
    "outputId": "0525d7c6-3313-46e8-fd2a-3193609b6b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "hard_coded: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "seq = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "toks1 = tokenizer(seq)\n",
    "toks2 = tokenizer.tokenize(seq)\n",
    "ids1= toks1.input_ids\n",
    "ids2 = tokenizer.convert_tokens_to_ids(toks2)\n",
    "print(f'normal: {ids1}')\n",
    "print(f'hard_coded: {ids2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cfYTEBjuDBw"
   },
   "source": [
    "* Now we will decode the different types of ID's we will get different decoded sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c93q5LUmt-qo",
    "outputId": "fd470fe7-f9f6-483f-bfa2-5f4d719731e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: [CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "hard-coded: i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(f'normal: {tokenizer.decode(ids1)}')\n",
    "print(f'hard-coded: {tokenizer.decode(ids2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BLKhUwsuP-y"
   },
   "source": [
    "* Tokenizer added special tokens to the sentence `[CLS]` and `[SEP]`, because the model was trained with this kind of architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THqrzOJ2uVqp"
   },
   "source": [
    "# Wrapping Up: From Tokenizer to Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRLh6kCruKdl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laFntGVRuYsc",
    "outputId": "d521f49f-7d5c-459b-92b5-f66b49b0e04f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = output.logits\n",
    "outs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
