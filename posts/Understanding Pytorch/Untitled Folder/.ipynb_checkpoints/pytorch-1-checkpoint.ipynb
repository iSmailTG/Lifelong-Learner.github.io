{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7e71f831-892d-44cd-936f-7c3f63be6a90",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Dive into PyTorch Internals: Autograd and torch.fx \"\n",
    "author: \"Ismail TG\"\n",
    "date: \"04/05/2025\"\n",
    "categories: [Pytorch, Deep Learning]\n",
    "image: \"illu_pytorch-49.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d343f2-81e7-4b72-8f82-aded26302cc0",
   "metadata": {},
   "source": [
    "# Deep Dive into PyTorch Internals: Autograd and torch.fx\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As I continue to deepen my understanding of machine learning systems, I’ve realized that knowing how models *run* is just as important as knowing how to *build* them. This post kicks off a series where I explore PyTorch internals, starting with two powerful components: **Autograd** and **torch.fx**.\n",
    "\n",
    "\n",
    "\n",
    "##  PyTorch Autograd – The Engine Behind Gradient Descent\n",
    "\n",
    "### What Is Autograd?\n",
    "\n",
    "PyTorch’s `autograd` is a dynamic automatic differentiation engine. It records operations on tensors to build a computation graph during the forward pass, and then traverses that graph in reverse to compute gradients during the backward pass.\n",
    "\n",
    "### How It Works Internally\n",
    "\n",
    "When you perform operations on `torch.Tensor` objects with `requires_grad=True`, PyTorch:\n",
    "\n",
    "1. Creates a **computation graph** on the fly.\n",
    "2. Each operation produces a `Function` object (e.g., `AddBackward`, `MulBackward`).\n",
    "3. When `.backward()` is called, the engine performs **reverse-mode automatic differentiation**.\n",
    "\n",
    "### Example: Simple Chain Rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a95e24-cedb-4ebc-a101-eaec289600f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * x + 3 * x\n",
    "z = y.mean()\n",
    "z.backward()\n",
    "print(x.grad)  # 7.0 = d(x^2 + 3x)/dx at x=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a743f7a-9060-4bd2-ae25-becbb53352fe",
   "metadata": {},
   "source": [
    "\n",
    "### Key Internals\n",
    "- `Tensor.grad_fn`: Points to the function that created the tensor.\n",
    "- `Tensor.grad`: Stores the computed gradient.\n",
    "- `torch.autograd.Function`: Base class for custom differentiable operations.\n",
    "\n",
    "\n",
    "## torch.fx – PyTorch’s Intermediate Representation\n",
    "\n",
    "### Why Use torch.fx?\n",
    "\n",
    "`torch.fx` allows you to **capture and transform** PyTorch programs as Python-level graphs. This is useful for:\n",
    "- Programmatic model transformations\n",
    "- Debugging and visualization\n",
    "- Building custom compiler backends\n",
    "\n",
    "### Core Components\n",
    "- `GraphModule`: A traced model with a modifiable structure.\n",
    "- `Tracer`: Walks through the model and builds a `Graph`.\n",
    "- `Graph`: Contains `Node` objects that represent operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48abeadb-bf61-4399-812e-af9cf37c9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, 2), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%mul, 3), kwargs = {})\n",
      "    return add\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * 2 + 3\n",
    "\n",
    "model = MyModel()\n",
    "traced = fx.symbolic_trace(model)\n",
    "print(traced.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93e7d9-479a-4245-bf6e-a8b7ab692991",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Real-World Use Cases\n",
    "- **TorchDynamo** and **TorchInductor** use FX graphs as part of the compilation pipeline.\n",
    "- FX enables **quantization** and **pruning** workflows by allowing insertion or transformation of operations.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Both Autograd and `torch.fx` are essential for understanding what happens *under the hood* in PyTorch. Whether you're debugging models, optimizing inference, or building custom backends, mastering these tools opens the door to deeper systems-level work in AI.\n",
    "\n",
    "In future posts, I plan to explore:\n",
    "- Implementing custom autograd functions\n",
    "- Writing your own FX passes for transformations\n",
    "- Diving into TorchDynamo and TorchInductor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ef3e0-77e9-4015-a7d4-434c09cf599a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
