{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leLWIjHwmM7n"
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "* In this notebook I will try to investigate the **`SWAG`** datasets.\n",
    "* The idea is to understand how to deal with multiple choice datasets and how to prepare them for the next step.\n",
    "* Multiple choice is frequent problem in the filed of LLMs and NLP in general\n",
    "* So the preprocessing of data will have a huge effect on the success of any proposed solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCnTN2I8n8Oy",
    "outputId": "02708000-f93c-4a0a-d7e1-3a97b24b16b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ja-JqPBEoIO6",
    "outputId": "2bc21966-52fe-4447-a039-64506a56e843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "MAi3oFQroQ2q"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('swag', 'regular')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frevJ4E2onpX",
    "outputId": "77217f90-a443-47cd-aeda-db885317e33a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's grab a sample\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGWa8W6PrATB"
   },
   "source": [
    "* These fields represent the idea begind this dataset\n",
    "   - a situation where we have to predict the right ending\n",
    "   - `sent1` and `sent2` represent the given situation and they added up to `startphrase`\n",
    "   - `endings 0 to 3` represent the the endings for that situation, only one is the right\n",
    "   - `label` index the right answer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUZ7r9evQA-U"
   },
   "source": [
    "* Now let's initialized `BERT` and load its `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "DcgqKQybtVA4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHFPNIXsQUCL"
   },
   "source": [
    "* The idea here is to tokenize a start sentence with each one of the 4 choices,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "DH8Zl0ZWov4G"
   },
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names]\n",
    "        for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdMiFTee8TwW"
   },
   "source": [
    "* To understand each operation of that function we will do it step-by-step:\n",
    "\n",
    "  - First let's create a sub-set of the training set\n",
    "  - Create a `endings` list that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "UK8S8wXUvYf4"
   },
   "outputs": [],
   "source": [
    "endings = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "train_ds = dataset['train']\n",
    "smp = train_ds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5Z8UjpP9EPS"
   },
   "source": [
    "* Multiply each `sent1` by 4 and stack them all in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "L1OtRJD39TT7"
   },
   "outputs": [],
   "source": [
    "sent_1 = [[sent] * 4 for sent in smp['sent1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onqh7m0W9n5m"
   },
   "source": [
    "* Let's retrieve the length of that list and see what's inside one element of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bU96Asn-9mo-",
    "outputId": "51416942-b830-4a47-e26f-c6453d2f0c1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A group of members in green uniforms walks waving flags.',\n",
       "  'A group of members in green uniforms walks waving flags.',\n",
       "  'A group of members in green uniforms walks waving flags.',\n",
       "  'A group of members in green uniforms walks waving flags.'],\n",
       " 20)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1[2], len(sent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3RvfvmK-Z7V"
   },
   "source": [
    "* So basically we have 4 copies of each first-sentence of the dataset.\n",
    "* Now we will create a list of the second-sentence or the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "kABBaBBT_qyq"
   },
   "outputs": [],
   "source": [
    "headers = smp['sent2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywfx7ZS5_4Lu"
   },
   "source": [
    "* At this point we have:\n",
    "  - `sent_1` which each element is multiplied by 4\n",
    "  - `headers` that complete `sent_1`\n",
    "* The idea here is to create pairs of each `header` +`sent_2` for each `sent_1`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Bw3vRZjUIjHZ"
   },
   "outputs": [],
   "source": [
    "sent_2 = [[f'{head}{smp[end][i]}' for end in endings] for i, head in enumerate(headers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pre-processing](Preprocessing_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUwAc_lgJ4ex"
   },
   "source": [
    "* Now we need to **flatten** the pair of sentences, so we could tokenize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "7znDtBorJ2e5"
   },
   "outputs": [],
   "source": [
    "frst_sent = sum(sent_1, [])\n",
    "scnd_sent = sum(sent_2, [])\n",
    "tok_smp = tokenizer(frst_sent, scnd_sent, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPb4dJCOpHO1"
   },
   "source": [
    "* We tokenize the pair of list sentences which will return a dictionary with 3 keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bfuL--GpBHo",
    "outputId": "a580136c-b58f-42d7-f71c-4a059c5e69a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_smp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCAqoXaHpwMf"
   },
   "source": [
    "* But since we already flattend the pairs before the tokenization step, we need to get them unflatten again so we can pass it through the `map()` function in order to be computed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "eLUTIRvGqVfk"
   },
   "outputs": [],
   "source": [
    "outputs = {k: [v[i: i + 4] for i in range(0, len(v), 4)] for k, v in tok_smp.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwZ9IcrirgOy"
   },
   "source": [
    "* Lets check if we get the unflatten step right, we just need to make sure that the `input_ids` of the first sentence has the same values in both: `tok_smp` and `outputs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "d7iR4tEEsGIL"
   },
   "outputs": [],
   "source": [
    "flatten_smp = tok_smp['input_ids']\n",
    "unflatten_smp = outputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ll4hRVSsmGZ",
    "outputId": "21ff1639-09f6-4aec-bfd3-4a866a21f985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_smp[0:4] == unflatten_smp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ajEsMHJs5Lw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
