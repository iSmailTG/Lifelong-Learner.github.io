{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0a094028-58e5-4eed-bc38-23632da2d71c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 2: Questionnaire \"\n",
    "author: \"Ismail TG\"\n",
    "date: \"10/25/2022\"\n",
    "categories: [Fastai, Pytorch, HugginFace, Gradio, Deep Learning]\n",
    "image: \"fastai-book-2.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4bfbe-a4fa-48db-97ad-c0e72418e4f5",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93254201-69b3-42bc-9d4e-44f9b1d2de71",
   "metadata": {},
   "source": [
    "Q1:  \n",
    "**Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.**  \n",
    "___\n",
    "* There is many cases where the `bear_classifaction` model could produce low results because of low quality of training data:\n",
    "   - imbalance dataset: where we have much datapoint of one class way more than other classes, what causes the model to be biased toward one class.\n",
    "   - the image used in training are low quality, low resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f689c91-c9b6-47b2-84ba-ed86d4a3a8a6",
   "metadata": {},
   "source": [
    "Q2:\n",
    "**Where do text models currently have a major deficiency?**\n",
    "___\n",
    "* The current transformers models shows oustanding results in generating texts and essais, understanding (in a way!) human language and can participate in a full conversation on different topics and give understandable and admirable responses.\n",
    "* However the way models learn from text is way different than the human do. Models needs a huge amount of text data:\n",
    "![LargeLanguageModel](q1.png)\n",
    "\n",
    "* In this paper,[Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://openreview.net/pdf?id=GKTvAcb12b) Emily Bender and Alexander Koller consider whether LMs such as GPT-3 or BERT can ever learn to “understand” language? the researchers insists on deffirentiate between form (which LMs are good at understanding) and meaning( which obviously LMs can't understand). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50597325-4b48-4dec-a669-00f097410b94",
   "metadata": {},
   "source": [
    "Q3:  \n",
    "**What are the possible negative societal implications of text generation models?**  \n",
    "___\n",
    "* If someone uses these LMs to generate highly-compeling responds on social media in order to spred misinformation or encourage conflits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a62497-88be-4ffa-a233-5f0156277477",
   "metadata": {},
   "source": [
    "Q4:  \n",
    "**In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?**  \n",
    "___  \n",
    "* In this case we need to set a system where there's a human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b77084-f7d5-453f-8822-ea5897c9f8ca",
   "metadata": {},
   "source": [
    "Q5:  \n",
    "**What are the steps of the Drivetrain approach?**  \n",
    "___\n",
    "- Define your Objective  \n",
    "- Levers  \n",
    "- Data  \n",
    "- Models  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90b7dd-f07d-47ba-9806-aa7a204706be",
   "metadata": {},
   "source": [
    "Q6:   \n",
    "\n",
    "**What is DataLoaders?**  \n",
    "___\n",
    "* DataLoaders is a Fastai thin class that coutains `dataloader` for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7582e33-aa85-4c94-8b6c-fca09a6a0f9c",
   "metadata": {},
   "source": [
    "Q7:  \n",
    "**What four things do we need to tell fastai to create DataLoaders?**  \n",
    "___\n",
    "* Data we have\n",
    "* How to get items\n",
    "* How to label them\n",
    "* How to create train/validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07213a7-67bf-4f27-b599-18dd281850a1",
   "metadata": {},
   "source": [
    "Q8:  \n",
    "\n",
    "**What does the splitter parameter to DataBlock do?**\n",
    "___\n",
    "* Splitter provide the way we want our data set to be splited.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262876b-33b5-4c59-aad9-7178b5685f12",
   "metadata": {},
   "source": [
    "Q9:  \n",
    "**How do we ensure a random split always gives the same validation set?**  \n",
    "___\n",
    "* By fixing the seed value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b67f4-634d-46a6-aad6-f5eada37664c",
   "metadata": {},
   "source": [
    "Q10:  \n",
    "**What letters are often used to signify the independent and dependent variables?**  \n",
    "____\n",
    "* x for independent variables, y for dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9bc4a-a75b-47f8-8b74-ef4d08ee906a",
   "metadata": {},
   "source": [
    "Q11:  \n",
    "**What’s the difference between crop, pad, and squish Resize() approaches? When might you choose one over the other?**  \n",
    "___\n",
    "\n",
    "* Crop is the default `Resize()` method, which crop the image and take desired dimension, this may cause losing important information.\n",
    "* Pad is an alternative Resize() method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images), this may results in a lower effective resolution for the part of the image we actually use.\n",
    "* Squish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\n",
    "\n",
    "* The better method is something depends on the problem we have, type of data we will use..\n",
    "* We will see later different methods, like `RandomResizeCrop` and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3ae0d-c0c9-43a6-a333-bf92bcfb611a",
   "metadata": {},
   "source": [
    "Q12:  \n",
    "**What is data augmentation? Why is it needed?**  \n",
    "___  \n",
    "* Data augmentation refer to the process of generating more datapoints from the actaual data we have, and representing it within the dataset.\n",
    "* For example we could take an image and do some type of transformation to it, like flipping it or ratating it the resize the crop it, which will give us many images with different views and sizes.\n",
    "* This method other than making our dataset larger, it make it rich and diverse which will without doubt influence the generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebe4b0-0e53-4b5a-860a-27899daf8755",
   "metadata": {},
   "source": [
    "Q13:  \n",
    "**What is the difference between item_tfms and batch_tfms?**  \n",
    "___\n",
    "* `item_tfms` is done on cpu, `batch_tfms` on gpu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894189f-7b94-4a6f-aadd-8ec5f2aa68f7",
   "metadata": {},
   "source": [
    "Q14:  \n",
    "**What is a confusion matrix?**  \n",
    "___\n",
    "* `confusion_matrix` return where the model get wrong prediction and what was the actual label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed384a-a7c5-47d1-9f3c-cfd94e2f507d",
   "metadata": {},
   "source": [
    "Q15:  \n",
    "**What does export save do?**  \n",
    "____\n",
    "* It saves 3 things:\n",
    "    - the architecture\n",
    "    - the updated parameters(weihts+biases)\n",
    "    - the way we built dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a212c5-0b7b-4abd-8ac1-4663be3f6759",
   "metadata": {},
   "source": [
    "Q15:  \n",
    "**What is it called when we use a model for getting predictions, instead of training?**  \n",
    "____\n",
    "* Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da8ba6-062d-46eb-a082-364176eed966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
