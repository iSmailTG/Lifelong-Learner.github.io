{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BjPgBtpDPBsS"
   },
   "source": [
    "---\n",
    "title: \"Chapter 5: Deep learning for coders with fastai and pytorch \"\n",
    "author: \"Ismail TG\"\n",
    "date: \"10/26/2022\"\n",
    "categories: [Fastai, Pytorch, Numpy, Pandas, Deep Learning]\n",
    "image: \"fastbook.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lCAvl4WzRUS"
   },
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKfdw_t7zWFs"
   },
   "source": [
    "* Since we now familiar the whole process of creating deep learning model, using pre-built model, building them from scratch, handling data, and putting these model into web apps, we will now to go deeper and keep focus on details that make model accurate and reliable.\n",
    "* It takes many tweaks and parameters changing in order to \"polish\" a model.\n",
    "* In order to achieve this goal we need to be familiar with many concepts and technics, different types of layers, regularization methods, optimizers, how to put layers together into architectures, labeling techniques, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gWGJzKlycno",
    "outputId": "b9dfbc8f-6580-442d-d1b9-d979e1c0109c"
   },
   "outputs": [],
   "source": [
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCWf-26-ymBn"
   },
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "from fastai.vision.widgets import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT7yiHF-0xNl"
   },
   "source": [
    "## From Dogs and Cats to Pet Breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etUo-WfZ0y5O"
   },
   "source": [
    "* In real world scenarios, the first thing we do is we get in contact with data, usualy at this phase we know nothing about the dataset. We then start to look how to extract the data we want from it, and what the data looks like, and how it is structured.\n",
    "* Usually data is provided in one of two ways:\n",
    "\n",
    "    - Individual files representing items of data, possibly organized into folder or with filenames representing information about those items\n",
    "       * text documents  \n",
    "       * images  \n",
    "    - A table of data in which each row is an item and may include filenames providing connections between the data in the table and data in other formats \n",
    "        - CSV files  \n",
    "* Exceptions:\n",
    "    - Domains like Genomics  \n",
    "    - binary database formats  \n",
    "    - network streams  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "V3Rqy10GzVQB",
    "outputId": "71d97011-ca2e-4a6a-99f5-7f10143579fa"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7_upR6X1kc9",
    "outputId": "07455616-0517-49b4-80ca-05c53ac34c50"
   },
   "outputs": [],
   "source": [
    "#get the path as variable, and see what inside\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbQAAleF1vJ-"
   },
   "source": [
    "* As we notice here, the data is provided with 2 directories: \n",
    "  - `images`\n",
    "  - `annotations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVL05er61r4o",
    "outputId": "c796f139-fc97-41ce-9f1c-86d731e96600"
   },
   "outputs": [],
   "source": [
    "#take a look at what inside the images directory\n",
    "(path/'images').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAcE4kfC2jfu"
   },
   "source": [
    "* When we took a look at these names, we see some paterns: we already know from `chapter1` that cats name are uppercase, here we see that after the breed's name there is a `(_)` then a number, and finally the extension. \n",
    "* This may help us to write some code that extract the breed from a single `Path`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AjsTx-Q2YcD",
    "outputId": "b1ae1724-4205-40eb-b039-fe1e885d7390"
   },
   "outputs": [],
   "source": [
    "#pick one \n",
    "fname = (path/\"images\").ls()[0]\n",
    "fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FNZVn6t2vyH"
   },
   "source": [
    "* The best way to work with strings and extract patterns from them is to use `Regex`, which stands for **Regular Expression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neqe3s132tM_",
    "outputId": "1bd98346-bcfd-4bec-82f5-3fa4ec8bdec0"
   },
   "outputs": [],
   "source": [
    "re.findall(r'(.+)_\\d+.jpg$', fname.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6OvUZnH26LV"
   },
   "source": [
    "* Now we need to label the whole dataset using this code. \n",
    "* Fastai comes with many classes for labeling, in this case when we need to label with help of `regex` we could use `RegexLabeller` class within `DataBlaock` `API`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FX466KWK20xX"
   },
   "outputs": [],
   "source": [
    "pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files, \n",
    "                 splitter=RandomSplitter(seed=42),\n",
    "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
    "dls = pets.dataloaders(path/\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8wA0ZLZ3w3P"
   },
   "source": [
    "## Presizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jAO6Ruy3yE0"
   },
   "source": [
    "* Fastai has this method of presizing the images in a way that conserve its quality after the data augmentation, so it helps the model to learn more lessons from data, and also it helps our dataset to be more varies\n",
    "* The idea behind the `presizing` is we crop the image and resize it to 460 by 460 first, which is a big size by deep learning norms, this operation is done on CPU, then we do the data augmentation in batches, by cropping a rotated random part of that 460^2 image, and taking the cropped image then resize again to a 224 by 224 image, all this operation are done on batch level, which mean on GPU.**  \n",
    "```\n",
    "item_tfms=Resize(460),  \n",
    "batch_tfms=aug_transforms(size=224, min_scale=0.75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j-gGPO745Bt"
   },
   "source": [
    "![REsize_method](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_o0ntPn5Ohg"
   },
   "source": [
    "* Usually all the augmentation operations we do on a image reduce the quality of the image, but with this approach we could say we can preserve big part of informations of that image so the model can learn better \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXVjus2e5sM1"
   },
   "source": [
    "### Checking and Debugging a DataBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGdFpbyF52Db"
   },
   "source": [
    "* `DataBlock` is just a blueprint for orginizing data before we feed it to the model, you have no guarantee that your template is going to work on your data source as you intend.\n",
    "* So, before training a model you should always check your data. You can do this using the show_batch method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "AXUGS1qb5uS9",
    "outputId": "0f1711e2-f2c6-4703-ed64-1ea456f04a2d"
   },
   "outputs": [],
   "source": [
    "dls.show_batch(nrows=1, ncols=4 ,unique= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mF8rloGi6jXZ"
   },
   "source": [
    "* In case we made a mistake in the process of creating datablock, we could use `.summary` to track the problem.Here we didn't resize the images in one formm, so couldn't use the batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-rANFdA7X3P"
   },
   "source": [
    "* As we see here the `.summary` gives us precise diagnostic of the problem:  \n",
    "\n",
    "  - `at least two tensors in the batch are not the same size.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IyJG2ckE1Je"
   },
   "source": [
    "### Clean the data with the model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2SzmHNfE5Fx"
   },
   "source": [
    "* Once we feel like the datablock is well created, we better begin train the model, and use it as a tool of cleaning the data. If there's a problem with data or the model, we better know that before we lost lot of time and energy on data cleaning even before testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315,
     "referenced_widgets": [
      "ecfa21a5a69f4935bcb3bdccbbe333c0",
      "584eef277e45424cb15c012a1dd1a0e4",
      "7544608ce86045ad8806b8405b5d186a",
      "e68cb964c5734951acabf37dedaca77e",
      "e4570c4419644c088f6ddad73443e0d9",
      "8d49518e8a854ff7afa6db7b35ba22ad",
      "de4daa621c034dbc96955e4d91037bf3",
      "88f46bced5544d0481c9289992b941e8",
      "103068837d454108ba7b1983f5509b83",
      "acc2d440f08c4537bcdce9de80dad6d8",
      "07eaf3abcfa9423cb778475986615814"
     ]
    },
    "id": "59YOWPCL6n8x",
    "outputId": "733c3312-d487-47c6-bebe-6f89eafeb16f"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clliuYGmMCb2"
   },
   "source": [
    "### Use the model to clean the data\n",
    "\n",
    "* As we saw before in **`Chapter 2`** the best tool to clean the data is basically the model itself\n",
    "* After creating the datablock and dataloader we better train the model and get some feedback so we know if something is wrong very early, and if not we start to use the model as tool to investigate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyeCwol7GHtY"
   },
   "source": [
    "* Usually before we train the model we have to decide the function that will update the parameters, a **`Loss Function`**. But here we didn't create any loss function?\n",
    "* If we didn't decide the way by which we update the paramters, Fastai by default will chose a loss function for us.\n",
    "  - the chosen loss function will suite the kind of model we build, and the type of dataset we have.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kDGhVceFCq9",
    "outputId": "2f3a0898-a25e-40c3-b4b0-0c4588133276"
   },
   "outputs": [],
   "source": [
    "# check the loss function\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu1CiE6dHNZk"
   },
   "source": [
    "* The loss function used to train this model is **`Cross Entropy Loss`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uIC-OCAOn30"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z-SMefVOr_a"
   },
   "source": [
    "* The Cross-Entropy Loss is a function similar to what we saw in the previous chapter, when we created the `mnist_loss` function: \n",
    "```\n",
    "def mnist_loss(predictions, targets):\n",
    "    prediction = prediction.sigmoid()\n",
    "    return torch.where(targets==1, 1-prediction, predictions).mean()\n",
    "```  \n",
    "* The problem with this function is, it only takes 2 categories(3, 7) but here we have 37 types of breeds.but here we have multiple classes.\n",
    "   - it can takes more than 2 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW0aIC72P6MC"
   },
   "source": [
    "### Viewing activations and labels\n",
    "\n",
    "* In order to understand the cross-entropy loss, let's grab a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnezKtqlHH3C"
   },
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc701g3QQpQp"
   },
   "source": [
    "* It return the activations of dependent and independent variable of one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpiQXyZsPmum",
    "outputId": "3a7dbedd-df41-44f0-a5ec-46d1d7d50a8b"
   },
   "outputs": [],
   "source": [
    "# independent variable\n",
    "y, len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrGwJ2YLUU1h"
   },
   "source": [
    "* It return 64 number, each represent on of the 37 breeds index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZzyhqzGUyf3",
    "outputId": "cf31f01a-3a69-4c05-e15f-74de2a83271c"
   },
   "outputs": [],
   "source": [
    "# all indepent variables\n",
    "dls.vocab, len(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrUfaojbVFiu"
   },
   "source": [
    "* Here we use `get_preds` to get the predictions for each image in the dataset or mini-batch (like in this example).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "fq2IeBxeQ93-",
    "outputId": "888eeac4-848a-40f4-c504-cfc62c91bdc1"
   },
   "outputs": [],
   "source": [
    "preds,_ = learn.get_preds(dl=[(x,y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7242-NFbSjCM",
    "outputId": "1353d4bf-cd6b-46c9-c2f4-76a12fe9ce54"
   },
   "outputs": [],
   "source": [
    "# prediction for image [0] in the mini-batch\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2ojVZlNWdEB"
   },
   "source": [
    "* The 37 predictions refer to the probability of each breed to match the image[0].\n",
    "    - if we sum() them up they add up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CW5Ng65HS_IK",
    "outputId": "49c56fa1-efc3-4c0b-d84d-e590eed56dd5"
   },
   "outputs": [],
   "source": [
    "preds[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPxTYdOlXbQy"
   },
   "source": [
    "* In order to transform the activations of our model into prediction, we use **`Soft-Max`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjrAvPATXtM4"
   },
   "source": [
    "### SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUqSvcWzaGMJ"
   },
   "source": [
    "* As we said before, `Softmax` is similar to `sigmoid` function we use before, but it only can handel more than 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "vYhJ1ZxkW5_s",
    "outputId": "1fb827da-8820-4f2e-89ff-67488f5ec344"
   },
   "outputs": [],
   "source": [
    "# reminder of sigmoid\n",
    "plot_function(torch.sigmoid, min=-4,max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjIDEn0Maydz"
   },
   "source": [
    "* This function allow us to predict whether a activation number is pointing to each category of the **two**, by calculating which activation is big and by much.\n",
    "But in our case today we have 37 category, which means by this logic we need a activation for each one.\n",
    "* First let's create a similar situation where we have only 2 categories, but we won't solve it as it's a binary problem (`it's 3`) but as 2 categories problem, each has it's activation, and their probabilty sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k62zBsnuc6EU"
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0Uxsvv8amrz",
    "outputId": "22a6a922-6506-484b-b8fb-09610287efa1"
   },
   "outputs": [],
   "source": [
    "acts = torch.randn((6,2))*2\n",
    "acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0aNvj5IdEzs"
   },
   "source": [
    "* We can't just take the sigmoid of this directly, since we don't get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Jiplz4bvJK",
    "outputId": "7f8ea585-604f-4d84-e0a1-d5d789552815"
   },
   "outputs": [],
   "source": [
    "sigmoid(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1S4ACIzda6O"
   },
   "source": [
    "* Eevn though we try different approach to solve the same problem, we still have some similarities.\n",
    "* We will use `sigmoid` on each activation.\n",
    "* And we still need to substract an activation from another beacuse that represent how much the model sure about an image is assigned to each category, thats for first column.\n",
    "* In the second colun we just use 1 - prediction (activation of the second column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIZQfzQwdJQt"
   },
   "outputs": [],
   "source": [
    "diffs = acts[:, 0] - acts[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbfbpP1StJnT"
   },
   "outputs": [],
   "source": [
    "# create the sigmoid function of both categories\n",
    "sigm_ver = torch.stack([diffs.sigmoid(), 1-diffs.sigmoid()], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwgtVfBOkq14"
   },
   "source": [
    "* We can express the `softmax` function as:\n",
    "\n",
    "```\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrEpvHr0nlBe"
   },
   "source": [
    "* Let's check that `softmax` returns the same values as `sigmoid` for the first column, and those values subtracted from 1 for the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GByMUfInmXyx",
    "outputId": "203825d0-2cb4-4183-aea3-58fd4970e9fa"
   },
   "outputs": [],
   "source": [
    "sm_acts = torch.softmax(acts, dim=1)\n",
    "sm_acts, sigm_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2z-Z-DzuRP0"
   },
   "source": [
    "* Softmax calculate the  $exp^{x}$ and divide it by sum $exp^{x}$ of all activations of other categories.\n",
    "    - the `exp` make sure the biggest activation is way bigger than others\n",
    "    - dividing by the sum is what make softmax values add up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1DmdXGd10F2"
   },
   "source": [
    "### Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmOBvMrW14J2"
   },
   "source": [
    "* In the previous chapter when we created `mnis_loss`, we used `torch.where` to select between the `input` and `1-input`.\n",
    "* With softmax, we will use indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXdVt2oM1e0F"
   },
   "outputs": [],
   "source": [
    "# the pretended targets\n",
    "targs = tensor([0, 1, 0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0JNZ0pt2qiB"
   },
   "outputs": [],
   "source": [
    "# create an index\n",
    "idx = range(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RkBk9xD25TL",
    "outputId": "18c61349-1e17-4f54-b1a5-421b00b2f4d3"
   },
   "outputs": [],
   "source": [
    "# the softmax activations\n",
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FklG75oK3QSR"
   },
   "source": [
    "* Here we make the `targs` decide which activation we pick in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "js2pdwmH27ig",
    "outputId": "c0889161-c10d-47bf-ca38-104330292bd6"
   },
   "outputs": [],
   "source": [
    "sm_acts[idx, targs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Breqcjt4aXQ"
   },
   "source": [
    "* let's display what we just did :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "PJLGYYTu4ga2",
    "outputId": "c3781985-2845-4245-aaae-55b8d8721f91"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "df = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\n",
    "df['targs'] = targs\n",
    "df['idx'] = idx\n",
    "df['result'] = sm_acts[range(6), targs]\n",
    "t = df.style.hide_index()\n",
    "#To have html code compatible with our script\n",
    "html = t._repr_html_().split('</style>')[1]\n",
    "html = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5sBFN6F37cA"
   },
   "source": [
    "* But idea here is not to use it in a simple binary problem, because `torch.where` could did the same job here, but is to use it in order to solve a multi-categorie problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhhXVIDnNwZH"
   },
   "source": [
    "PyTorch provides a function that does exactly the same thing as `sm_acts[range(n), targ]` (except it takes the negative, because when applying the log afterward, we will have negative numbers), called `nll_loss` (*NLL* stands for *negative log likelihood*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRfTJoQw3h6C",
    "outputId": "0e82a345-5cf2-435e-856f-64183342c501"
   },
   "outputs": [],
   "source": [
    "-sm_acts[idx, targs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IV5yzkIOAPt",
    "outputId": "72e5d1dd-f8e0-4489-cea3-5eae8423ad49"
   },
   "outputs": [],
   "source": [
    "F.nll_loss(sm_acts, targs, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQev5RaiRIwK"
   },
   "source": [
    "### Taking the Log\n",
    "![cross-entropy-loss-function.png](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsc5SyNLRtih"
   },
   "source": [
    "* The using of `logarithms` allow us to do all kind of multiplications without carring about the size of the output.\n",
    "    - the nature of `log` functions make them increase lineary when the underlying signal increase exponentialy.\n",
    "    - `log(a*b) = log(a)+log(b)`\n",
    "    - the `log` of a number approaches negative infinity when the number approaches zero\n",
    "* In our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is \"good\" (closer to 1) and a large value when the prediction is \"bad\" (closer to 0).\n",
    "* Notice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class.  One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong.  This kind of penalty works well in practice to aid in more effective model training.  \n",
    "* Calculating the loss pay attention only to the high softmax value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98yZ3d34rNA0"
   },
   "source": [
    "### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWu6jiZDsooa"
   },
   "source": [
    "* After taking the log of the softmax, we can then call the negative log likelihood.\n",
    "   - first : `log_softmax`\n",
    "   - then  : `nll_loss`\n",
    "   - or : `nn.CrossEntropyLoss()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pKKNB2pPx5i"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOMx8A2XtU8r",
    "outputId": "6b778c8a-99be-4f33-d6be-2867252718e1"
   },
   "outputs": [],
   "source": [
    "loss_func(acts, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFLfXVfoT5An",
    "outputId": "43f1c051-68ac-412d-a4dc-e2ec3a9af052"
   },
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss()(acts, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEyQKK3qfbVn"
   },
   "source": [
    "* The `nn.CrossEntrpyLoss()` make do all the steps for us, but if we want to go through all those steps one by one `softmas+log then negative log` we could do it also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuQJA3MMfaqQ",
    "outputId": "c4767369-8cad-4d51-9ed7-e7ebe4a39083"
   },
   "outputs": [],
   "source": [
    "F.nll_loss(nn.Softmax()(acts).log(), targs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1FsHTQQgtf3"
   },
   "source": [
    "* Adding the `reduction='none'` to this functions will return the loss of each row, if we didn't add this aparameter the fuction will return the mean loss of all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71Up7inmhe-z",
    "outputId": "d8f36ee1-1886-4ae6-a7d0-f236546eb76d"
   },
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss(reduction='none')(acts, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3oCviE2Vp7a"
   },
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR4YjhEgVuKn"
   },
   "source": [
    "* As we saw in **chapter 3** it's hard for us to interpret the loss function, since it's some the computers use in order to updates the parameters and optimize the performance.\n",
    "* But we can use some kind of demonstration that shows where the model did good, and where did bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "VUZUQsfmUOFq",
    "outputId": "57acddc2-fb98-4523-cb2f-e3cfbca23c5a"
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B5EsqZOWlBF"
   },
   "source": [
    "* Its was easy to understand what happened when they were only 3 classes in bears model, but here we have 37 breeds.\n",
    "    - thats why we will use`interp.most_confused(min_val=5)` to output to most bad decisions the model taked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "7CfKQMJkWbQ1",
    "outputId": "e26e46bb-eebd-490a-df0c-9ff64806764a"
   },
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRbAgMYrXKzt"
   },
   "source": [
    "* The best way to understand what happend is to google the names of each breed and see why the model confused it with the other breed, so we know that the model is in the right track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_ba7gqFXeRK"
   },
   "source": [
    "## Improving Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ud6aOKUX1yh"
   },
   "source": [
    "* At this point all we can do is improve the model by correcting some detaills that may optimize the final prefromance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZVsJEHeYHKd"
   },
   "source": [
    "### The Learning Rate Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMJaS72iYJKl"
   },
   "source": [
    "* One way of improving our model is by picking the right learning rate.\n",
    "   - it will help to get faster result per epoch\n",
    "   - minimize the loss and updating parameters with less steps      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "GTNboZWsXBVw",
    "outputId": "c7a26dd8-cacb-40fe-f136-da3b6b2eaf45"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1, base_lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNmC-J4zn4Rs"
   },
   "source": [
    "* Here we pick a learning rate `0.1` which is 5 times bigger than the last one `0.002` and we get bad results `error rate at: 0.5`\n",
    "    - big learning rate may reduce the computation needed for the training process but the model performance will be bad\n",
    "* Also if we pick a small learning rate it will take forever to achieve something.\n",
    "* The answear for this dilemma is **`The Learning Rate Finder`**    \n",
    "* Fastai library adopte this method created by the resaercher  [Leslie Smith](https://scholar.google.com/citations?user=pwh7Pw4AAAAJ&hl=en) in a [paper](https://arxiv.org/abs/1506.01186) in 2015.\n",
    "    - the idea of Smith is to start with a small learning rate (very small), and use it for one mini-batch, see how much the loss changed, and then start increasing the learning rate by some percentage (doubling it since its very small anyway)\n",
    "    - repeate this process again(track the loss, double the learning rate ..) until the loss get worse.\n",
    "    - at this point we just pick a learning rate smaller than the one that causes the loss to get worse.\n",
    "* Fastai course advice is either:\n",
    "    - one order of magnitude less than where the minimun loss was achieved(divide by 10)\n",
    "    - the last point where the loss was clearly decreasing\n",
    "* Both point are giving the same value usually.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "NFOKsstVcVwy",
    "outputId": "948d8f51-6ee3-406d-85ab-db6890d3c602"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "lr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5jlSMu4c-1V",
    "outputId": "e02c5d2b-86d2-41fc-a733-91e5c46d5829"
   },
   "outputs": [],
   "source": [
    "print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6AT_gB74EAt"
   },
   "source": [
    "* The plot shows that the loss between 10e-6 and 10e-3 almost didn't change, but after it start to decrease until it reachs the minimum at 10e-1.\n",
    "* We don't want a learning rate bigger than 10e-1 because there where the loss get worse, and we don't need learning rate at 10e-1 because at this value we've left the stage where the loss was decreasing.\n",
    "    - we need to pick the learning rate where the just start to decrease all the way to the minimum: `1e-3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "q9u0A4V91CeV",
    "outputId": "321bda5c-51f6-4e7c-cea5-7b1defe411bc"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(2, base_lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4uCMTz068Iz"
   },
   "source": [
    "* The error rate get better 10 times just by using the learning rate methode. Loss also get better by this percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIJFwUerAMdo"
   },
   "source": [
    "### Unfreezing and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5e-fQQMAYS-"
   },
   "source": [
    "* We are familiar with the idea of **Transfer Learning**, where we use a pretrainned model on our dataset, by fine tuning it in a way that keep all the learned weights and use them in our task.\n",
    "* We know tha Convolutional Neural Network consist of many linear layers, and between each two of them there's a nonlinear activation function (ReLU for example), followed by the final layer with an activation function such as **Softmax**. The final layer uses a matrix with enough columns such that the ouput size is has the number of classes our model trained to predict(assuming we have a classfication task)\n",
    "This final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset. \n",
    "* So we first delete it when we start the transfer learning process, and replace it with a new linear layer with the correct number of outputs that matches our desired task(in this case 37 breeds, so 37 activations)\n",
    "* This new linear layer have total randome set of weights, but that doesn't mean we should set all weights randomly even for the pretrained part.\n",
    "   - All of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the [Zeiler and Fergus paper](https://arxiv.org/pdf/1311.2901.pdf), the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\n",
    "* We want to build a model such as preserve all the learned weights, and apply them on our dataset, so only adjust them as required for the specifics of our particular task.\n",
    "* So, the idea is to keep the pretrained part's weights intact, and only update the weights of the added part. This process is called **Freezing**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAseyedwGA0E"
   },
   "source": [
    "* When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the `fine_tune` method fastai does two things:\n",
    "\n",
    "   - Trains the randomly added layers for one epoch, with all other layers frozen\n",
    "   - Unfreezes all of the layers, and trains them all for the number of epochs requested\n",
    "* Of Course this is just the *default* approach, `fine_tune` has many parameters that allow us to apply different tweaks for each specific situation. \n",
    "* For now, let's do this process manually without using `fine_tune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bscau8TR52KN"
   },
   "outputs": [],
   "source": [
    "# check fine_tune source acode\n",
    "learn.fine_tune??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh7pg95js9gu"
   },
   "source": [
    "* First we create our learner from the `dls` and `arch` using `vision_learner`\n",
    "     - by default `vision_learner` will freeze the pre-trained part of the model (freeze the params)\n",
    "* Then train the added layer with randome weights for number of epochs with a learning rate we pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "6cfZ1-2Cr4RF",
    "outputId": "77d14197-3adf-487f-e6e5-729f9e0dca0d"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fit_one_cycle(3, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tRAezuxxD5A"
   },
   "source": [
    "* Now we need to unfreeze the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceqQOtkWvUbz"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyXqwYlixTc2"
   },
   "source": [
    "* Now we run `lr_find` again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn't appropriate any more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "rYEHlYjDxMN7",
    "outputId": "17b697ff-8faf-45a6-c727-b19b1ca8ece7"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4wzDiGEyeOh"
   },
   "source": [
    "* As we see here the graph is different than what we saw before when we use randome weights to train the model, because that the model has been trained already.\n",
    "* The approach to pick the right `lr` here is to chose a point before the sharp increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWI1QoqgYA8H"
   },
   "outputs": [],
   "source": [
    "34se3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "WFWU3_kBxvdz",
    "outputId": "00577add-6b54-4930-9785-e9661152acdd"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6 , lr_max=4.786300905834651e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSAp70s69TRp"
   },
   "source": [
    "### Discriminative Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMU66GF02l47"
   },
   "source": [
    "* After training the model for 6 epochs we get eror_rate at `6%` which is fine, but we could do better.\n",
    "* The thing we could optimize here is to rethink the learning rate again.\n",
    "   - picking one learning rate value for the whole neural network isn't a good idea.\n",
    "   - the model is consisted of 2 parts as we know:\n",
    "      * the pre-trained part contained good parameters that has been trained for many epochs\n",
    "      * the last layer which we trained ourself for not more than 10 (3+6)\n",
    "   - so idea here is we shouldn't trait both parts as if they are the same by picking one learning rate for the whole model\n",
    "   - instead we could go with a small `lr` value for the first part, then aplly a slightly bigger one for the last layer.\n",
    "* This technic is devloped by [Jason Yosinski](https://arxiv.org/abs/1411.1792) and his team. They shows in 2014 that with transfer learning, different layer should be trained at different speed.\n",
    "![att_00039.png](3.png)\n",
    "   - Fastai adopt this idea by using `slice`, which is a built-in object that let you pass 2 values:\n",
    "      * the first define the learning rate of the earlier layer\n",
    "      * the second for the last layers\n",
    "   - The layers in between will have learning rates that are multiplicatively equidistant throughout that range\n",
    "* Let's see this technic in action   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "9qmoqT3f9Cup",
    "outputId": "90cdc81c-ca8a-4669-8bab-0650946d9771"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fit_one_cycle(3, 3e-3)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(14, lr_max=slice(1e-6,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snI-cMoHE4gy"
   },
   "source": [
    "* We can plot the training and the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "h1eE-D3vAmm5",
    "outputId": "9888b6fe-d4fd-44bd-ed83-018e4a262eac"
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDcaLdCoFcyE"
   },
   "source": [
    "### Selecting the Number of Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRZ9uR6EFePh"
   },
   "source": [
    "* Choosing the right amount of epoch you will train the model on is also something we should address properly.\n",
    "* We need to keep eye on the train/val loss as shown above, but also on error rate (or any metric we pick).\n",
    "* If the loss and the netric are getting better significantly at the end of training, that's mean we didn't train for too long\n",
    "* The loss is just something we use to allow the optimizer to have something it can different and optimize, it's not something we really should care about in practice.\n",
    "   - if the loss of the validation get worse at during the training because the model is getting over confident, only later it get worse because of overfitting, in practice we care only about the later issue\n",
    "   - In case of overfitting, the easy solution is to retrain from scratch again, and this time select a total number of epochs based on where your previous best results were found\n",
    "* It's not all about epochs, we could add more parameters to the model to get better result   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLOedtSWPpT8"
   },
   "source": [
    "### Deeper Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMNZM9rbQgji"
   },
   "source": [
    "* In general, more parameters handle the date more accuratly.\n",
    "* Using a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an `out-of-memory error`. \n",
    "     -  The way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your `DataLoaders` with `bs=`\n",
    "* The other downside of deeper architectures is that they take quite a bit longer to train. \n",
    "     - One technique that can speed things up a lot is *mixed-precision training*. This refers to using less-precise numbers (*half-precision floating point*, also called *fp16*) where possible during training.\n",
    "     -  To enable this feature in fastai, just add `to_fp16()` after your `Learner` creation (you also need to import the module).\n",
    "\n",
    "* You can't really know ahead of time what the best architecture for your particular problem is you need to try training some. So let's try a ResNet-50 now with mixed precision:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "3480deaea9b9405cbf63fa9fd8bf2b34",
      "7810a682e4064d628440084bbb886450",
      "cb5dd886c6ce44279b36886e2ceaeb0c",
      "3473e96a6da64d5db5bc47888b5bc71a",
      "491e912c48364ecd9167c81c1456ba6f",
      "e75f0d3818e24d27b1f2c53beb752163",
      "a175cff1faa24be69f041775d8c782e9",
      "95f776162258441cbc9bbd433b772ac1",
      "124ab37ffe1f45509df61eb4f5d9d69a",
      "20a035e67e904d4b810286c86cf26e05",
      "cd8eb6976e0445dbad2b0542be1c520d"
     ]
    },
    "id": "eTFdUouQFBaO",
    "outputId": "da9de3a5-281a-4b62-e633-b9ca61c4998d"
   },
   "outputs": [],
   "source": [
    "from fastai.callback.fp16 import *\n",
    "learn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\n",
    "learn.fine_tune(6, freeze_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQgCX1bET_p1"
   },
   "source": [
    "* We get better results, at less epochs, and less time per epochs only by usung deeper architecture.\n",
    "   - but it's allways better to start with small model, before scaling-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W8qrE5QXRkjo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07eaf3abcfa9423cb778475986615814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "103068837d454108ba7b1983f5509b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "124ab37ffe1f45509df61eb4f5d9d69a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20a035e67e904d4b810286c86cf26e05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3473e96a6da64d5db5bc47888b5bc71a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20a035e67e904d4b810286c86cf26e05",
      "placeholder": "​",
      "style": "IPY_MODEL_cd8eb6976e0445dbad2b0542be1c520d",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 238MB/s]"
     }
    },
    "3480deaea9b9405cbf63fa9fd8bf2b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7810a682e4064d628440084bbb886450",
       "IPY_MODEL_cb5dd886c6ce44279b36886e2ceaeb0c",
       "IPY_MODEL_3473e96a6da64d5db5bc47888b5bc71a"
      ],
      "layout": "IPY_MODEL_491e912c48364ecd9167c81c1456ba6f"
     }
    },
    "491e912c48364ecd9167c81c1456ba6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "584eef277e45424cb15c012a1dd1a0e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d49518e8a854ff7afa6db7b35ba22ad",
      "placeholder": "​",
      "style": "IPY_MODEL_de4daa621c034dbc96955e4d91037bf3",
      "value": "100%"
     }
    },
    "7544608ce86045ad8806b8405b5d186a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88f46bced5544d0481c9289992b941e8",
      "max": 87319819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_103068837d454108ba7b1983f5509b83",
      "value": 87319819
     }
    },
    "7810a682e4064d628440084bbb886450": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e75f0d3818e24d27b1f2c53beb752163",
      "placeholder": "​",
      "style": "IPY_MODEL_a175cff1faa24be69f041775d8c782e9",
      "value": "100%"
     }
    },
    "88f46bced5544d0481c9289992b941e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d49518e8a854ff7afa6db7b35ba22ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95f776162258441cbc9bbd433b772ac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a175cff1faa24be69f041775d8c782e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acc2d440f08c4537bcdce9de80dad6d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb5dd886c6ce44279b36886e2ceaeb0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95f776162258441cbc9bbd433b772ac1",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_124ab37ffe1f45509df61eb4f5d9d69a",
      "value": 102530333
     }
    },
    "cd8eb6976e0445dbad2b0542be1c520d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de4daa621c034dbc96955e4d91037bf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4570c4419644c088f6ddad73443e0d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e68cb964c5734951acabf37dedaca77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acc2d440f08c4537bcdce9de80dad6d8",
      "placeholder": "​",
      "style": "IPY_MODEL_07eaf3abcfa9423cb778475986615814",
      "value": " 83.3M/83.3M [00:00&lt;00:00, 238MB/s]"
     }
    },
    "e75f0d3818e24d27b1f2c53beb752163": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecfa21a5a69f4935bcb3bdccbbe333c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_584eef277e45424cb15c012a1dd1a0e4",
       "IPY_MODEL_7544608ce86045ad8806b8405b5d186a",
       "IPY_MODEL_e68cb964c5734951acabf37dedaca77e"
      ],
      "layout": "IPY_MODEL_e4570c4419644c088f6ddad73443e0d9"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
